{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.8951734120135835,
  "eval_steps": 500,
  "global_step": 13000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.002226799532372098,
      "grad_norm": 14.66821575164795,
      "learning_rate": 4.9974016332590946e-05,
      "loss": 23.9607,
      "step": 10
    },
    {
      "epoch": 0.004453599064744196,
      "grad_norm": 9.758298873901367,
      "learning_rate": 4.994060876020787e-05,
      "loss": 23.2537,
      "step": 20
    },
    {
      "epoch": 0.0066803985971162945,
      "grad_norm": 13.095754623413086,
      "learning_rate": 4.990348923533779e-05,
      "loss": 22.3146,
      "step": 30
    },
    {
      "epoch": 0.008907198129488392,
      "grad_norm": 17.27506446838379,
      "learning_rate": 4.987379361544173e-05,
      "loss": 20.3153,
      "step": 40
    },
    {
      "epoch": 0.011133997661860491,
      "grad_norm": 38.3668098449707,
      "learning_rate": 4.9836674090571646e-05,
      "loss": 18.6291,
      "step": 50
    },
    {
      "epoch": 0.013360797194232589,
      "grad_norm": 24.948598861694336,
      "learning_rate": 4.979955456570156e-05,
      "loss": 15.7982,
      "step": 60
    },
    {
      "epoch": 0.015587596726604687,
      "grad_norm": 22.660381317138672,
      "learning_rate": 4.976243504083148e-05,
      "loss": 12.121,
      "step": 70
    },
    {
      "epoch": 0.017814396258976784,
      "grad_norm": 17.65057373046875,
      "learning_rate": 4.9725315515961396e-05,
      "loss": 9.6813,
      "step": 80
    },
    {
      "epoch": 0.020041195791348883,
      "grad_norm": 12.64260196685791,
      "learning_rate": 4.968819599109132e-05,
      "loss": 7.4249,
      "step": 90
    },
    {
      "epoch": 0.022267995323720983,
      "grad_norm": 6.997387886047363,
      "learning_rate": 4.9651076466221236e-05,
      "loss": 5.3363,
      "step": 100
    },
    {
      "epoch": 0.02449479485609308,
      "grad_norm": 12.004903793334961,
      "learning_rate": 4.961395694135115e-05,
      "loss": 4.0718,
      "step": 110
    },
    {
      "epoch": 0.026721594388465178,
      "grad_norm": 3.7087900638580322,
      "learning_rate": 4.957683741648107e-05,
      "loss": 3.0917,
      "step": 120
    },
    {
      "epoch": 0.028948393920837277,
      "grad_norm": 3.5670042037963867,
      "learning_rate": 4.9539717891610986e-05,
      "loss": 2.4145,
      "step": 130
    },
    {
      "epoch": 0.031175193453209373,
      "grad_norm": 1.3206578493118286,
      "learning_rate": 4.950259836674091e-05,
      "loss": 1.9971,
      "step": 140
    },
    {
      "epoch": 0.033401992985581476,
      "grad_norm": 1.1285094022750854,
      "learning_rate": 4.9465478841870826e-05,
      "loss": 1.8011,
      "step": 150
    },
    {
      "epoch": 0.03562879251795357,
      "grad_norm": 1.0621482133865356,
      "learning_rate": 4.942835931700074e-05,
      "loss": 1.5843,
      "step": 160
    },
    {
      "epoch": 0.03785559205032567,
      "grad_norm": 1.250888466835022,
      "learning_rate": 4.939123979213066e-05,
      "loss": 1.5199,
      "step": 170
    },
    {
      "epoch": 0.04008239158269777,
      "grad_norm": 1.0022248029708862,
      "learning_rate": 4.935412026726058e-05,
      "loss": 1.3718,
      "step": 180
    },
    {
      "epoch": 0.042309191115069866,
      "grad_norm": 1.060944676399231,
      "learning_rate": 4.93170007423905e-05,
      "loss": 1.2495,
      "step": 190
    },
    {
      "epoch": 0.044535990647441966,
      "grad_norm": 1.8683961629867554,
      "learning_rate": 4.9279881217520416e-05,
      "loss": 1.2451,
      "step": 200
    },
    {
      "epoch": 0.046762790179814065,
      "grad_norm": 0.9312345385551453,
      "learning_rate": 4.924276169265033e-05,
      "loss": 1.2233,
      "step": 210
    },
    {
      "epoch": 0.04898958971218616,
      "grad_norm": 0.9179047346115112,
      "learning_rate": 4.9205642167780256e-05,
      "loss": 1.1662,
      "step": 220
    },
    {
      "epoch": 0.05121638924455826,
      "grad_norm": 0.9266819953918457,
      "learning_rate": 4.916852264291017e-05,
      "loss": 1.1118,
      "step": 230
    },
    {
      "epoch": 0.053443188776930356,
      "grad_norm": 0.8367058634757996,
      "learning_rate": 4.913140311804009e-05,
      "loss": 1.1142,
      "step": 240
    },
    {
      "epoch": 0.055669988309302455,
      "grad_norm": 0.7706857919692993,
      "learning_rate": 4.909428359317001e-05,
      "loss": 1.0759,
      "step": 250
    },
    {
      "epoch": 0.057896787841674555,
      "grad_norm": 0.8481189012527466,
      "learning_rate": 4.905716406829993e-05,
      "loss": 1.0799,
      "step": 260
    },
    {
      "epoch": 0.060123587374046654,
      "grad_norm": 0.98292076587677,
      "learning_rate": 4.9020044543429846e-05,
      "loss": 1.0863,
      "step": 270
    },
    {
      "epoch": 0.062350386906418746,
      "grad_norm": 0.8008242845535278,
      "learning_rate": 4.898292501855977e-05,
      "loss": 1.0622,
      "step": 280
    },
    {
      "epoch": 0.06457718643879085,
      "grad_norm": 0.7465980648994446,
      "learning_rate": 4.8945805493689686e-05,
      "loss": 1.067,
      "step": 290
    },
    {
      "epoch": 0.06680398597116295,
      "grad_norm": 0.7446974515914917,
      "learning_rate": 4.89086859688196e-05,
      "loss": 1.0106,
      "step": 300
    },
    {
      "epoch": 0.06903078550353504,
      "grad_norm": 0.8235477805137634,
      "learning_rate": 4.887156644394952e-05,
      "loss": 1.0469,
      "step": 310
    },
    {
      "epoch": 0.07125758503590714,
      "grad_norm": 0.7597760558128357,
      "learning_rate": 4.883444691907944e-05,
      "loss": 1.0165,
      "step": 320
    },
    {
      "epoch": 0.07348438456827924,
      "grad_norm": 0.8317661285400391,
      "learning_rate": 4.879732739420936e-05,
      "loss": 1.0419,
      "step": 330
    },
    {
      "epoch": 0.07571118410065134,
      "grad_norm": 0.7364541888237,
      "learning_rate": 4.8760207869339276e-05,
      "loss": 0.9914,
      "step": 340
    },
    {
      "epoch": 0.07793798363302344,
      "grad_norm": 0.9802740216255188,
      "learning_rate": 4.872308834446919e-05,
      "loss": 1.0424,
      "step": 350
    },
    {
      "epoch": 0.08016478316539553,
      "grad_norm": 0.7993160486221313,
      "learning_rate": 4.868596881959911e-05,
      "loss": 0.9899,
      "step": 360
    },
    {
      "epoch": 0.08239158269776764,
      "grad_norm": 1.1277949810028076,
      "learning_rate": 4.864884929472903e-05,
      "loss": 0.9718,
      "step": 370
    },
    {
      "epoch": 0.08461838223013973,
      "grad_norm": 0.695780336856842,
      "learning_rate": 4.861172976985895e-05,
      "loss": 0.9883,
      "step": 380
    },
    {
      "epoch": 0.08684518176251182,
      "grad_norm": 0.6930192112922668,
      "learning_rate": 4.8574610244988865e-05,
      "loss": 0.9612,
      "step": 390
    },
    {
      "epoch": 0.08907198129488393,
      "grad_norm": 0.9355268478393555,
      "learning_rate": 4.853749072011878e-05,
      "loss": 0.959,
      "step": 400
    },
    {
      "epoch": 0.09129878082725602,
      "grad_norm": 0.9123572707176208,
      "learning_rate": 4.85003711952487e-05,
      "loss": 1.0132,
      "step": 410
    },
    {
      "epoch": 0.09352558035962813,
      "grad_norm": 0.7761102318763733,
      "learning_rate": 4.846325167037862e-05,
      "loss": 0.923,
      "step": 420
    },
    {
      "epoch": 0.09575237989200022,
      "grad_norm": 0.9641487002372742,
      "learning_rate": 4.842613214550854e-05,
      "loss": 0.9793,
      "step": 430
    },
    {
      "epoch": 0.09797917942437231,
      "grad_norm": 0.7041003108024597,
      "learning_rate": 4.8389012620638455e-05,
      "loss": 0.983,
      "step": 440
    },
    {
      "epoch": 0.10020597895674442,
      "grad_norm": 0.8746806979179382,
      "learning_rate": 4.835189309576837e-05,
      "loss": 1.0189,
      "step": 450
    },
    {
      "epoch": 0.10243277848911651,
      "grad_norm": 1.0642991065979004,
      "learning_rate": 4.8314773570898295e-05,
      "loss": 0.9779,
      "step": 460
    },
    {
      "epoch": 0.10465957802148862,
      "grad_norm": 1.570439338684082,
      "learning_rate": 4.827765404602821e-05,
      "loss": 0.982,
      "step": 470
    },
    {
      "epoch": 0.10688637755386071,
      "grad_norm": 0.987711489200592,
      "learning_rate": 4.824053452115813e-05,
      "loss": 0.911,
      "step": 480
    },
    {
      "epoch": 0.10911317708623282,
      "grad_norm": 0.884252667427063,
      "learning_rate": 4.820341499628805e-05,
      "loss": 0.9633,
      "step": 490
    },
    {
      "epoch": 0.11133997661860491,
      "grad_norm": 0.8737984895706177,
      "learning_rate": 4.816629547141797e-05,
      "loss": 0.9292,
      "step": 500
    },
    {
      "epoch": 0.113566776150977,
      "grad_norm": 0.7848667502403259,
      "learning_rate": 4.812917594654789e-05,
      "loss": 0.8774,
      "step": 510
    },
    {
      "epoch": 0.11579357568334911,
      "grad_norm": 0.8214846849441528,
      "learning_rate": 4.809205642167781e-05,
      "loss": 0.8978,
      "step": 520
    },
    {
      "epoch": 0.1180203752157212,
      "grad_norm": 1.1208187341690063,
      "learning_rate": 4.8054936896807725e-05,
      "loss": 0.8927,
      "step": 530
    },
    {
      "epoch": 0.12024717474809331,
      "grad_norm": 0.8337661027908325,
      "learning_rate": 4.801781737193764e-05,
      "loss": 0.9131,
      "step": 540
    },
    {
      "epoch": 0.1224739742804654,
      "grad_norm": 0.6425654292106628,
      "learning_rate": 4.798069784706756e-05,
      "loss": 0.9149,
      "step": 550
    },
    {
      "epoch": 0.12470077381283749,
      "grad_norm": 0.788195013999939,
      "learning_rate": 4.794357832219748e-05,
      "loss": 0.913,
      "step": 560
    },
    {
      "epoch": 0.1269275733452096,
      "grad_norm": 0.7609685063362122,
      "learning_rate": 4.79064587973274e-05,
      "loss": 0.9218,
      "step": 570
    },
    {
      "epoch": 0.1291543728775817,
      "grad_norm": 0.7567569613456726,
      "learning_rate": 4.7869339272457315e-05,
      "loss": 0.8703,
      "step": 580
    },
    {
      "epoch": 0.13138117240995378,
      "grad_norm": 0.8693071007728577,
      "learning_rate": 4.783221974758723e-05,
      "loss": 0.8523,
      "step": 590
    },
    {
      "epoch": 0.1336079719423259,
      "grad_norm": 0.7149233818054199,
      "learning_rate": 4.7795100222717155e-05,
      "loss": 0.8482,
      "step": 600
    },
    {
      "epoch": 0.135834771474698,
      "grad_norm": 0.678123414516449,
      "learning_rate": 4.775798069784707e-05,
      "loss": 0.8759,
      "step": 610
    },
    {
      "epoch": 0.1380615710070701,
      "grad_norm": 0.8091370463371277,
      "learning_rate": 4.772086117297699e-05,
      "loss": 0.8746,
      "step": 620
    },
    {
      "epoch": 0.14028837053944218,
      "grad_norm": 0.926754355430603,
      "learning_rate": 4.7683741648106905e-05,
      "loss": 0.9014,
      "step": 630
    },
    {
      "epoch": 0.14251517007181427,
      "grad_norm": 4.353663921356201,
      "learning_rate": 4.764662212323682e-05,
      "loss": 0.8385,
      "step": 640
    },
    {
      "epoch": 0.1447419696041864,
      "grad_norm": 1.3799912929534912,
      "learning_rate": 4.7609502598366745e-05,
      "loss": 0.8629,
      "step": 650
    },
    {
      "epoch": 0.14696876913655849,
      "grad_norm": 0.7781888246536255,
      "learning_rate": 4.757238307349666e-05,
      "loss": 0.8991,
      "step": 660
    },
    {
      "epoch": 0.14919556866893058,
      "grad_norm": 0.8200317025184631,
      "learning_rate": 4.753526354862658e-05,
      "loss": 0.8609,
      "step": 670
    },
    {
      "epoch": 0.15142236820130267,
      "grad_norm": 0.6744457483291626,
      "learning_rate": 4.7498144023756495e-05,
      "loss": 0.8248,
      "step": 680
    },
    {
      "epoch": 0.15364916773367476,
      "grad_norm": 0.7905731201171875,
      "learning_rate": 4.746102449888641e-05,
      "loss": 0.8547,
      "step": 690
    },
    {
      "epoch": 0.15587596726604688,
      "grad_norm": 1.2735360860824585,
      "learning_rate": 4.7423904974016335e-05,
      "loss": 0.8211,
      "step": 700
    },
    {
      "epoch": 0.15810276679841898,
      "grad_norm": 0.8646710515022278,
      "learning_rate": 4.738678544914625e-05,
      "loss": 0.8157,
      "step": 710
    },
    {
      "epoch": 0.16032956633079107,
      "grad_norm": 0.9809786677360535,
      "learning_rate": 4.734966592427617e-05,
      "loss": 0.8961,
      "step": 720
    },
    {
      "epoch": 0.16255636586316316,
      "grad_norm": 0.7350757122039795,
      "learning_rate": 4.731254639940609e-05,
      "loss": 0.8594,
      "step": 730
    },
    {
      "epoch": 0.16478316539553528,
      "grad_norm": 0.6866456270217896,
      "learning_rate": 4.727542687453601e-05,
      "loss": 0.9001,
      "step": 740
    },
    {
      "epoch": 0.16700996492790737,
      "grad_norm": 0.787811279296875,
      "learning_rate": 4.7238307349665925e-05,
      "loss": 0.881,
      "step": 750
    },
    {
      "epoch": 0.16923676446027947,
      "grad_norm": 0.7467749714851379,
      "learning_rate": 4.720118782479585e-05,
      "loss": 0.8412,
      "step": 760
    },
    {
      "epoch": 0.17146356399265156,
      "grad_norm": 0.6176684498786926,
      "learning_rate": 4.7164068299925765e-05,
      "loss": 0.8473,
      "step": 770
    },
    {
      "epoch": 0.17369036352502365,
      "grad_norm": 0.6569315791130066,
      "learning_rate": 4.712694877505568e-05,
      "loss": 0.8383,
      "step": 780
    },
    {
      "epoch": 0.17591716305739577,
      "grad_norm": 0.8072376847267151,
      "learning_rate": 4.7089829250185605e-05,
      "loss": 0.8789,
      "step": 790
    },
    {
      "epoch": 0.17814396258976786,
      "grad_norm": 0.6588357090950012,
      "learning_rate": 4.705270972531552e-05,
      "loss": 0.8787,
      "step": 800
    },
    {
      "epoch": 0.18037076212213995,
      "grad_norm": 0.6959137916564941,
      "learning_rate": 4.701559020044544e-05,
      "loss": 0.8931,
      "step": 810
    },
    {
      "epoch": 0.18259756165451205,
      "grad_norm": 0.745698094367981,
      "learning_rate": 4.6978470675575354e-05,
      "loss": 0.882,
      "step": 820
    },
    {
      "epoch": 0.18482436118688414,
      "grad_norm": 2.9723730087280273,
      "learning_rate": 4.694135115070527e-05,
      "loss": 0.8735,
      "step": 830
    },
    {
      "epoch": 0.18705116071925626,
      "grad_norm": 0.677076518535614,
      "learning_rate": 4.6904231625835194e-05,
      "loss": 0.8367,
      "step": 840
    },
    {
      "epoch": 0.18927796025162835,
      "grad_norm": 0.6991994380950928,
      "learning_rate": 4.686711210096511e-05,
      "loss": 0.8088,
      "step": 850
    },
    {
      "epoch": 0.19150475978400044,
      "grad_norm": 0.9346654415130615,
      "learning_rate": 4.682999257609503e-05,
      "loss": 0.8087,
      "step": 860
    },
    {
      "epoch": 0.19373155931637254,
      "grad_norm": 0.7003084421157837,
      "learning_rate": 4.6792873051224944e-05,
      "loss": 0.8358,
      "step": 870
    },
    {
      "epoch": 0.19595835884874463,
      "grad_norm": 0.8519902229309082,
      "learning_rate": 4.675575352635487e-05,
      "loss": 0.8739,
      "step": 880
    },
    {
      "epoch": 0.19818515838111675,
      "grad_norm": 0.7072235345840454,
      "learning_rate": 4.6718634001484784e-05,
      "loss": 0.8281,
      "step": 890
    },
    {
      "epoch": 0.20041195791348884,
      "grad_norm": 0.801732063293457,
      "learning_rate": 4.66815144766147e-05,
      "loss": 0.836,
      "step": 900
    },
    {
      "epoch": 0.20263875744586093,
      "grad_norm": 0.7367526888847351,
      "learning_rate": 4.664439495174462e-05,
      "loss": 0.854,
      "step": 910
    },
    {
      "epoch": 0.20486555697823303,
      "grad_norm": 0.7225454449653625,
      "learning_rate": 4.6607275426874534e-05,
      "loss": 0.8081,
      "step": 920
    },
    {
      "epoch": 0.20709235651060512,
      "grad_norm": 0.6516729593276978,
      "learning_rate": 4.657015590200446e-05,
      "loss": 0.8845,
      "step": 930
    },
    {
      "epoch": 0.20931915604297724,
      "grad_norm": 0.7376064658164978,
      "learning_rate": 4.6533036377134374e-05,
      "loss": 0.8441,
      "step": 940
    },
    {
      "epoch": 0.21154595557534933,
      "grad_norm": 0.7833684682846069,
      "learning_rate": 4.649591685226429e-05,
      "loss": 0.8342,
      "step": 950
    },
    {
      "epoch": 0.21377275510772142,
      "grad_norm": 0.6526098847389221,
      "learning_rate": 4.645879732739421e-05,
      "loss": 0.8669,
      "step": 960
    },
    {
      "epoch": 0.21599955464009352,
      "grad_norm": 0.6942539215087891,
      "learning_rate": 4.6421677802524124e-05,
      "loss": 0.8077,
      "step": 970
    },
    {
      "epoch": 0.21822635417246564,
      "grad_norm": 0.6662600636482239,
      "learning_rate": 4.638455827765405e-05,
      "loss": 0.817,
      "step": 980
    },
    {
      "epoch": 0.22045315370483773,
      "grad_norm": 0.8437987565994263,
      "learning_rate": 4.6347438752783964e-05,
      "loss": 0.7712,
      "step": 990
    },
    {
      "epoch": 0.22267995323720982,
      "grad_norm": 0.6703783869743347,
      "learning_rate": 4.631031922791389e-05,
      "loss": 0.9164,
      "step": 1000
    },
    {
      "epoch": 0.2249067527695819,
      "grad_norm": 0.8478041291236877,
      "learning_rate": 4.6273199703043804e-05,
      "loss": 0.8229,
      "step": 1010
    },
    {
      "epoch": 0.227133552301954,
      "grad_norm": 0.7436451315879822,
      "learning_rate": 4.623608017817373e-05,
      "loss": 0.8701,
      "step": 1020
    },
    {
      "epoch": 0.22936035183432613,
      "grad_norm": 0.7374935746192932,
      "learning_rate": 4.6198960653303644e-05,
      "loss": 0.7971,
      "step": 1030
    },
    {
      "epoch": 0.23158715136669822,
      "grad_norm": 0.7013046145439148,
      "learning_rate": 4.616184112843356e-05,
      "loss": 0.8461,
      "step": 1040
    },
    {
      "epoch": 0.2338139508990703,
      "grad_norm": 0.7092753648757935,
      "learning_rate": 4.612472160356348e-05,
      "loss": 0.8498,
      "step": 1050
    },
    {
      "epoch": 0.2360407504314424,
      "grad_norm": 0.7591295838356018,
      "learning_rate": 4.6087602078693394e-05,
      "loss": 0.8356,
      "step": 1060
    },
    {
      "epoch": 0.2382675499638145,
      "grad_norm": 0.6636714339256287,
      "learning_rate": 4.605048255382332e-05,
      "loss": 0.8549,
      "step": 1070
    },
    {
      "epoch": 0.24049434949618662,
      "grad_norm": 0.7400960326194763,
      "learning_rate": 4.6013363028953234e-05,
      "loss": 0.798,
      "step": 1080
    },
    {
      "epoch": 0.2427211490285587,
      "grad_norm": 1.0903042554855347,
      "learning_rate": 4.597624350408315e-05,
      "loss": 0.8455,
      "step": 1090
    },
    {
      "epoch": 0.2449479485609308,
      "grad_norm": 0.7878203392028809,
      "learning_rate": 4.593912397921307e-05,
      "loss": 0.8747,
      "step": 1100
    },
    {
      "epoch": 0.2471747480933029,
      "grad_norm": 0.6804738640785217,
      "learning_rate": 4.5902004454342984e-05,
      "loss": 0.8411,
      "step": 1110
    },
    {
      "epoch": 0.24940154762567499,
      "grad_norm": 0.6426119804382324,
      "learning_rate": 4.586488492947291e-05,
      "loss": 0.8442,
      "step": 1120
    },
    {
      "epoch": 0.2516283471580471,
      "grad_norm": 0.6578150391578674,
      "learning_rate": 4.5827765404602824e-05,
      "loss": 0.7918,
      "step": 1130
    },
    {
      "epoch": 0.2538551466904192,
      "grad_norm": 0.7880244255065918,
      "learning_rate": 4.579064587973274e-05,
      "loss": 0.8067,
      "step": 1140
    },
    {
      "epoch": 0.2560819462227913,
      "grad_norm": 0.6983262300491333,
      "learning_rate": 4.575352635486266e-05,
      "loss": 0.7968,
      "step": 1150
    },
    {
      "epoch": 0.2583087457551634,
      "grad_norm": 0.6907840371131897,
      "learning_rate": 4.571640682999258e-05,
      "loss": 0.8031,
      "step": 1160
    },
    {
      "epoch": 0.2605355452875355,
      "grad_norm": 0.7575370669364929,
      "learning_rate": 4.56792873051225e-05,
      "loss": 0.8409,
      "step": 1170
    },
    {
      "epoch": 0.26276234481990757,
      "grad_norm": 0.794827938079834,
      "learning_rate": 4.5642167780252414e-05,
      "loss": 0.8693,
      "step": 1180
    },
    {
      "epoch": 0.26498914435227966,
      "grad_norm": 0.6923088431358337,
      "learning_rate": 4.560504825538233e-05,
      "loss": 0.7995,
      "step": 1190
    },
    {
      "epoch": 0.2672159438846518,
      "grad_norm": 0.696804404258728,
      "learning_rate": 4.556792873051225e-05,
      "loss": 0.8463,
      "step": 1200
    },
    {
      "epoch": 0.2694427434170239,
      "grad_norm": 0.7573562860488892,
      "learning_rate": 4.553080920564217e-05,
      "loss": 0.8309,
      "step": 1210
    },
    {
      "epoch": 0.271669542949396,
      "grad_norm": 0.6451741456985474,
      "learning_rate": 4.549368968077209e-05,
      "loss": 0.8209,
      "step": 1220
    },
    {
      "epoch": 0.2738963424817681,
      "grad_norm": 0.7017320394515991,
      "learning_rate": 4.5456570155902003e-05,
      "loss": 0.8372,
      "step": 1230
    },
    {
      "epoch": 0.2761231420141402,
      "grad_norm": 0.6698085069656372,
      "learning_rate": 4.541945063103193e-05,
      "loss": 0.7797,
      "step": 1240
    },
    {
      "epoch": 0.27834994154651227,
      "grad_norm": 0.6951372623443604,
      "learning_rate": 4.5382331106161843e-05,
      "loss": 0.8187,
      "step": 1250
    },
    {
      "epoch": 0.28057674107888436,
      "grad_norm": 0.8200973868370056,
      "learning_rate": 4.534521158129176e-05,
      "loss": 0.833,
      "step": 1260
    },
    {
      "epoch": 0.28280354061125645,
      "grad_norm": 0.631298840045929,
      "learning_rate": 4.5308092056421683e-05,
      "loss": 0.7871,
      "step": 1270
    },
    {
      "epoch": 0.28503034014362855,
      "grad_norm": 0.6630624532699585,
      "learning_rate": 4.52709725315516e-05,
      "loss": 0.7948,
      "step": 1280
    },
    {
      "epoch": 0.2872571396760007,
      "grad_norm": 0.6238565444946289,
      "learning_rate": 4.523385300668152e-05,
      "loss": 0.8016,
      "step": 1290
    },
    {
      "epoch": 0.2894839392083728,
      "grad_norm": 0.6584715247154236,
      "learning_rate": 4.519673348181144e-05,
      "loss": 0.8731,
      "step": 1300
    },
    {
      "epoch": 0.2917107387407449,
      "grad_norm": 0.8977395296096802,
      "learning_rate": 4.515961395694136e-05,
      "loss": 0.8144,
      "step": 1310
    },
    {
      "epoch": 0.29393753827311697,
      "grad_norm": 0.6978199481964111,
      "learning_rate": 4.512249443207127e-05,
      "loss": 0.8174,
      "step": 1320
    },
    {
      "epoch": 0.29616433780548906,
      "grad_norm": 0.6409092545509338,
      "learning_rate": 4.508537490720119e-05,
      "loss": 0.8021,
      "step": 1330
    },
    {
      "epoch": 0.29839113733786116,
      "grad_norm": 0.8609493970870972,
      "learning_rate": 4.5048255382331106e-05,
      "loss": 0.7829,
      "step": 1340
    },
    {
      "epoch": 0.30061793687023325,
      "grad_norm": 0.7369710803031921,
      "learning_rate": 4.501113585746103e-05,
      "loss": 0.8421,
      "step": 1350
    },
    {
      "epoch": 0.30284473640260534,
      "grad_norm": 0.7300055623054504,
      "learning_rate": 4.4974016332590946e-05,
      "loss": 0.7873,
      "step": 1360
    },
    {
      "epoch": 0.30507153593497743,
      "grad_norm": 0.7607771158218384,
      "learning_rate": 4.493689680772086e-05,
      "loss": 0.7896,
      "step": 1370
    },
    {
      "epoch": 0.3072983354673495,
      "grad_norm": 1.4170583486557007,
      "learning_rate": 4.489977728285078e-05,
      "loss": 0.8424,
      "step": 1380
    },
    {
      "epoch": 0.3095251349997217,
      "grad_norm": 0.6714330315589905,
      "learning_rate": 4.4862657757980696e-05,
      "loss": 0.855,
      "step": 1390
    },
    {
      "epoch": 0.31175193453209377,
      "grad_norm": 0.7294443249702454,
      "learning_rate": 4.482553823311062e-05,
      "loss": 0.8245,
      "step": 1400
    },
    {
      "epoch": 0.31397873406446586,
      "grad_norm": 0.7518985271453857,
      "learning_rate": 4.4788418708240536e-05,
      "loss": 0.8336,
      "step": 1410
    },
    {
      "epoch": 0.31620553359683795,
      "grad_norm": 0.6664482951164246,
      "learning_rate": 4.475129918337045e-05,
      "loss": 0.7807,
      "step": 1420
    },
    {
      "epoch": 0.31843233312921004,
      "grad_norm": 0.6482394337654114,
      "learning_rate": 4.471417965850037e-05,
      "loss": 0.8874,
      "step": 1430
    },
    {
      "epoch": 0.32065913266158214,
      "grad_norm": 0.6718713641166687,
      "learning_rate": 4.467706013363029e-05,
      "loss": 0.8159,
      "step": 1440
    },
    {
      "epoch": 0.32288593219395423,
      "grad_norm": 0.6440957188606262,
      "learning_rate": 4.463994060876021e-05,
      "loss": 0.8257,
      "step": 1450
    },
    {
      "epoch": 0.3251127317263263,
      "grad_norm": 0.6522163152694702,
      "learning_rate": 4.4602821083890126e-05,
      "loss": 0.8016,
      "step": 1460
    },
    {
      "epoch": 0.3273395312586984,
      "grad_norm": 0.7377733588218689,
      "learning_rate": 4.456570155902004e-05,
      "loss": 0.7671,
      "step": 1470
    },
    {
      "epoch": 0.32956633079107056,
      "grad_norm": 0.7059469223022461,
      "learning_rate": 4.452858203414996e-05,
      "loss": 0.8558,
      "step": 1480
    },
    {
      "epoch": 0.33179313032344265,
      "grad_norm": 0.9343034625053406,
      "learning_rate": 4.449146250927988e-05,
      "loss": 0.8756,
      "step": 1490
    },
    {
      "epoch": 0.33401992985581475,
      "grad_norm": 0.7538800239562988,
      "learning_rate": 4.44543429844098e-05,
      "loss": 0.834,
      "step": 1500
    },
    {
      "epoch": 0.33624672938818684,
      "grad_norm": 0.7416929602622986,
      "learning_rate": 4.441722345953972e-05,
      "loss": 0.8529,
      "step": 1510
    },
    {
      "epoch": 0.33847352892055893,
      "grad_norm": 0.9571720361709595,
      "learning_rate": 4.438010393466964e-05,
      "loss": 0.7881,
      "step": 1520
    },
    {
      "epoch": 0.340700328452931,
      "grad_norm": 0.7944339513778687,
      "learning_rate": 4.4342984409799556e-05,
      "loss": 0.8649,
      "step": 1530
    },
    {
      "epoch": 0.3429271279853031,
      "grad_norm": 0.7286670804023743,
      "learning_rate": 4.430586488492948e-05,
      "loss": 0.8588,
      "step": 1540
    },
    {
      "epoch": 0.3451539275176752,
      "grad_norm": 0.7111908793449402,
      "learning_rate": 4.4268745360059396e-05,
      "loss": 0.8119,
      "step": 1550
    },
    {
      "epoch": 0.3473807270500473,
      "grad_norm": 0.6998295187950134,
      "learning_rate": 4.423162583518931e-05,
      "loss": 0.8154,
      "step": 1560
    },
    {
      "epoch": 0.3496075265824194,
      "grad_norm": 0.9531648755073547,
      "learning_rate": 4.419450631031923e-05,
      "loss": 0.8525,
      "step": 1570
    },
    {
      "epoch": 0.35183432611479154,
      "grad_norm": 0.7525128722190857,
      "learning_rate": 4.415738678544915e-05,
      "loss": 0.834,
      "step": 1580
    },
    {
      "epoch": 0.35406112564716363,
      "grad_norm": 0.7109634876251221,
      "learning_rate": 4.412026726057907e-05,
      "loss": 0.753,
      "step": 1590
    },
    {
      "epoch": 0.3562879251795357,
      "grad_norm": 1.1111856698989868,
      "learning_rate": 4.4083147735708986e-05,
      "loss": 0.8008,
      "step": 1600
    },
    {
      "epoch": 0.3585147247119078,
      "grad_norm": 0.7478974461555481,
      "learning_rate": 4.40460282108389e-05,
      "loss": 0.8082,
      "step": 1610
    },
    {
      "epoch": 0.3607415242442799,
      "grad_norm": 0.6461352109909058,
      "learning_rate": 4.400890868596882e-05,
      "loss": 0.8042,
      "step": 1620
    },
    {
      "epoch": 0.362968323776652,
      "grad_norm": 0.7143762707710266,
      "learning_rate": 4.397178916109874e-05,
      "loss": 0.8345,
      "step": 1630
    },
    {
      "epoch": 0.3651951233090241,
      "grad_norm": 0.7176993489265442,
      "learning_rate": 4.393466963622866e-05,
      "loss": 0.8206,
      "step": 1640
    },
    {
      "epoch": 0.3674219228413962,
      "grad_norm": 0.7796996831893921,
      "learning_rate": 4.3897550111358576e-05,
      "loss": 0.8568,
      "step": 1650
    },
    {
      "epoch": 0.3696487223737683,
      "grad_norm": 0.9770617485046387,
      "learning_rate": 4.386043058648849e-05,
      "loss": 0.8178,
      "step": 1660
    },
    {
      "epoch": 0.3718755219061404,
      "grad_norm": 0.7524321675300598,
      "learning_rate": 4.3823311061618416e-05,
      "loss": 0.7688,
      "step": 1670
    },
    {
      "epoch": 0.3741023214385125,
      "grad_norm": 0.66427081823349,
      "learning_rate": 4.378619153674833e-05,
      "loss": 0.7748,
      "step": 1680
    },
    {
      "epoch": 0.3763291209708846,
      "grad_norm": 0.6998381614685059,
      "learning_rate": 4.374907201187825e-05,
      "loss": 0.8024,
      "step": 1690
    },
    {
      "epoch": 0.3785559205032567,
      "grad_norm": 0.8316366672515869,
      "learning_rate": 4.3711952487008166e-05,
      "loss": 0.8579,
      "step": 1700
    },
    {
      "epoch": 0.3807827200356288,
      "grad_norm": 0.7071395516395569,
      "learning_rate": 4.367483296213808e-05,
      "loss": 0.834,
      "step": 1710
    },
    {
      "epoch": 0.3830095195680009,
      "grad_norm": 0.7640641927719116,
      "learning_rate": 4.3637713437268006e-05,
      "loss": 0.826,
      "step": 1720
    },
    {
      "epoch": 0.385236319100373,
      "grad_norm": 0.618716299533844,
      "learning_rate": 4.360059391239792e-05,
      "loss": 0.8029,
      "step": 1730
    },
    {
      "epoch": 0.3874631186327451,
      "grad_norm": 0.654655396938324,
      "learning_rate": 4.356347438752784e-05,
      "loss": 0.8222,
      "step": 1740
    },
    {
      "epoch": 0.38968991816511717,
      "grad_norm": 0.9483233690261841,
      "learning_rate": 4.352635486265776e-05,
      "loss": 0.8111,
      "step": 1750
    },
    {
      "epoch": 0.39191671769748926,
      "grad_norm": 0.7688679695129395,
      "learning_rate": 4.348923533778768e-05,
      "loss": 0.8,
      "step": 1760
    },
    {
      "epoch": 0.3941435172298614,
      "grad_norm": 0.8375850915908813,
      "learning_rate": 4.3452115812917595e-05,
      "loss": 0.8441,
      "step": 1770
    },
    {
      "epoch": 0.3963703167622335,
      "grad_norm": 0.6778483986854553,
      "learning_rate": 4.341499628804752e-05,
      "loss": 0.7888,
      "step": 1780
    },
    {
      "epoch": 0.3985971162946056,
      "grad_norm": 0.8983689546585083,
      "learning_rate": 4.3377876763177435e-05,
      "loss": 0.8562,
      "step": 1790
    },
    {
      "epoch": 0.4008239158269777,
      "grad_norm": 0.6855500936508179,
      "learning_rate": 4.334075723830735e-05,
      "loss": 0.8024,
      "step": 1800
    },
    {
      "epoch": 0.4030507153593498,
      "grad_norm": 0.6814900040626526,
      "learning_rate": 4.3303637713437275e-05,
      "loss": 0.8442,
      "step": 1810
    },
    {
      "epoch": 0.40527751489172187,
      "grad_norm": 0.780815064907074,
      "learning_rate": 4.326651818856719e-05,
      "loss": 0.8298,
      "step": 1820
    },
    {
      "epoch": 0.40750431442409396,
      "grad_norm": 0.6919480562210083,
      "learning_rate": 4.322939866369711e-05,
      "loss": 0.7904,
      "step": 1830
    },
    {
      "epoch": 0.40973111395646605,
      "grad_norm": 0.6545960903167725,
      "learning_rate": 4.3192279138827025e-05,
      "loss": 0.7879,
      "step": 1840
    },
    {
      "epoch": 0.41195791348883815,
      "grad_norm": 0.8691038489341736,
      "learning_rate": 4.315515961395694e-05,
      "loss": 0.8049,
      "step": 1850
    },
    {
      "epoch": 0.41418471302121024,
      "grad_norm": 0.7181137204170227,
      "learning_rate": 4.3118040089086865e-05,
      "loss": 0.8332,
      "step": 1860
    },
    {
      "epoch": 0.4164115125535824,
      "grad_norm": 0.7269111275672913,
      "learning_rate": 4.308092056421678e-05,
      "loss": 0.7895,
      "step": 1870
    },
    {
      "epoch": 0.4186383120859545,
      "grad_norm": 0.6590062975883484,
      "learning_rate": 4.30438010393467e-05,
      "loss": 0.7683,
      "step": 1880
    },
    {
      "epoch": 0.42086511161832657,
      "grad_norm": 1.020476222038269,
      "learning_rate": 4.3006681514476615e-05,
      "loss": 0.8026,
      "step": 1890
    },
    {
      "epoch": 0.42309191115069866,
      "grad_norm": 0.6869522333145142,
      "learning_rate": 4.296956198960653e-05,
      "loss": 0.7871,
      "step": 1900
    },
    {
      "epoch": 0.42531871068307076,
      "grad_norm": 0.9387830495834351,
      "learning_rate": 4.2932442464736455e-05,
      "loss": 0.8076,
      "step": 1910
    },
    {
      "epoch": 0.42754551021544285,
      "grad_norm": 0.5979350209236145,
      "learning_rate": 4.289532293986637e-05,
      "loss": 0.7627,
      "step": 1920
    },
    {
      "epoch": 0.42977230974781494,
      "grad_norm": 0.7753580212593079,
      "learning_rate": 4.285820341499629e-05,
      "loss": 0.7845,
      "step": 1930
    },
    {
      "epoch": 0.43199910928018703,
      "grad_norm": 0.8202956914901733,
      "learning_rate": 4.2821083890126205e-05,
      "loss": 0.8411,
      "step": 1940
    },
    {
      "epoch": 0.4342259088125591,
      "grad_norm": 0.8086866736412048,
      "learning_rate": 4.278396436525613e-05,
      "loss": 0.7979,
      "step": 1950
    },
    {
      "epoch": 0.4364527083449313,
      "grad_norm": 0.7219944000244141,
      "learning_rate": 4.2746844840386045e-05,
      "loss": 0.793,
      "step": 1960
    },
    {
      "epoch": 0.43867950787730337,
      "grad_norm": 0.6085410118103027,
      "learning_rate": 4.270972531551596e-05,
      "loss": 0.7491,
      "step": 1970
    },
    {
      "epoch": 0.44090630740967546,
      "grad_norm": 0.7105334997177124,
      "learning_rate": 4.267260579064588e-05,
      "loss": 0.8141,
      "step": 1980
    },
    {
      "epoch": 0.44313310694204755,
      "grad_norm": 0.7104158401489258,
      "learning_rate": 4.2635486265775795e-05,
      "loss": 0.779,
      "step": 1990
    },
    {
      "epoch": 0.44535990647441964,
      "grad_norm": 0.9089052677154541,
      "learning_rate": 4.259836674090572e-05,
      "loss": 0.7729,
      "step": 2000
    },
    {
      "epoch": 0.44758670600679173,
      "grad_norm": 0.7012033462524414,
      "learning_rate": 4.2561247216035635e-05,
      "loss": 0.7549,
      "step": 2010
    },
    {
      "epoch": 0.4498135055391638,
      "grad_norm": 0.6492276191711426,
      "learning_rate": 4.252412769116556e-05,
      "loss": 0.7689,
      "step": 2020
    },
    {
      "epoch": 0.4520403050715359,
      "grad_norm": 0.7100054025650024,
      "learning_rate": 4.2487008166295475e-05,
      "loss": 0.839,
      "step": 2030
    },
    {
      "epoch": 0.454267104603908,
      "grad_norm": 0.7473520040512085,
      "learning_rate": 4.244988864142539e-05,
      "loss": 0.809,
      "step": 2040
    },
    {
      "epoch": 0.4564939041362801,
      "grad_norm": 0.7570422291755676,
      "learning_rate": 4.2412769116555315e-05,
      "loss": 0.7503,
      "step": 2050
    },
    {
      "epoch": 0.45872070366865225,
      "grad_norm": 0.7055118083953857,
      "learning_rate": 4.237564959168523e-05,
      "loss": 0.7993,
      "step": 2060
    },
    {
      "epoch": 0.46094750320102434,
      "grad_norm": 0.6947019696235657,
      "learning_rate": 4.233853006681515e-05,
      "loss": 0.7779,
      "step": 2070
    },
    {
      "epoch": 0.46317430273339644,
      "grad_norm": 0.7151405811309814,
      "learning_rate": 4.2301410541945065e-05,
      "loss": 0.7294,
      "step": 2080
    },
    {
      "epoch": 0.46540110226576853,
      "grad_norm": 0.6660397052764893,
      "learning_rate": 4.226429101707499e-05,
      "loss": 0.772,
      "step": 2090
    },
    {
      "epoch": 0.4676279017981406,
      "grad_norm": 0.7595598697662354,
      "learning_rate": 4.2227171492204905e-05,
      "loss": 0.8156,
      "step": 2100
    },
    {
      "epoch": 0.4698547013305127,
      "grad_norm": 0.7563270926475525,
      "learning_rate": 4.219005196733482e-05,
      "loss": 0.8322,
      "step": 2110
    },
    {
      "epoch": 0.4720815008628848,
      "grad_norm": 0.686993420124054,
      "learning_rate": 4.215293244246474e-05,
      "loss": 0.7619,
      "step": 2120
    },
    {
      "epoch": 0.4743083003952569,
      "grad_norm": 0.6029719710350037,
      "learning_rate": 4.2115812917594655e-05,
      "loss": 0.7416,
      "step": 2130
    },
    {
      "epoch": 0.476535099927629,
      "grad_norm": 0.6777060031890869,
      "learning_rate": 4.207869339272458e-05,
      "loss": 0.7809,
      "step": 2140
    },
    {
      "epoch": 0.47876189946000114,
      "grad_norm": 0.7185437083244324,
      "learning_rate": 4.2041573867854495e-05,
      "loss": 0.8035,
      "step": 2150
    },
    {
      "epoch": 0.48098869899237323,
      "grad_norm": 0.5672807693481445,
      "learning_rate": 4.200445434298441e-05,
      "loss": 0.7868,
      "step": 2160
    },
    {
      "epoch": 0.4832154985247453,
      "grad_norm": 0.6727445125579834,
      "learning_rate": 4.196733481811433e-05,
      "loss": 0.8173,
      "step": 2170
    },
    {
      "epoch": 0.4854422980571174,
      "grad_norm": 0.6938126683235168,
      "learning_rate": 4.1930215293244244e-05,
      "loss": 0.8264,
      "step": 2180
    },
    {
      "epoch": 0.4876690975894895,
      "grad_norm": 0.7368951439857483,
      "learning_rate": 4.189309576837417e-05,
      "loss": 0.801,
      "step": 2190
    },
    {
      "epoch": 0.4898958971218616,
      "grad_norm": 0.658383846282959,
      "learning_rate": 4.1855976243504084e-05,
      "loss": 0.8408,
      "step": 2200
    },
    {
      "epoch": 0.4921226966542337,
      "grad_norm": 0.7029894590377808,
      "learning_rate": 4.1818856718634e-05,
      "loss": 0.8034,
      "step": 2210
    },
    {
      "epoch": 0.4943494961866058,
      "grad_norm": 0.6685906052589417,
      "learning_rate": 4.178173719376392e-05,
      "loss": 0.7739,
      "step": 2220
    },
    {
      "epoch": 0.4965762957189779,
      "grad_norm": 2.280322313308716,
      "learning_rate": 4.174461766889384e-05,
      "loss": 0.8152,
      "step": 2230
    },
    {
      "epoch": 0.49880309525134997,
      "grad_norm": 0.7587133646011353,
      "learning_rate": 4.170749814402376e-05,
      "loss": 0.8312,
      "step": 2240
    },
    {
      "epoch": 0.5010298947837221,
      "grad_norm": 0.8166228532791138,
      "learning_rate": 4.1670378619153674e-05,
      "loss": 0.7716,
      "step": 2250
    },
    {
      "epoch": 0.5032566943160942,
      "grad_norm": 0.7293276190757751,
      "learning_rate": 4.163325909428359e-05,
      "loss": 0.8222,
      "step": 2260
    },
    {
      "epoch": 0.5054834938484662,
      "grad_norm": 0.8192786574363708,
      "learning_rate": 4.1596139569413514e-05,
      "loss": 0.8183,
      "step": 2270
    },
    {
      "epoch": 0.5077102933808384,
      "grad_norm": 0.7387670278549194,
      "learning_rate": 4.155902004454343e-05,
      "loss": 0.8284,
      "step": 2280
    },
    {
      "epoch": 0.5099370929132105,
      "grad_norm": 0.7269425392150879,
      "learning_rate": 4.1521900519673354e-05,
      "loss": 0.7525,
      "step": 2290
    },
    {
      "epoch": 0.5121638924455826,
      "grad_norm": 0.8584343194961548,
      "learning_rate": 4.148478099480327e-05,
      "loss": 0.7555,
      "step": 2300
    },
    {
      "epoch": 0.5143906919779547,
      "grad_norm": 0.8560596108436584,
      "learning_rate": 4.144766146993319e-05,
      "loss": 0.784,
      "step": 2310
    },
    {
      "epoch": 0.5166174915103268,
      "grad_norm": 0.7598962187767029,
      "learning_rate": 4.1410541945063104e-05,
      "loss": 0.7737,
      "step": 2320
    },
    {
      "epoch": 0.5188442910426989,
      "grad_norm": 0.7083260416984558,
      "learning_rate": 4.137342242019303e-05,
      "loss": 0.7844,
      "step": 2330
    },
    {
      "epoch": 0.521071090575071,
      "grad_norm": 0.6428332328796387,
      "learning_rate": 4.1336302895322944e-05,
      "loss": 0.7745,
      "step": 2340
    },
    {
      "epoch": 0.5232978901074431,
      "grad_norm": 0.6496397256851196,
      "learning_rate": 4.129918337045286e-05,
      "loss": 0.8311,
      "step": 2350
    },
    {
      "epoch": 0.5255246896398151,
      "grad_norm": 0.7074921727180481,
      "learning_rate": 4.126206384558278e-05,
      "loss": 0.7968,
      "step": 2360
    },
    {
      "epoch": 0.5277514891721873,
      "grad_norm": 0.7327256798744202,
      "learning_rate": 4.12249443207127e-05,
      "loss": 0.784,
      "step": 2370
    },
    {
      "epoch": 0.5299782887045593,
      "grad_norm": 0.852788507938385,
      "learning_rate": 4.118782479584262e-05,
      "loss": 0.7082,
      "step": 2380
    },
    {
      "epoch": 0.5322050882369315,
      "grad_norm": 0.7504939436912537,
      "learning_rate": 4.1150705270972534e-05,
      "loss": 0.7727,
      "step": 2390
    },
    {
      "epoch": 0.5344318877693036,
      "grad_norm": 0.6697701811790466,
      "learning_rate": 4.111358574610245e-05,
      "loss": 0.7686,
      "step": 2400
    },
    {
      "epoch": 0.5366586873016757,
      "grad_norm": 0.6892404556274414,
      "learning_rate": 4.107646622123237e-05,
      "loss": 0.8158,
      "step": 2410
    },
    {
      "epoch": 0.5388854868340478,
      "grad_norm": 0.655302882194519,
      "learning_rate": 4.103934669636229e-05,
      "loss": 0.7528,
      "step": 2420
    },
    {
      "epoch": 0.5411122863664198,
      "grad_norm": 0.8057594299316406,
      "learning_rate": 4.100222717149221e-05,
      "loss": 0.7492,
      "step": 2430
    },
    {
      "epoch": 0.543339085898792,
      "grad_norm": 0.6283117532730103,
      "learning_rate": 4.0965107646622124e-05,
      "loss": 0.8368,
      "step": 2440
    },
    {
      "epoch": 0.545565885431164,
      "grad_norm": 0.8545389771461487,
      "learning_rate": 4.092798812175204e-05,
      "loss": 0.7383,
      "step": 2450
    },
    {
      "epoch": 0.5477926849635362,
      "grad_norm": 0.6171378493309021,
      "learning_rate": 4.089086859688196e-05,
      "loss": 0.8124,
      "step": 2460
    },
    {
      "epoch": 0.5500194844959082,
      "grad_norm": 0.5719888806343079,
      "learning_rate": 4.085374907201188e-05,
      "loss": 0.7971,
      "step": 2470
    },
    {
      "epoch": 0.5522462840282804,
      "grad_norm": 0.6965490579605103,
      "learning_rate": 4.08166295471418e-05,
      "loss": 0.7874,
      "step": 2480
    },
    {
      "epoch": 0.5544730835606525,
      "grad_norm": 0.6804815530776978,
      "learning_rate": 4.0779510022271714e-05,
      "loss": 0.8076,
      "step": 2490
    },
    {
      "epoch": 0.5566998830930245,
      "grad_norm": 0.6706594228744507,
      "learning_rate": 4.074239049740163e-05,
      "loss": 0.8603,
      "step": 2500
    },
    {
      "epoch": 0.5589266826253967,
      "grad_norm": 2.0305209159851074,
      "learning_rate": 4.0705270972531554e-05,
      "loss": 0.7806,
      "step": 2510
    },
    {
      "epoch": 0.5611534821577687,
      "grad_norm": 0.7460853457450867,
      "learning_rate": 4.066815144766147e-05,
      "loss": 0.7988,
      "step": 2520
    },
    {
      "epoch": 0.5633802816901409,
      "grad_norm": 0.5787326097488403,
      "learning_rate": 4.0631031922791394e-05,
      "loss": 0.7966,
      "step": 2530
    },
    {
      "epoch": 0.5656070812225129,
      "grad_norm": 2.5604617595672607,
      "learning_rate": 4.059391239792131e-05,
      "loss": 0.8188,
      "step": 2540
    },
    {
      "epoch": 0.5678338807548851,
      "grad_norm": 0.8051695823669434,
      "learning_rate": 4.055679287305123e-05,
      "loss": 0.7759,
      "step": 2550
    },
    {
      "epoch": 0.5700606802872571,
      "grad_norm": 0.7184392809867859,
      "learning_rate": 4.051967334818115e-05,
      "loss": 0.7856,
      "step": 2560
    },
    {
      "epoch": 0.5722874798196292,
      "grad_norm": 0.8522855043411255,
      "learning_rate": 4.048255382331107e-05,
      "loss": 0.7925,
      "step": 2570
    },
    {
      "epoch": 0.5745142793520014,
      "grad_norm": 0.8828080296516418,
      "learning_rate": 4.0445434298440984e-05,
      "loss": 0.7694,
      "step": 2580
    },
    {
      "epoch": 0.5767410788843734,
      "grad_norm": 0.6874622106552124,
      "learning_rate": 4.04083147735709e-05,
      "loss": 0.7519,
      "step": 2590
    },
    {
      "epoch": 0.5789678784167456,
      "grad_norm": 0.7318276166915894,
      "learning_rate": 4.037119524870082e-05,
      "loss": 0.7582,
      "step": 2600
    },
    {
      "epoch": 0.5811946779491176,
      "grad_norm": 0.755773663520813,
      "learning_rate": 4.033407572383074e-05,
      "loss": 0.8015,
      "step": 2610
    },
    {
      "epoch": 0.5834214774814898,
      "grad_norm": 0.6979315876960754,
      "learning_rate": 4.029695619896066e-05,
      "loss": 0.7786,
      "step": 2620
    },
    {
      "epoch": 0.5856482770138618,
      "grad_norm": 0.7359354496002197,
      "learning_rate": 4.0259836674090573e-05,
      "loss": 0.8076,
      "step": 2630
    },
    {
      "epoch": 0.5878750765462339,
      "grad_norm": 0.668100893497467,
      "learning_rate": 4.022271714922049e-05,
      "loss": 0.8438,
      "step": 2640
    },
    {
      "epoch": 0.590101876078606,
      "grad_norm": 0.8134401440620422,
      "learning_rate": 4.0185597624350413e-05,
      "loss": 0.7913,
      "step": 2650
    },
    {
      "epoch": 0.5923286756109781,
      "grad_norm": 0.9259719848632812,
      "learning_rate": 4.014847809948033e-05,
      "loss": 0.8159,
      "step": 2660
    },
    {
      "epoch": 0.5945554751433502,
      "grad_norm": 0.7048337459564209,
      "learning_rate": 4.011135857461025e-05,
      "loss": 0.8015,
      "step": 2670
    },
    {
      "epoch": 0.5967822746757223,
      "grad_norm": 0.573860764503479,
      "learning_rate": 4.007423904974016e-05,
      "loss": 0.7909,
      "step": 2680
    },
    {
      "epoch": 0.5990090742080945,
      "grad_norm": 0.7733274698257446,
      "learning_rate": 4.003711952487008e-05,
      "loss": 0.7974,
      "step": 2690
    },
    {
      "epoch": 0.6012358737404665,
      "grad_norm": 0.6977517008781433,
      "learning_rate": 4e-05,
      "loss": 0.7758,
      "step": 2700
    },
    {
      "epoch": 0.6034626732728386,
      "grad_norm": 0.6547228097915649,
      "learning_rate": 3.996288047512992e-05,
      "loss": 0.7522,
      "step": 2710
    },
    {
      "epoch": 0.6056894728052107,
      "grad_norm": 1.3059492111206055,
      "learning_rate": 3.9925760950259837e-05,
      "loss": 0.7651,
      "step": 2720
    },
    {
      "epoch": 0.6079162723375828,
      "grad_norm": 0.6593403220176697,
      "learning_rate": 3.988864142538975e-05,
      "loss": 0.7968,
      "step": 2730
    },
    {
      "epoch": 0.6101430718699549,
      "grad_norm": 0.6629544496536255,
      "learning_rate": 3.985152190051967e-05,
      "loss": 0.7593,
      "step": 2740
    },
    {
      "epoch": 0.612369871402327,
      "grad_norm": 0.5940393805503845,
      "learning_rate": 3.981440237564959e-05,
      "loss": 0.7803,
      "step": 2750
    },
    {
      "epoch": 0.614596670934699,
      "grad_norm": 0.7447066307067871,
      "learning_rate": 3.977728285077951e-05,
      "loss": 0.8299,
      "step": 2760
    },
    {
      "epoch": 0.6168234704670712,
      "grad_norm": 0.6387094855308533,
      "learning_rate": 3.9740163325909426e-05,
      "loss": 0.8481,
      "step": 2770
    },
    {
      "epoch": 0.6190502699994433,
      "grad_norm": 0.7371793985366821,
      "learning_rate": 3.970304380103935e-05,
      "loss": 0.8,
      "step": 2780
    },
    {
      "epoch": 0.6212770695318154,
      "grad_norm": 0.7847250699996948,
      "learning_rate": 3.9665924276169266e-05,
      "loss": 0.7707,
      "step": 2790
    },
    {
      "epoch": 0.6235038690641875,
      "grad_norm": 0.7467342615127563,
      "learning_rate": 3.962880475129919e-05,
      "loss": 0.8048,
      "step": 2800
    },
    {
      "epoch": 0.6257306685965596,
      "grad_norm": 0.7905979752540588,
      "learning_rate": 3.9591685226429106e-05,
      "loss": 0.8008,
      "step": 2810
    },
    {
      "epoch": 0.6279574681289317,
      "grad_norm": 0.700637698173523,
      "learning_rate": 3.955456570155902e-05,
      "loss": 0.8122,
      "step": 2820
    },
    {
      "epoch": 0.6301842676613038,
      "grad_norm": 0.7621083855628967,
      "learning_rate": 3.951744617668894e-05,
      "loss": 0.7609,
      "step": 2830
    },
    {
      "epoch": 0.6324110671936759,
      "grad_norm": 8.803159713745117,
      "learning_rate": 3.948032665181886e-05,
      "loss": 0.7879,
      "step": 2840
    },
    {
      "epoch": 0.6346378667260479,
      "grad_norm": 0.6792770624160767,
      "learning_rate": 3.944320712694878e-05,
      "loss": 0.8165,
      "step": 2850
    },
    {
      "epoch": 0.6368646662584201,
      "grad_norm": 0.7798673510551453,
      "learning_rate": 3.9406087602078696e-05,
      "loss": 0.7921,
      "step": 2860
    },
    {
      "epoch": 0.6390914657907922,
      "grad_norm": 0.6280562281608582,
      "learning_rate": 3.936896807720861e-05,
      "loss": 0.7446,
      "step": 2870
    },
    {
      "epoch": 0.6413182653231643,
      "grad_norm": 0.6055284738540649,
      "learning_rate": 3.933184855233853e-05,
      "loss": 0.8184,
      "step": 2880
    },
    {
      "epoch": 0.6435450648555364,
      "grad_norm": 0.7886499762535095,
      "learning_rate": 3.929472902746845e-05,
      "loss": 0.7498,
      "step": 2890
    },
    {
      "epoch": 0.6457718643879085,
      "grad_norm": 0.6780925393104553,
      "learning_rate": 3.925760950259837e-05,
      "loss": 0.7811,
      "step": 2900
    },
    {
      "epoch": 0.6479986639202806,
      "grad_norm": 0.724295437335968,
      "learning_rate": 3.9220489977728286e-05,
      "loss": 0.8121,
      "step": 2910
    },
    {
      "epoch": 0.6502254634526526,
      "grad_norm": 0.6874496936798096,
      "learning_rate": 3.91833704528582e-05,
      "loss": 0.7617,
      "step": 2920
    },
    {
      "epoch": 0.6524522629850248,
      "grad_norm": 0.6242699027061462,
      "learning_rate": 3.9146250927988126e-05,
      "loss": 0.7723,
      "step": 2930
    },
    {
      "epoch": 0.6546790625173968,
      "grad_norm": 0.7456614971160889,
      "learning_rate": 3.910913140311804e-05,
      "loss": 0.7811,
      "step": 2940
    },
    {
      "epoch": 0.656905862049769,
      "grad_norm": 0.6661070585250854,
      "learning_rate": 3.907201187824796e-05,
      "loss": 0.7846,
      "step": 2950
    },
    {
      "epoch": 0.6591326615821411,
      "grad_norm": 0.8642137050628662,
      "learning_rate": 3.9034892353377876e-05,
      "loss": 0.7259,
      "step": 2960
    },
    {
      "epoch": 0.6613594611145132,
      "grad_norm": 0.8097719550132751,
      "learning_rate": 3.899777282850779e-05,
      "loss": 0.8203,
      "step": 2970
    },
    {
      "epoch": 0.6635862606468853,
      "grad_norm": 0.6355460286140442,
      "learning_rate": 3.8960653303637716e-05,
      "loss": 0.7287,
      "step": 2980
    },
    {
      "epoch": 0.6658130601792573,
      "grad_norm": 0.7074278593063354,
      "learning_rate": 3.892353377876763e-05,
      "loss": 0.7737,
      "step": 2990
    },
    {
      "epoch": 0.6680398597116295,
      "grad_norm": 0.6791340112686157,
      "learning_rate": 3.888641425389755e-05,
      "loss": 0.7646,
      "step": 3000
    },
    {
      "epoch": 0.6702666592440015,
      "grad_norm": 0.6448999643325806,
      "learning_rate": 3.8849294729027466e-05,
      "loss": 0.7512,
      "step": 3010
    },
    {
      "epoch": 0.6724934587763737,
      "grad_norm": 0.7630389928817749,
      "learning_rate": 3.881217520415739e-05,
      "loss": 0.8141,
      "step": 3020
    },
    {
      "epoch": 0.6747202583087457,
      "grad_norm": 0.6759735941886902,
      "learning_rate": 3.8775055679287306e-05,
      "loss": 0.8234,
      "step": 3030
    },
    {
      "epoch": 0.6769470578411179,
      "grad_norm": 0.6670734286308289,
      "learning_rate": 3.873793615441723e-05,
      "loss": 0.8069,
      "step": 3040
    },
    {
      "epoch": 0.6791738573734899,
      "grad_norm": 0.6575620770454407,
      "learning_rate": 3.8700816629547146e-05,
      "loss": 0.7709,
      "step": 3050
    },
    {
      "epoch": 0.681400656905862,
      "grad_norm": 0.6539946794509888,
      "learning_rate": 3.866369710467706e-05,
      "loss": 0.7877,
      "step": 3060
    },
    {
      "epoch": 0.6836274564382342,
      "grad_norm": 0.6690184473991394,
      "learning_rate": 3.8626577579806986e-05,
      "loss": 0.7522,
      "step": 3070
    },
    {
      "epoch": 0.6858542559706062,
      "grad_norm": 0.7216694951057434,
      "learning_rate": 3.85894580549369e-05,
      "loss": 0.7603,
      "step": 3080
    },
    {
      "epoch": 0.6880810555029784,
      "grad_norm": 0.7648894190788269,
      "learning_rate": 3.855233853006682e-05,
      "loss": 0.811,
      "step": 3090
    },
    {
      "epoch": 0.6903078550353504,
      "grad_norm": 0.7328987121582031,
      "learning_rate": 3.8515219005196736e-05,
      "loss": 0.7741,
      "step": 3100
    },
    {
      "epoch": 0.6925346545677226,
      "grad_norm": 0.7459956407546997,
      "learning_rate": 3.847809948032665e-05,
      "loss": 0.7884,
      "step": 3110
    },
    {
      "epoch": 0.6947614541000946,
      "grad_norm": 0.6362810134887695,
      "learning_rate": 3.8440979955456576e-05,
      "loss": 0.7344,
      "step": 3120
    },
    {
      "epoch": 0.6969882536324667,
      "grad_norm": 0.6561038494110107,
      "learning_rate": 3.840386043058649e-05,
      "loss": 0.7558,
      "step": 3130
    },
    {
      "epoch": 0.6992150531648388,
      "grad_norm": 0.9147181510925293,
      "learning_rate": 3.836674090571641e-05,
      "loss": 0.7853,
      "step": 3140
    },
    {
      "epoch": 0.7014418526972109,
      "grad_norm": 0.6737825870513916,
      "learning_rate": 3.8329621380846325e-05,
      "loss": 0.7448,
      "step": 3150
    },
    {
      "epoch": 0.7036686522295831,
      "grad_norm": 0.6877556443214417,
      "learning_rate": 3.829250185597624e-05,
      "loss": 0.7288,
      "step": 3160
    },
    {
      "epoch": 0.7058954517619551,
      "grad_norm": 0.7050380110740662,
      "learning_rate": 3.8255382331106165e-05,
      "loss": 0.8026,
      "step": 3170
    },
    {
      "epoch": 0.7081222512943273,
      "grad_norm": 0.796515941619873,
      "learning_rate": 3.821826280623608e-05,
      "loss": 0.8353,
      "step": 3180
    },
    {
      "epoch": 0.7103490508266993,
      "grad_norm": 0.6327263712882996,
      "learning_rate": 3.8181143281366e-05,
      "loss": 0.7268,
      "step": 3190
    },
    {
      "epoch": 0.7125758503590714,
      "grad_norm": 0.6851264238357544,
      "learning_rate": 3.8144023756495915e-05,
      "loss": 0.8087,
      "step": 3200
    },
    {
      "epoch": 0.7148026498914435,
      "grad_norm": 0.6874608993530273,
      "learning_rate": 3.810690423162584e-05,
      "loss": 0.7727,
      "step": 3210
    },
    {
      "epoch": 0.7170294494238156,
      "grad_norm": 0.6210469007492065,
      "learning_rate": 3.8069784706755755e-05,
      "loss": 0.7404,
      "step": 3220
    },
    {
      "epoch": 0.7192562489561877,
      "grad_norm": 0.740814208984375,
      "learning_rate": 3.803266518188567e-05,
      "loss": 0.7665,
      "step": 3230
    },
    {
      "epoch": 0.7214830484885598,
      "grad_norm": 0.6756761074066162,
      "learning_rate": 3.799554565701559e-05,
      "loss": 0.8221,
      "step": 3240
    },
    {
      "epoch": 0.723709848020932,
      "grad_norm": 0.7952859997749329,
      "learning_rate": 3.7958426132145505e-05,
      "loss": 0.7695,
      "step": 3250
    },
    {
      "epoch": 0.725936647553304,
      "grad_norm": 0.7019531726837158,
      "learning_rate": 3.792130660727543e-05,
      "loss": 0.7995,
      "step": 3260
    },
    {
      "epoch": 0.7281634470856762,
      "grad_norm": 0.6301530599594116,
      "learning_rate": 3.7884187082405345e-05,
      "loss": 0.7877,
      "step": 3270
    },
    {
      "epoch": 0.7303902466180482,
      "grad_norm": 0.7785789370536804,
      "learning_rate": 3.784706755753526e-05,
      "loss": 0.7957,
      "step": 3280
    },
    {
      "epoch": 0.7326170461504203,
      "grad_norm": 0.8285890221595764,
      "learning_rate": 3.7809948032665185e-05,
      "loss": 0.8104,
      "step": 3290
    },
    {
      "epoch": 0.7348438456827924,
      "grad_norm": 0.7997207045555115,
      "learning_rate": 3.77728285077951e-05,
      "loss": 0.8054,
      "step": 3300
    },
    {
      "epoch": 0.7370706452151645,
      "grad_norm": 0.6809173822402954,
      "learning_rate": 3.7735708982925025e-05,
      "loss": 0.8252,
      "step": 3310
    },
    {
      "epoch": 0.7392974447475366,
      "grad_norm": 0.700628936290741,
      "learning_rate": 3.769858945805494e-05,
      "loss": 0.76,
      "step": 3320
    },
    {
      "epoch": 0.7415242442799087,
      "grad_norm": 0.8331026434898376,
      "learning_rate": 3.766146993318486e-05,
      "loss": 0.7567,
      "step": 3330
    },
    {
      "epoch": 0.7437510438122809,
      "grad_norm": 0.7035760879516602,
      "learning_rate": 3.7624350408314775e-05,
      "loss": 0.7752,
      "step": 3340
    },
    {
      "epoch": 0.7459778433446529,
      "grad_norm": 0.788544237613678,
      "learning_rate": 3.75872308834447e-05,
      "loss": 0.7413,
      "step": 3350
    },
    {
      "epoch": 0.748204642877025,
      "grad_norm": 0.712313711643219,
      "learning_rate": 3.7550111358574615e-05,
      "loss": 0.7764,
      "step": 3360
    },
    {
      "epoch": 0.7504314424093971,
      "grad_norm": 0.7507556676864624,
      "learning_rate": 3.751299183370453e-05,
      "loss": 0.8007,
      "step": 3370
    },
    {
      "epoch": 0.7526582419417692,
      "grad_norm": 0.802642285823822,
      "learning_rate": 3.747587230883445e-05,
      "loss": 0.747,
      "step": 3380
    },
    {
      "epoch": 0.7548850414741413,
      "grad_norm": 0.6613129377365112,
      "learning_rate": 3.7438752783964365e-05,
      "loss": 0.8169,
      "step": 3390
    },
    {
      "epoch": 0.7571118410065134,
      "grad_norm": 0.8257374167442322,
      "learning_rate": 3.740163325909429e-05,
      "loss": 0.7931,
      "step": 3400
    },
    {
      "epoch": 0.7593386405388854,
      "grad_norm": 0.6796057820320129,
      "learning_rate": 3.7364513734224205e-05,
      "loss": 0.7822,
      "step": 3410
    },
    {
      "epoch": 0.7615654400712576,
      "grad_norm": 0.6576204299926758,
      "learning_rate": 3.732739420935412e-05,
      "loss": 0.7395,
      "step": 3420
    },
    {
      "epoch": 0.7637922396036296,
      "grad_norm": 1.0739728212356567,
      "learning_rate": 3.729027468448404e-05,
      "loss": 0.7522,
      "step": 3430
    },
    {
      "epoch": 0.7660190391360018,
      "grad_norm": 0.8052051067352295,
      "learning_rate": 3.7253155159613955e-05,
      "loss": 0.8264,
      "step": 3440
    },
    {
      "epoch": 0.7682458386683739,
      "grad_norm": 0.6842712759971619,
      "learning_rate": 3.721603563474388e-05,
      "loss": 0.791,
      "step": 3450
    },
    {
      "epoch": 0.770472638200746,
      "grad_norm": 0.6737287044525146,
      "learning_rate": 3.7178916109873795e-05,
      "loss": 0.8093,
      "step": 3460
    },
    {
      "epoch": 0.7726994377331181,
      "grad_norm": 0.759248673915863,
      "learning_rate": 3.714179658500371e-05,
      "loss": 0.7893,
      "step": 3470
    },
    {
      "epoch": 0.7749262372654901,
      "grad_norm": 0.6292998790740967,
      "learning_rate": 3.710467706013363e-05,
      "loss": 0.7856,
      "step": 3480
    },
    {
      "epoch": 0.7771530367978623,
      "grad_norm": 0.6696497201919556,
      "learning_rate": 3.706755753526355e-05,
      "loss": 0.7509,
      "step": 3490
    },
    {
      "epoch": 0.7793798363302343,
      "grad_norm": 0.7105752825737,
      "learning_rate": 3.703043801039347e-05,
      "loss": 0.795,
      "step": 3500
    },
    {
      "epoch": 0.7816066358626065,
      "grad_norm": 0.6961216926574707,
      "learning_rate": 3.6993318485523385e-05,
      "loss": 0.7756,
      "step": 3510
    },
    {
      "epoch": 0.7838334353949785,
      "grad_norm": 0.606775164604187,
      "learning_rate": 3.69561989606533e-05,
      "loss": 0.7514,
      "step": 3520
    },
    {
      "epoch": 0.7860602349273507,
      "grad_norm": 0.6096238493919373,
      "learning_rate": 3.6919079435783225e-05,
      "loss": 0.791,
      "step": 3530
    },
    {
      "epoch": 0.7882870344597228,
      "grad_norm": 0.8083302974700928,
      "learning_rate": 3.688195991091314e-05,
      "loss": 0.7737,
      "step": 3540
    },
    {
      "epoch": 0.7905138339920948,
      "grad_norm": 0.6300158500671387,
      "learning_rate": 3.6844840386043065e-05,
      "loss": 0.7815,
      "step": 3550
    },
    {
      "epoch": 0.792740633524467,
      "grad_norm": 0.6595794558525085,
      "learning_rate": 3.680772086117298e-05,
      "loss": 0.8048,
      "step": 3560
    },
    {
      "epoch": 0.794967433056839,
      "grad_norm": 0.7141900062561035,
      "learning_rate": 3.67706013363029e-05,
      "loss": 0.7439,
      "step": 3570
    },
    {
      "epoch": 0.7971942325892112,
      "grad_norm": 0.669205904006958,
      "learning_rate": 3.6733481811432814e-05,
      "loss": 0.7978,
      "step": 3580
    },
    {
      "epoch": 0.7994210321215832,
      "grad_norm": 0.6371734142303467,
      "learning_rate": 3.669636228656274e-05,
      "loss": 0.7563,
      "step": 3590
    },
    {
      "epoch": 0.8016478316539554,
      "grad_norm": 0.5727713108062744,
      "learning_rate": 3.6659242761692654e-05,
      "loss": 0.7914,
      "step": 3600
    },
    {
      "epoch": 0.8038746311863274,
      "grad_norm": 0.7765418887138367,
      "learning_rate": 3.662212323682257e-05,
      "loss": 0.7851,
      "step": 3610
    },
    {
      "epoch": 0.8061014307186996,
      "grad_norm": 0.6949277520179749,
      "learning_rate": 3.658500371195249e-05,
      "loss": 0.7677,
      "step": 3620
    },
    {
      "epoch": 0.8083282302510717,
      "grad_norm": 0.6383729577064514,
      "learning_rate": 3.654788418708241e-05,
      "loss": 0.748,
      "step": 3630
    },
    {
      "epoch": 0.8105550297834437,
      "grad_norm": 0.7018441557884216,
      "learning_rate": 3.651076466221233e-05,
      "loss": 0.809,
      "step": 3640
    },
    {
      "epoch": 0.8127818293158159,
      "grad_norm": 0.6552280783653259,
      "learning_rate": 3.6473645137342244e-05,
      "loss": 0.7803,
      "step": 3650
    },
    {
      "epoch": 0.8150086288481879,
      "grad_norm": 0.9277740120887756,
      "learning_rate": 3.643652561247216e-05,
      "loss": 0.7857,
      "step": 3660
    },
    {
      "epoch": 0.8172354283805601,
      "grad_norm": 0.6792328953742981,
      "learning_rate": 3.639940608760208e-05,
      "loss": 0.7207,
      "step": 3670
    },
    {
      "epoch": 0.8194622279129321,
      "grad_norm": 0.6959212422370911,
      "learning_rate": 3.6362286562732e-05,
      "loss": 0.7548,
      "step": 3680
    },
    {
      "epoch": 0.8216890274453043,
      "grad_norm": 0.8835271000862122,
      "learning_rate": 3.632516703786192e-05,
      "loss": 0.8111,
      "step": 3690
    },
    {
      "epoch": 0.8239158269776763,
      "grad_norm": 0.6635619401931763,
      "learning_rate": 3.6288047512991834e-05,
      "loss": 0.7928,
      "step": 3700
    },
    {
      "epoch": 0.8261426265100484,
      "grad_norm": 0.8723694682121277,
      "learning_rate": 3.625092798812175e-05,
      "loss": 0.7629,
      "step": 3710
    },
    {
      "epoch": 0.8283694260424205,
      "grad_norm": 0.8563607931137085,
      "learning_rate": 3.621380846325167e-05,
      "loss": 0.7944,
      "step": 3720
    },
    {
      "epoch": 0.8305962255747926,
      "grad_norm": 0.9253266453742981,
      "learning_rate": 3.617668893838159e-05,
      "loss": 0.78,
      "step": 3730
    },
    {
      "epoch": 0.8328230251071648,
      "grad_norm": 0.6863981485366821,
      "learning_rate": 3.613956941351151e-05,
      "loss": 0.7722,
      "step": 3740
    },
    {
      "epoch": 0.8350498246395368,
      "grad_norm": 0.8209280967712402,
      "learning_rate": 3.6102449888641424e-05,
      "loss": 0.7873,
      "step": 3750
    },
    {
      "epoch": 0.837276624171909,
      "grad_norm": 0.767092764377594,
      "learning_rate": 3.606533036377134e-05,
      "loss": 0.7569,
      "step": 3760
    },
    {
      "epoch": 0.839503423704281,
      "grad_norm": 0.6967658996582031,
      "learning_rate": 3.6028210838901264e-05,
      "loss": 0.7642,
      "step": 3770
    },
    {
      "epoch": 0.8417302232366531,
      "grad_norm": 0.7260239124298096,
      "learning_rate": 3.599109131403118e-05,
      "loss": 0.7726,
      "step": 3780
    },
    {
      "epoch": 0.8439570227690252,
      "grad_norm": 0.6867008209228516,
      "learning_rate": 3.59539717891611e-05,
      "loss": 0.7615,
      "step": 3790
    },
    {
      "epoch": 0.8461838223013973,
      "grad_norm": 0.6880731582641602,
      "learning_rate": 3.591685226429102e-05,
      "loss": 0.7761,
      "step": 3800
    },
    {
      "epoch": 0.8484106218337694,
      "grad_norm": 0.7300545573234558,
      "learning_rate": 3.587973273942094e-05,
      "loss": 0.7953,
      "step": 3810
    },
    {
      "epoch": 0.8506374213661415,
      "grad_norm": 0.6415327787399292,
      "learning_rate": 3.584261321455086e-05,
      "loss": 0.7579,
      "step": 3820
    },
    {
      "epoch": 0.8528642208985137,
      "grad_norm": 0.6598155498504639,
      "learning_rate": 3.580549368968078e-05,
      "loss": 0.8018,
      "step": 3830
    },
    {
      "epoch": 0.8550910204308857,
      "grad_norm": 0.7852761745452881,
      "learning_rate": 3.5768374164810694e-05,
      "loss": 0.7259,
      "step": 3840
    },
    {
      "epoch": 0.8573178199632578,
      "grad_norm": 0.7308691143989563,
      "learning_rate": 3.573125463994061e-05,
      "loss": 0.7682,
      "step": 3850
    },
    {
      "epoch": 0.8595446194956299,
      "grad_norm": 0.6323678493499756,
      "learning_rate": 3.569413511507053e-05,
      "loss": 0.7975,
      "step": 3860
    },
    {
      "epoch": 0.861771419028002,
      "grad_norm": 0.6828011870384216,
      "learning_rate": 3.565701559020045e-05,
      "loss": 0.7367,
      "step": 3870
    },
    {
      "epoch": 0.8639982185603741,
      "grad_norm": 0.9217970967292786,
      "learning_rate": 3.561989606533037e-05,
      "loss": 0.7661,
      "step": 3880
    },
    {
      "epoch": 0.8662250180927462,
      "grad_norm": 1.5877240896224976,
      "learning_rate": 3.5582776540460284e-05,
      "loss": 0.7525,
      "step": 3890
    },
    {
      "epoch": 0.8684518176251182,
      "grad_norm": 0.6875610947608948,
      "learning_rate": 3.55456570155902e-05,
      "loss": 0.7656,
      "step": 3900
    },
    {
      "epoch": 0.8706786171574904,
      "grad_norm": 0.9532701373100281,
      "learning_rate": 3.5508537490720124e-05,
      "loss": 0.7726,
      "step": 3910
    },
    {
      "epoch": 0.8729054166898625,
      "grad_norm": 0.7402509450912476,
      "learning_rate": 3.547141796585004e-05,
      "loss": 0.7301,
      "step": 3920
    },
    {
      "epoch": 0.8751322162222346,
      "grad_norm": 0.7378153204917908,
      "learning_rate": 3.543429844097996e-05,
      "loss": 0.7282,
      "step": 3930
    },
    {
      "epoch": 0.8773590157546067,
      "grad_norm": 0.6878129839897156,
      "learning_rate": 3.5397178916109874e-05,
      "loss": 0.7483,
      "step": 3940
    },
    {
      "epoch": 0.8795858152869788,
      "grad_norm": 0.7143961787223816,
      "learning_rate": 3.536005939123979e-05,
      "loss": 0.7005,
      "step": 3950
    },
    {
      "epoch": 0.8818126148193509,
      "grad_norm": 0.7292985320091248,
      "learning_rate": 3.5322939866369714e-05,
      "loss": 0.8304,
      "step": 3960
    },
    {
      "epoch": 0.884039414351723,
      "grad_norm": 0.783902645111084,
      "learning_rate": 3.528582034149963e-05,
      "loss": 0.7314,
      "step": 3970
    },
    {
      "epoch": 0.8862662138840951,
      "grad_norm": 0.7096127867698669,
      "learning_rate": 3.524870081662955e-05,
      "loss": 0.8015,
      "step": 3980
    },
    {
      "epoch": 0.8884930134164671,
      "grad_norm": 0.6818627715110779,
      "learning_rate": 3.5211581291759463e-05,
      "loss": 0.7485,
      "step": 3990
    },
    {
      "epoch": 0.8907198129488393,
      "grad_norm": 0.7425234913825989,
      "learning_rate": 3.517446176688938e-05,
      "loss": 0.7595,
      "step": 4000
    },
    {
      "epoch": 0.8929466124812114,
      "grad_norm": 0.6861529350280762,
      "learning_rate": 3.5137342242019303e-05,
      "loss": 0.7799,
      "step": 4010
    },
    {
      "epoch": 0.8951734120135835,
      "grad_norm": 0.8582069873809814,
      "learning_rate": 3.510022271714922e-05,
      "loss": 0.7584,
      "step": 4020
    },
    {
      "epoch": 0.8974002115459556,
      "grad_norm": 0.7569743990898132,
      "learning_rate": 3.506310319227914e-05,
      "loss": 0.7921,
      "step": 4030
    },
    {
      "epoch": 0.8996270110783277,
      "grad_norm": 0.7290728092193604,
      "learning_rate": 3.502598366740906e-05,
      "loss": 0.753,
      "step": 4040
    },
    {
      "epoch": 0.9018538106106998,
      "grad_norm": 0.69609534740448,
      "learning_rate": 3.498886414253898e-05,
      "loss": 0.7988,
      "step": 4050
    },
    {
      "epoch": 0.9040806101430718,
      "grad_norm": 0.6830891370773315,
      "learning_rate": 3.49517446176689e-05,
      "loss": 0.7211,
      "step": 4060
    },
    {
      "epoch": 0.906307409675444,
      "grad_norm": 0.7912219762802124,
      "learning_rate": 3.491462509279882e-05,
      "loss": 0.7961,
      "step": 4070
    },
    {
      "epoch": 0.908534209207816,
      "grad_norm": 0.800623893737793,
      "learning_rate": 3.487750556792873e-05,
      "loss": 0.7808,
      "step": 4080
    },
    {
      "epoch": 0.9107610087401882,
      "grad_norm": 0.6814238429069519,
      "learning_rate": 3.484038604305865e-05,
      "loss": 0.7826,
      "step": 4090
    },
    {
      "epoch": 0.9129878082725602,
      "grad_norm": 0.7463175058364868,
      "learning_rate": 3.480326651818857e-05,
      "loss": 0.7635,
      "step": 4100
    },
    {
      "epoch": 0.9152146078049324,
      "grad_norm": 0.9208093881607056,
      "learning_rate": 3.476614699331849e-05,
      "loss": 0.7316,
      "step": 4110
    },
    {
      "epoch": 0.9174414073373045,
      "grad_norm": 0.7359884977340698,
      "learning_rate": 3.4729027468448407e-05,
      "loss": 0.8479,
      "step": 4120
    },
    {
      "epoch": 0.9196682068696765,
      "grad_norm": 0.6869218945503235,
      "learning_rate": 3.469190794357832e-05,
      "loss": 0.7292,
      "step": 4130
    },
    {
      "epoch": 0.9218950064020487,
      "grad_norm": 0.7052565813064575,
      "learning_rate": 3.465478841870824e-05,
      "loss": 0.7001,
      "step": 4140
    },
    {
      "epoch": 0.9241218059344207,
      "grad_norm": 0.7681931257247925,
      "learning_rate": 3.461766889383816e-05,
      "loss": 0.7682,
      "step": 4150
    },
    {
      "epoch": 0.9263486054667929,
      "grad_norm": 0.9103145003318787,
      "learning_rate": 3.458054936896808e-05,
      "loss": 0.772,
      "step": 4160
    },
    {
      "epoch": 0.9285754049991649,
      "grad_norm": 0.6870903968811035,
      "learning_rate": 3.4543429844097996e-05,
      "loss": 0.7973,
      "step": 4170
    },
    {
      "epoch": 0.9308022045315371,
      "grad_norm": 1.0178725719451904,
      "learning_rate": 3.450631031922791e-05,
      "loss": 0.752,
      "step": 4180
    },
    {
      "epoch": 0.9330290040639091,
      "grad_norm": 0.6698091626167297,
      "learning_rate": 3.4469190794357836e-05,
      "loss": 0.7428,
      "step": 4190
    },
    {
      "epoch": 0.9352558035962812,
      "grad_norm": 0.7281084656715393,
      "learning_rate": 3.443207126948775e-05,
      "loss": 0.8094,
      "step": 4200
    },
    {
      "epoch": 0.9374826031286534,
      "grad_norm": 0.7008448839187622,
      "learning_rate": 3.439495174461767e-05,
      "loss": 0.7221,
      "step": 4210
    },
    {
      "epoch": 0.9397094026610254,
      "grad_norm": 0.7134460210800171,
      "learning_rate": 3.4357832219747586e-05,
      "loss": 0.7699,
      "step": 4220
    },
    {
      "epoch": 0.9419362021933976,
      "grad_norm": 0.6715124249458313,
      "learning_rate": 3.43207126948775e-05,
      "loss": 0.7546,
      "step": 4230
    },
    {
      "epoch": 0.9441630017257696,
      "grad_norm": 1.0243374109268188,
      "learning_rate": 3.4283593170007426e-05,
      "loss": 0.7344,
      "step": 4240
    },
    {
      "epoch": 0.9463898012581418,
      "grad_norm": 1.2377995252609253,
      "learning_rate": 3.424647364513734e-05,
      "loss": 0.7527,
      "step": 4250
    },
    {
      "epoch": 0.9486166007905138,
      "grad_norm": 0.6926962733268738,
      "learning_rate": 3.420935412026726e-05,
      "loss": 0.7304,
      "step": 4260
    },
    {
      "epoch": 0.950843400322886,
      "grad_norm": 0.6239506602287292,
      "learning_rate": 3.4172234595397176e-05,
      "loss": 0.8247,
      "step": 4270
    },
    {
      "epoch": 0.953070199855258,
      "grad_norm": 0.7747322916984558,
      "learning_rate": 3.41351150705271e-05,
      "loss": 0.7556,
      "step": 4280
    },
    {
      "epoch": 0.9552969993876301,
      "grad_norm": 0.7210913300514221,
      "learning_rate": 3.4097995545657016e-05,
      "loss": 0.7246,
      "step": 4290
    },
    {
      "epoch": 0.9575237989200023,
      "grad_norm": 0.6888320446014404,
      "learning_rate": 3.406087602078693e-05,
      "loss": 0.752,
      "step": 4300
    },
    {
      "epoch": 0.9597505984523743,
      "grad_norm": 0.7294943928718567,
      "learning_rate": 3.4023756495916856e-05,
      "loss": 0.7888,
      "step": 4310
    },
    {
      "epoch": 0.9619773979847465,
      "grad_norm": 0.6951174139976501,
      "learning_rate": 3.398663697104677e-05,
      "loss": 0.8193,
      "step": 4320
    },
    {
      "epoch": 0.9642041975171185,
      "grad_norm": 0.6213974952697754,
      "learning_rate": 3.3949517446176696e-05,
      "loss": 0.7641,
      "step": 4330
    },
    {
      "epoch": 0.9664309970494906,
      "grad_norm": 0.6668617725372314,
      "learning_rate": 3.391239792130661e-05,
      "loss": 0.7386,
      "step": 4340
    },
    {
      "epoch": 0.9686577965818627,
      "grad_norm": 0.645205020904541,
      "learning_rate": 3.387527839643653e-05,
      "loss": 0.7362,
      "step": 4350
    },
    {
      "epoch": 0.9708845961142348,
      "grad_norm": 0.699923038482666,
      "learning_rate": 3.3838158871566446e-05,
      "loss": 0.7715,
      "step": 4360
    },
    {
      "epoch": 0.9731113956466069,
      "grad_norm": 0.703048825263977,
      "learning_rate": 3.380103934669636e-05,
      "loss": 0.7628,
      "step": 4370
    },
    {
      "epoch": 0.975338195178979,
      "grad_norm": 0.7785279154777527,
      "learning_rate": 3.3763919821826286e-05,
      "loss": 0.7948,
      "step": 4380
    },
    {
      "epoch": 0.9775649947113512,
      "grad_norm": 0.8058567643165588,
      "learning_rate": 3.37268002969562e-05,
      "loss": 0.7722,
      "step": 4390
    },
    {
      "epoch": 0.9797917942437232,
      "grad_norm": 0.6850937604904175,
      "learning_rate": 3.368968077208612e-05,
      "loss": 0.7906,
      "step": 4400
    },
    {
      "epoch": 0.9820185937760953,
      "grad_norm": 2.1885929107666016,
      "learning_rate": 3.3652561247216036e-05,
      "loss": 0.7462,
      "step": 4410
    },
    {
      "epoch": 0.9842453933084674,
      "grad_norm": 0.9882069826126099,
      "learning_rate": 3.361544172234596e-05,
      "loss": 0.7911,
      "step": 4420
    },
    {
      "epoch": 0.9864721928408395,
      "grad_norm": 0.6949601769447327,
      "learning_rate": 3.3578322197475876e-05,
      "loss": 0.7804,
      "step": 4430
    },
    {
      "epoch": 0.9886989923732116,
      "grad_norm": 0.707123875617981,
      "learning_rate": 3.354120267260579e-05,
      "loss": 0.7838,
      "step": 4440
    },
    {
      "epoch": 0.9909257919055837,
      "grad_norm": 0.6848813891410828,
      "learning_rate": 3.350408314773571e-05,
      "loss": 0.7781,
      "step": 4450
    },
    {
      "epoch": 0.9931525914379558,
      "grad_norm": 0.8587812781333923,
      "learning_rate": 3.3466963622865626e-05,
      "loss": 0.7904,
      "step": 4460
    },
    {
      "epoch": 0.9953793909703279,
      "grad_norm": 0.7259647250175476,
      "learning_rate": 3.342984409799555e-05,
      "loss": 0.7299,
      "step": 4470
    },
    {
      "epoch": 0.9976061905026999,
      "grad_norm": 0.7113165259361267,
      "learning_rate": 3.3392724573125466e-05,
      "loss": 0.7965,
      "step": 4480
    },
    {
      "epoch": 0.9998329900350721,
      "grad_norm": 0.6624984741210938,
      "learning_rate": 3.335560504825538e-05,
      "loss": 0.7821,
      "step": 4490
    },
    {
      "epoch": 1.0022267995323721,
      "grad_norm": 0.7143515348434448,
      "learning_rate": 3.33184855233853e-05,
      "loss": 0.8461,
      "step": 4500
    },
    {
      "epoch": 1.0044535990647443,
      "grad_norm": 0.7154459953308105,
      "learning_rate": 3.3281365998515215e-05,
      "loss": 0.8059,
      "step": 4510
    },
    {
      "epoch": 1.0066803985971162,
      "grad_norm": 2.691899299621582,
      "learning_rate": 3.324424647364514e-05,
      "loss": 0.7776,
      "step": 4520
    },
    {
      "epoch": 1.0089071981294884,
      "grad_norm": 0.7136985063552856,
      "learning_rate": 3.3207126948775055e-05,
      "loss": 0.8545,
      "step": 4530
    },
    {
      "epoch": 1.0111339976618605,
      "grad_norm": 0.7616201639175415,
      "learning_rate": 3.317000742390497e-05,
      "loss": 0.8162,
      "step": 4540
    },
    {
      "epoch": 1.0133607971942327,
      "grad_norm": 0.7010557651519775,
      "learning_rate": 3.3132887899034895e-05,
      "loss": 0.7423,
      "step": 4550
    },
    {
      "epoch": 1.0155875967266046,
      "grad_norm": 0.654484212398529,
      "learning_rate": 3.309576837416481e-05,
      "loss": 0.7467,
      "step": 4560
    },
    {
      "epoch": 1.0178143962589767,
      "grad_norm": 0.6991159319877625,
      "learning_rate": 3.3058648849294735e-05,
      "loss": 0.7465,
      "step": 4570
    },
    {
      "epoch": 1.0200411957913489,
      "grad_norm": 0.6711081862449646,
      "learning_rate": 3.302152932442465e-05,
      "loss": 0.8011,
      "step": 4580
    },
    {
      "epoch": 1.022267995323721,
      "grad_norm": 0.7980421781539917,
      "learning_rate": 3.298440979955457e-05,
      "loss": 0.7777,
      "step": 4590
    },
    {
      "epoch": 1.0244947948560932,
      "grad_norm": 0.689415454864502,
      "learning_rate": 3.2947290274684485e-05,
      "loss": 0.7423,
      "step": 4600
    },
    {
      "epoch": 1.026721594388465,
      "grad_norm": 0.9466648101806641,
      "learning_rate": 3.291017074981441e-05,
      "loss": 0.7865,
      "step": 4610
    },
    {
      "epoch": 1.0289483939208373,
      "grad_norm": 0.688032329082489,
      "learning_rate": 3.2873051224944325e-05,
      "loss": 0.7749,
      "step": 4620
    },
    {
      "epoch": 1.0311751934532094,
      "grad_norm": 0.7142589092254639,
      "learning_rate": 3.283593170007424e-05,
      "loss": 0.7595,
      "step": 4630
    },
    {
      "epoch": 1.0334019929855816,
      "grad_norm": 0.6642049551010132,
      "learning_rate": 3.279881217520416e-05,
      "loss": 0.7716,
      "step": 4640
    },
    {
      "epoch": 1.0356287925179535,
      "grad_norm": 0.6633667349815369,
      "learning_rate": 3.2761692650334075e-05,
      "loss": 0.7459,
      "step": 4650
    },
    {
      "epoch": 1.0378555920503256,
      "grad_norm": 0.6967892646789551,
      "learning_rate": 3.2724573125464e-05,
      "loss": 0.73,
      "step": 4660
    },
    {
      "epoch": 1.0400823915826978,
      "grad_norm": 0.7672275900840759,
      "learning_rate": 3.2687453600593915e-05,
      "loss": 0.7531,
      "step": 4670
    },
    {
      "epoch": 1.04230919111507,
      "grad_norm": 0.7030031085014343,
      "learning_rate": 3.265033407572383e-05,
      "loss": 0.7462,
      "step": 4680
    },
    {
      "epoch": 1.044535990647442,
      "grad_norm": 0.7004479169845581,
      "learning_rate": 3.261321455085375e-05,
      "loss": 0.7629,
      "step": 4690
    },
    {
      "epoch": 1.046762790179814,
      "grad_norm": 0.6538515090942383,
      "learning_rate": 3.257609502598367e-05,
      "loss": 0.7463,
      "step": 4700
    },
    {
      "epoch": 1.0489895897121861,
      "grad_norm": 0.7720393538475037,
      "learning_rate": 3.253897550111359e-05,
      "loss": 0.7632,
      "step": 4710
    },
    {
      "epoch": 1.0512163892445583,
      "grad_norm": 1.586764931678772,
      "learning_rate": 3.2501855976243505e-05,
      "loss": 0.7936,
      "step": 4720
    },
    {
      "epoch": 1.0534431887769304,
      "grad_norm": 0.6987286806106567,
      "learning_rate": 3.246473645137342e-05,
      "loss": 0.7233,
      "step": 4730
    },
    {
      "epoch": 1.0556699883093024,
      "grad_norm": 0.7586960792541504,
      "learning_rate": 3.242761692650334e-05,
      "loss": 0.8074,
      "step": 4740
    },
    {
      "epoch": 1.0578967878416745,
      "grad_norm": 0.7060577869415283,
      "learning_rate": 3.239049740163326e-05,
      "loss": 0.7327,
      "step": 4750
    },
    {
      "epoch": 1.0601235873740467,
      "grad_norm": 0.816336989402771,
      "learning_rate": 3.235337787676318e-05,
      "loss": 0.7562,
      "step": 4760
    },
    {
      "epoch": 1.0623503869064188,
      "grad_norm": 0.7195945978164673,
      "learning_rate": 3.2316258351893095e-05,
      "loss": 0.7723,
      "step": 4770
    },
    {
      "epoch": 1.064577186438791,
      "grad_norm": 0.755379855632782,
      "learning_rate": 3.227913882702301e-05,
      "loss": 0.7533,
      "step": 4780
    },
    {
      "epoch": 1.0668039859711629,
      "grad_norm": 0.7197352051734924,
      "learning_rate": 3.2242019302152935e-05,
      "loss": 0.719,
      "step": 4790
    },
    {
      "epoch": 1.069030785503535,
      "grad_norm": 0.7655177116394043,
      "learning_rate": 3.220489977728285e-05,
      "loss": 0.7317,
      "step": 4800
    },
    {
      "epoch": 1.0712575850359072,
      "grad_norm": 0.7688267230987549,
      "learning_rate": 3.216778025241277e-05,
      "loss": 0.7599,
      "step": 4810
    },
    {
      "epoch": 1.0734843845682793,
      "grad_norm": 0.6580440998077393,
      "learning_rate": 3.213066072754269e-05,
      "loss": 0.7225,
      "step": 4820
    },
    {
      "epoch": 1.0757111841006513,
      "grad_norm": 0.6196346879005432,
      "learning_rate": 3.209354120267261e-05,
      "loss": 0.7307,
      "step": 4830
    },
    {
      "epoch": 1.0779379836330234,
      "grad_norm": 0.7233759164810181,
      "learning_rate": 3.205642167780253e-05,
      "loss": 0.7237,
      "step": 4840
    },
    {
      "epoch": 1.0801647831653955,
      "grad_norm": 0.6277560591697693,
      "learning_rate": 3.201930215293245e-05,
      "loss": 0.7618,
      "step": 4850
    },
    {
      "epoch": 1.0823915826977677,
      "grad_norm": 0.7314577102661133,
      "learning_rate": 3.1982182628062365e-05,
      "loss": 0.8012,
      "step": 4860
    },
    {
      "epoch": 1.0846183822301398,
      "grad_norm": 0.6676052212715149,
      "learning_rate": 3.194506310319228e-05,
      "loss": 0.8043,
      "step": 4870
    },
    {
      "epoch": 1.0868451817625118,
      "grad_norm": 0.6984238624572754,
      "learning_rate": 3.19079435783222e-05,
      "loss": 0.7403,
      "step": 4880
    },
    {
      "epoch": 1.089071981294884,
      "grad_norm": 0.6546226739883423,
      "learning_rate": 3.187082405345212e-05,
      "loss": 0.7537,
      "step": 4890
    },
    {
      "epoch": 1.091298780827256,
      "grad_norm": 0.6232191920280457,
      "learning_rate": 3.183370452858204e-05,
      "loss": 0.7729,
      "step": 4900
    },
    {
      "epoch": 1.0935255803596282,
      "grad_norm": 0.7083796262741089,
      "learning_rate": 3.1796585003711955e-05,
      "loss": 0.7714,
      "step": 4910
    },
    {
      "epoch": 1.0957523798920001,
      "grad_norm": 0.6631678938865662,
      "learning_rate": 3.175946547884187e-05,
      "loss": 0.7568,
      "step": 4920
    },
    {
      "epoch": 1.0979791794243723,
      "grad_norm": 0.7866506576538086,
      "learning_rate": 3.172234595397179e-05,
      "loss": 0.7585,
      "step": 4930
    },
    {
      "epoch": 1.1002059789567444,
      "grad_norm": 0.676672637462616,
      "learning_rate": 3.168522642910171e-05,
      "loss": 0.7331,
      "step": 4940
    },
    {
      "epoch": 1.1024327784891166,
      "grad_norm": 0.7112308144569397,
      "learning_rate": 3.164810690423163e-05,
      "loss": 0.7397,
      "step": 4950
    },
    {
      "epoch": 1.1046595780214887,
      "grad_norm": 0.760333776473999,
      "learning_rate": 3.1610987379361544e-05,
      "loss": 0.7571,
      "step": 4960
    },
    {
      "epoch": 1.1068863775538607,
      "grad_norm": 0.654082179069519,
      "learning_rate": 3.157386785449146e-05,
      "loss": 0.7361,
      "step": 4970
    },
    {
      "epoch": 1.1091131770862328,
      "grad_norm": 0.843571126461029,
      "learning_rate": 3.1536748329621384e-05,
      "loss": 0.7737,
      "step": 4980
    },
    {
      "epoch": 1.111339976618605,
      "grad_norm": 0.7044945955276489,
      "learning_rate": 3.14996288047513e-05,
      "loss": 0.7662,
      "step": 4990
    },
    {
      "epoch": 1.113566776150977,
      "grad_norm": 0.6966602802276611,
      "learning_rate": 3.146250927988122e-05,
      "loss": 0.7629,
      "step": 5000
    },
    {
      "epoch": 1.115793575683349,
      "grad_norm": 0.7071329951286316,
      "learning_rate": 3.1425389755011134e-05,
      "loss": 0.7245,
      "step": 5010
    },
    {
      "epoch": 1.1180203752157212,
      "grad_norm": 0.7406683564186096,
      "learning_rate": 3.138827023014105e-05,
      "loss": 0.7633,
      "step": 5020
    },
    {
      "epoch": 1.1202471747480933,
      "grad_norm": 0.710303544998169,
      "learning_rate": 3.1351150705270974e-05,
      "loss": 0.7806,
      "step": 5030
    },
    {
      "epoch": 1.1224739742804655,
      "grad_norm": 0.7815390229225159,
      "learning_rate": 3.131403118040089e-05,
      "loss": 0.7843,
      "step": 5040
    },
    {
      "epoch": 1.1247007738128374,
      "grad_norm": 0.7164521813392639,
      "learning_rate": 3.127691165553081e-05,
      "loss": 0.7592,
      "step": 5050
    },
    {
      "epoch": 1.1269275733452095,
      "grad_norm": 0.7878636121749878,
      "learning_rate": 3.123979213066073e-05,
      "loss": 0.7137,
      "step": 5060
    },
    {
      "epoch": 1.1291543728775817,
      "grad_norm": 0.7398771047592163,
      "learning_rate": 3.120267260579065e-05,
      "loss": 0.7342,
      "step": 5070
    },
    {
      "epoch": 1.1313811724099538,
      "grad_norm": 0.6209139823913574,
      "learning_rate": 3.116555308092057e-05,
      "loss": 0.7234,
      "step": 5080
    },
    {
      "epoch": 1.133607971942326,
      "grad_norm": 0.7064754962921143,
      "learning_rate": 3.112843355605049e-05,
      "loss": 0.7688,
      "step": 5090
    },
    {
      "epoch": 1.135834771474698,
      "grad_norm": 0.7669930458068848,
      "learning_rate": 3.1091314031180404e-05,
      "loss": 0.7942,
      "step": 5100
    },
    {
      "epoch": 1.13806157100707,
      "grad_norm": 0.7930243015289307,
      "learning_rate": 3.105419450631032e-05,
      "loss": 0.7402,
      "step": 5110
    },
    {
      "epoch": 1.1402883705394422,
      "grad_norm": 0.6509525775909424,
      "learning_rate": 3.1017074981440244e-05,
      "loss": 0.7642,
      "step": 5120
    },
    {
      "epoch": 1.1425151700718144,
      "grad_norm": 0.7045754790306091,
      "learning_rate": 3.097995545657016e-05,
      "loss": 0.7825,
      "step": 5130
    },
    {
      "epoch": 1.1447419696041865,
      "grad_norm": 0.6569305062294006,
      "learning_rate": 3.094283593170008e-05,
      "loss": 0.7619,
      "step": 5140
    },
    {
      "epoch": 1.1469687691365584,
      "grad_norm": 0.7031239867210388,
      "learning_rate": 3.0905716406829994e-05,
      "loss": 0.7733,
      "step": 5150
    },
    {
      "epoch": 1.1491955686689306,
      "grad_norm": 0.6551156640052795,
      "learning_rate": 3.086859688195991e-05,
      "loss": 0.7734,
      "step": 5160
    },
    {
      "epoch": 1.1514223682013027,
      "grad_norm": 0.6863043308258057,
      "learning_rate": 3.0831477357089834e-05,
      "loss": 0.7657,
      "step": 5170
    },
    {
      "epoch": 1.1536491677336747,
      "grad_norm": 0.7301562428474426,
      "learning_rate": 3.079435783221975e-05,
      "loss": 0.7533,
      "step": 5180
    },
    {
      "epoch": 1.1558759672660468,
      "grad_norm": 0.7440469264984131,
      "learning_rate": 3.075723830734967e-05,
      "loss": 0.7915,
      "step": 5190
    },
    {
      "epoch": 1.158102766798419,
      "grad_norm": 0.7603288292884827,
      "learning_rate": 3.0720118782479584e-05,
      "loss": 0.8058,
      "step": 5200
    },
    {
      "epoch": 1.160329566330791,
      "grad_norm": 0.8806775808334351,
      "learning_rate": 3.06829992576095e-05,
      "loss": 0.7617,
      "step": 5210
    },
    {
      "epoch": 1.1625563658631632,
      "grad_norm": 0.725163459777832,
      "learning_rate": 3.0645879732739424e-05,
      "loss": 0.7468,
      "step": 5220
    },
    {
      "epoch": 1.1647831653955354,
      "grad_norm": 0.651526927947998,
      "learning_rate": 3.060876020786934e-05,
      "loss": 0.7021,
      "step": 5230
    },
    {
      "epoch": 1.1670099649279073,
      "grad_norm": 0.7306944727897644,
      "learning_rate": 3.057164068299926e-05,
      "loss": 0.7322,
      "step": 5240
    },
    {
      "epoch": 1.1692367644602795,
      "grad_norm": 0.6811608076095581,
      "learning_rate": 3.0534521158129174e-05,
      "loss": 0.7227,
      "step": 5250
    },
    {
      "epoch": 1.1714635639926516,
      "grad_norm": 0.8224678039550781,
      "learning_rate": 3.0497401633259097e-05,
      "loss": 0.6955,
      "step": 5260
    },
    {
      "epoch": 1.1736903635250235,
      "grad_norm": 0.6425427198410034,
      "learning_rate": 3.0460282108389017e-05,
      "loss": 0.7751,
      "step": 5270
    },
    {
      "epoch": 1.1759171630573957,
      "grad_norm": 0.7495797276496887,
      "learning_rate": 3.0423162583518934e-05,
      "loss": 0.7751,
      "step": 5280
    },
    {
      "epoch": 1.1781439625897678,
      "grad_norm": 0.7759863138198853,
      "learning_rate": 3.038604305864885e-05,
      "loss": 0.7609,
      "step": 5290
    },
    {
      "epoch": 1.18037076212214,
      "grad_norm": 0.8217105269432068,
      "learning_rate": 3.0348923533778767e-05,
      "loss": 0.7397,
      "step": 5300
    },
    {
      "epoch": 1.1825975616545121,
      "grad_norm": 0.6676408648490906,
      "learning_rate": 3.031180400890869e-05,
      "loss": 0.7394,
      "step": 5310
    },
    {
      "epoch": 1.184824361186884,
      "grad_norm": 0.7283023595809937,
      "learning_rate": 3.0274684484038607e-05,
      "loss": 0.7519,
      "step": 5320
    },
    {
      "epoch": 1.1870511607192562,
      "grad_norm": 1.248062014579773,
      "learning_rate": 3.0237564959168524e-05,
      "loss": 0.7422,
      "step": 5330
    },
    {
      "epoch": 1.1892779602516284,
      "grad_norm": 0.608057975769043,
      "learning_rate": 3.020044543429844e-05,
      "loss": 0.7394,
      "step": 5340
    },
    {
      "epoch": 1.1915047597840005,
      "grad_norm": 0.7110833525657654,
      "learning_rate": 3.0163325909428357e-05,
      "loss": 0.7011,
      "step": 5350
    },
    {
      "epoch": 1.1937315593163724,
      "grad_norm": 0.6818245053291321,
      "learning_rate": 3.012620638455828e-05,
      "loss": 0.7529,
      "step": 5360
    },
    {
      "epoch": 1.1959583588487446,
      "grad_norm": 0.685060977935791,
      "learning_rate": 3.0089086859688197e-05,
      "loss": 0.768,
      "step": 5370
    },
    {
      "epoch": 1.1981851583811167,
      "grad_norm": 0.7401444911956787,
      "learning_rate": 3.0051967334818117e-05,
      "loss": 0.7949,
      "step": 5380
    },
    {
      "epoch": 1.2004119579134889,
      "grad_norm": 0.9640072584152222,
      "learning_rate": 3.0014847809948033e-05,
      "loss": 0.777,
      "step": 5390
    },
    {
      "epoch": 1.202638757445861,
      "grad_norm": 0.7761234641075134,
      "learning_rate": 2.9977728285077953e-05,
      "loss": 0.7601,
      "step": 5400
    },
    {
      "epoch": 1.204865556978233,
      "grad_norm": 0.6461951732635498,
      "learning_rate": 2.9940608760207873e-05,
      "loss": 0.7279,
      "step": 5410
    },
    {
      "epoch": 1.207092356510605,
      "grad_norm": 0.6771827936172485,
      "learning_rate": 2.990348923533779e-05,
      "loss": 0.7625,
      "step": 5420
    },
    {
      "epoch": 1.2093191560429772,
      "grad_norm": 0.6213855147361755,
      "learning_rate": 2.9866369710467707e-05,
      "loss": 0.7697,
      "step": 5430
    },
    {
      "epoch": 1.2115459555753494,
      "grad_norm": 0.8501216769218445,
      "learning_rate": 2.9829250185597623e-05,
      "loss": 0.7312,
      "step": 5440
    },
    {
      "epoch": 1.2137727551077213,
      "grad_norm": 0.7553547024726868,
      "learning_rate": 2.9792130660727547e-05,
      "loss": 0.7642,
      "step": 5450
    },
    {
      "epoch": 1.2159995546400935,
      "grad_norm": 1.188753366470337,
      "learning_rate": 2.9755011135857463e-05,
      "loss": 0.7709,
      "step": 5460
    },
    {
      "epoch": 1.2182263541724656,
      "grad_norm": 0.742760956287384,
      "learning_rate": 2.971789161098738e-05,
      "loss": 0.7945,
      "step": 5470
    },
    {
      "epoch": 1.2204531537048378,
      "grad_norm": 0.7582342028617859,
      "learning_rate": 2.9680772086117297e-05,
      "loss": 0.7637,
      "step": 5480
    },
    {
      "epoch": 1.22267995323721,
      "grad_norm": 0.623539924621582,
      "learning_rate": 2.9643652561247217e-05,
      "loss": 0.7496,
      "step": 5490
    },
    {
      "epoch": 1.2249067527695818,
      "grad_norm": 0.7748719453811646,
      "learning_rate": 2.9606533036377137e-05,
      "loss": 0.7743,
      "step": 5500
    },
    {
      "epoch": 1.227133552301954,
      "grad_norm": 0.6546092629432678,
      "learning_rate": 2.9569413511507053e-05,
      "loss": 0.7616,
      "step": 5510
    },
    {
      "epoch": 1.2293603518343261,
      "grad_norm": 0.5974254012107849,
      "learning_rate": 2.9532293986636973e-05,
      "loss": 0.7354,
      "step": 5520
    },
    {
      "epoch": 1.2315871513666983,
      "grad_norm": 0.6712405681610107,
      "learning_rate": 2.949517446176689e-05,
      "loss": 0.7662,
      "step": 5530
    },
    {
      "epoch": 1.2338139508990702,
      "grad_norm": 0.7165700793266296,
      "learning_rate": 2.9458054936896813e-05,
      "loss": 0.7675,
      "step": 5540
    },
    {
      "epoch": 1.2360407504314423,
      "grad_norm": 0.6912986636161804,
      "learning_rate": 2.942093541202673e-05,
      "loss": 0.8166,
      "step": 5550
    },
    {
      "epoch": 1.2382675499638145,
      "grad_norm": 0.7304540872573853,
      "learning_rate": 2.9383815887156646e-05,
      "loss": 0.756,
      "step": 5560
    },
    {
      "epoch": 1.2404943494961866,
      "grad_norm": 0.7373080849647522,
      "learning_rate": 2.9346696362286563e-05,
      "loss": 0.73,
      "step": 5570
    },
    {
      "epoch": 1.2427211490285588,
      "grad_norm": 0.804351806640625,
      "learning_rate": 2.930957683741648e-05,
      "loss": 0.7198,
      "step": 5580
    },
    {
      "epoch": 1.2449479485609307,
      "grad_norm": 0.6959893107414246,
      "learning_rate": 2.9272457312546403e-05,
      "loss": 0.7325,
      "step": 5590
    },
    {
      "epoch": 1.2471747480933029,
      "grad_norm": 0.7913898229598999,
      "learning_rate": 2.923533778767632e-05,
      "loss": 0.7522,
      "step": 5600
    },
    {
      "epoch": 1.249401547625675,
      "grad_norm": 0.7287315130233765,
      "learning_rate": 2.9198218262806236e-05,
      "loss": 0.7277,
      "step": 5610
    },
    {
      "epoch": 1.2516283471580472,
      "grad_norm": 0.7140116095542908,
      "learning_rate": 2.9161098737936153e-05,
      "loss": 0.7011,
      "step": 5620
    },
    {
      "epoch": 1.253855146690419,
      "grad_norm": 0.7791926860809326,
      "learning_rate": 2.9123979213066073e-05,
      "loss": 0.7707,
      "step": 5630
    },
    {
      "epoch": 1.2560819462227912,
      "grad_norm": 0.7643635272979736,
      "learning_rate": 2.9086859688195993e-05,
      "loss": 0.748,
      "step": 5640
    },
    {
      "epoch": 1.2583087457551634,
      "grad_norm": 0.6876518726348877,
      "learning_rate": 2.9049740163325913e-05,
      "loss": 0.75,
      "step": 5650
    },
    {
      "epoch": 1.2605355452875355,
      "grad_norm": 0.822669267654419,
      "learning_rate": 2.901262063845583e-05,
      "loss": 0.7608,
      "step": 5660
    },
    {
      "epoch": 1.2627623448199077,
      "grad_norm": 0.7400004863739014,
      "learning_rate": 2.8975501113585746e-05,
      "loss": 0.7284,
      "step": 5670
    },
    {
      "epoch": 1.2649891443522796,
      "grad_norm": 0.7152448892593384,
      "learning_rate": 2.893838158871567e-05,
      "loss": 0.7292,
      "step": 5680
    },
    {
      "epoch": 1.2672159438846518,
      "grad_norm": 0.6747407913208008,
      "learning_rate": 2.8901262063845586e-05,
      "loss": 0.803,
      "step": 5690
    },
    {
      "epoch": 1.269442743417024,
      "grad_norm": 0.6531800627708435,
      "learning_rate": 2.8864142538975503e-05,
      "loss": 0.7674,
      "step": 5700
    },
    {
      "epoch": 1.271669542949396,
      "grad_norm": 0.7293782234191895,
      "learning_rate": 2.882702301410542e-05,
      "loss": 0.7575,
      "step": 5710
    },
    {
      "epoch": 1.273896342481768,
      "grad_norm": 0.6869536638259888,
      "learning_rate": 2.8789903489235336e-05,
      "loss": 0.7739,
      "step": 5720
    },
    {
      "epoch": 1.2761231420141401,
      "grad_norm": 0.6642188429832458,
      "learning_rate": 2.875278396436526e-05,
      "loss": 0.7348,
      "step": 5730
    },
    {
      "epoch": 1.2783499415465123,
      "grad_norm": 0.676843523979187,
      "learning_rate": 2.8715664439495176e-05,
      "loss": 0.7494,
      "step": 5740
    },
    {
      "epoch": 1.2805767410788844,
      "grad_norm": 0.8202441334724426,
      "learning_rate": 2.8678544914625093e-05,
      "loss": 0.784,
      "step": 5750
    },
    {
      "epoch": 1.2828035406112566,
      "grad_norm": 0.7214891910552979,
      "learning_rate": 2.8641425389755013e-05,
      "loss": 0.7429,
      "step": 5760
    },
    {
      "epoch": 1.2850303401436285,
      "grad_norm": 0.7661691308021545,
      "learning_rate": 2.860430586488493e-05,
      "loss": 0.7407,
      "step": 5770
    },
    {
      "epoch": 1.2872571396760006,
      "grad_norm": 0.7994995713233948,
      "learning_rate": 2.8567186340014853e-05,
      "loss": 0.7316,
      "step": 5780
    },
    {
      "epoch": 1.2894839392083728,
      "grad_norm": 0.8705100417137146,
      "learning_rate": 2.853006681514477e-05,
      "loss": 0.7847,
      "step": 5790
    },
    {
      "epoch": 1.291710738740745,
      "grad_norm": 0.6891695261001587,
      "learning_rate": 2.8492947290274686e-05,
      "loss": 0.7659,
      "step": 5800
    },
    {
      "epoch": 1.2939375382731169,
      "grad_norm": 0.7167856693267822,
      "learning_rate": 2.8455827765404602e-05,
      "loss": 0.754,
      "step": 5810
    },
    {
      "epoch": 1.296164337805489,
      "grad_norm": 0.730687141418457,
      "learning_rate": 2.8418708240534526e-05,
      "loss": 0.7646,
      "step": 5820
    },
    {
      "epoch": 1.2983911373378612,
      "grad_norm": 0.681077241897583,
      "learning_rate": 2.8381588715664442e-05,
      "loss": 0.737,
      "step": 5830
    },
    {
      "epoch": 1.3006179368702333,
      "grad_norm": 0.7288097143173218,
      "learning_rate": 2.834446919079436e-05,
      "loss": 0.7519,
      "step": 5840
    },
    {
      "epoch": 1.3028447364026055,
      "grad_norm": 0.8044522404670715,
      "learning_rate": 2.8307349665924276e-05,
      "loss": 0.7391,
      "step": 5850
    },
    {
      "epoch": 1.3050715359349774,
      "grad_norm": 0.6229957938194275,
      "learning_rate": 2.8270230141054192e-05,
      "loss": 0.7547,
      "step": 5860
    },
    {
      "epoch": 1.3072983354673495,
      "grad_norm": 0.6975060701370239,
      "learning_rate": 2.8233110616184116e-05,
      "loss": 0.7234,
      "step": 5870
    },
    {
      "epoch": 1.3095251349997217,
      "grad_norm": 0.7013423442840576,
      "learning_rate": 2.8195991091314032e-05,
      "loss": 0.7306,
      "step": 5880
    },
    {
      "epoch": 1.3117519345320938,
      "grad_norm": 0.6605308651924133,
      "learning_rate": 2.8158871566443952e-05,
      "loss": 0.7407,
      "step": 5890
    },
    {
      "epoch": 1.3139787340644657,
      "grad_norm": 0.8312839269638062,
      "learning_rate": 2.812175204157387e-05,
      "loss": 0.7292,
      "step": 5900
    },
    {
      "epoch": 1.316205533596838,
      "grad_norm": 0.9830321669578552,
      "learning_rate": 2.8084632516703785e-05,
      "loss": 0.7473,
      "step": 5910
    },
    {
      "epoch": 1.31843233312921,
      "grad_norm": 0.6725685000419617,
      "learning_rate": 2.804751299183371e-05,
      "loss": 0.7563,
      "step": 5920
    },
    {
      "epoch": 1.3206591326615822,
      "grad_norm": 0.783569872379303,
      "learning_rate": 2.8010393466963626e-05,
      "loss": 0.7709,
      "step": 5930
    },
    {
      "epoch": 1.3228859321939543,
      "grad_norm": 0.6690471768379211,
      "learning_rate": 2.7973273942093542e-05,
      "loss": 0.7303,
      "step": 5940
    },
    {
      "epoch": 1.3251127317263263,
      "grad_norm": 0.7515848875045776,
      "learning_rate": 2.793615441722346e-05,
      "loss": 0.7331,
      "step": 5950
    },
    {
      "epoch": 1.3273395312586984,
      "grad_norm": 0.6112656593322754,
      "learning_rate": 2.7899034892353382e-05,
      "loss": 0.7115,
      "step": 5960
    },
    {
      "epoch": 1.3295663307910706,
      "grad_norm": 1.8489335775375366,
      "learning_rate": 2.78619153674833e-05,
      "loss": 0.7408,
      "step": 5970
    },
    {
      "epoch": 1.3317931303234427,
      "grad_norm": 0.709915041923523,
      "learning_rate": 2.7824795842613215e-05,
      "loss": 0.7511,
      "step": 5980
    },
    {
      "epoch": 1.3340199298558146,
      "grad_norm": 0.8063032627105713,
      "learning_rate": 2.7787676317743132e-05,
      "loss": 0.7663,
      "step": 5990
    },
    {
      "epoch": 1.3362467293881868,
      "grad_norm": 0.6246403455734253,
      "learning_rate": 2.7750556792873052e-05,
      "loss": 0.7479,
      "step": 6000
    },
    {
      "epoch": 1.338473528920559,
      "grad_norm": 0.6789895296096802,
      "learning_rate": 2.7713437268002972e-05,
      "loss": 0.7448,
      "step": 6010
    },
    {
      "epoch": 1.340700328452931,
      "grad_norm": 0.6732999682426453,
      "learning_rate": 2.767631774313289e-05,
      "loss": 0.7646,
      "step": 6020
    },
    {
      "epoch": 1.3429271279853032,
      "grad_norm": 1.311739206314087,
      "learning_rate": 2.763919821826281e-05,
      "loss": 0.7405,
      "step": 6030
    },
    {
      "epoch": 1.3451539275176752,
      "grad_norm": 0.8456215262413025,
      "learning_rate": 2.7602078693392725e-05,
      "loss": 0.7728,
      "step": 6040
    },
    {
      "epoch": 1.3473807270500473,
      "grad_norm": 0.7894157767295837,
      "learning_rate": 2.7564959168522642e-05,
      "loss": 0.8052,
      "step": 6050
    },
    {
      "epoch": 1.3496075265824194,
      "grad_norm": 0.6612246036529541,
      "learning_rate": 2.7527839643652565e-05,
      "loss": 0.7642,
      "step": 6060
    },
    {
      "epoch": 1.3518343261147916,
      "grad_norm": 0.711757481098175,
      "learning_rate": 2.7490720118782482e-05,
      "loss": 0.7246,
      "step": 6070
    },
    {
      "epoch": 1.3540611256471635,
      "grad_norm": 0.6853042840957642,
      "learning_rate": 2.74536005939124e-05,
      "loss": 0.7589,
      "step": 6080
    },
    {
      "epoch": 1.3562879251795357,
      "grad_norm": 0.7019213438034058,
      "learning_rate": 2.7416481069042315e-05,
      "loss": 0.7249,
      "step": 6090
    },
    {
      "epoch": 1.3585147247119078,
      "grad_norm": 0.8103993535041809,
      "learning_rate": 2.737936154417224e-05,
      "loss": 0.7452,
      "step": 6100
    },
    {
      "epoch": 1.36074152424428,
      "grad_norm": 0.7691714763641357,
      "learning_rate": 2.7342242019302155e-05,
      "loss": 0.7634,
      "step": 6110
    },
    {
      "epoch": 1.3629683237766521,
      "grad_norm": 0.7727727890014648,
      "learning_rate": 2.730512249443207e-05,
      "loss": 0.7604,
      "step": 6120
    },
    {
      "epoch": 1.365195123309024,
      "grad_norm": 0.7871968150138855,
      "learning_rate": 2.7268002969561988e-05,
      "loss": 0.7424,
      "step": 6130
    },
    {
      "epoch": 1.3674219228413962,
      "grad_norm": 0.7813353538513184,
      "learning_rate": 2.7230883444691908e-05,
      "loss": 0.741,
      "step": 6140
    },
    {
      "epoch": 1.3696487223737683,
      "grad_norm": 0.7802376747131348,
      "learning_rate": 2.7193763919821828e-05,
      "loss": 0.7326,
      "step": 6150
    },
    {
      "epoch": 1.3718755219061405,
      "grad_norm": 0.8199480772018433,
      "learning_rate": 2.7156644394951748e-05,
      "loss": 0.7848,
      "step": 6160
    },
    {
      "epoch": 1.3741023214385124,
      "grad_norm": 0.685432493686676,
      "learning_rate": 2.7119524870081665e-05,
      "loss": 0.7606,
      "step": 6170
    },
    {
      "epoch": 1.3763291209708846,
      "grad_norm": 1.076026201248169,
      "learning_rate": 2.708240534521158e-05,
      "loss": 0.7581,
      "step": 6180
    },
    {
      "epoch": 1.3785559205032567,
      "grad_norm": 0.7615979313850403,
      "learning_rate": 2.7045285820341498e-05,
      "loss": 0.7533,
      "step": 6190
    },
    {
      "epoch": 1.3807827200356289,
      "grad_norm": 0.6549177765846252,
      "learning_rate": 2.700816629547142e-05,
      "loss": 0.8019,
      "step": 6200
    },
    {
      "epoch": 1.383009519568001,
      "grad_norm": 0.6463903188705444,
      "learning_rate": 2.6971046770601338e-05,
      "loss": 0.6994,
      "step": 6210
    },
    {
      "epoch": 1.385236319100373,
      "grad_norm": 0.7398439645767212,
      "learning_rate": 2.6933927245731255e-05,
      "loss": 0.7761,
      "step": 6220
    },
    {
      "epoch": 1.387463118632745,
      "grad_norm": 0.6689356565475464,
      "learning_rate": 2.689680772086117e-05,
      "loss": 0.764,
      "step": 6230
    },
    {
      "epoch": 1.3896899181651172,
      "grad_norm": 0.6567887663841248,
      "learning_rate": 2.6859688195991095e-05,
      "loss": 0.7973,
      "step": 6240
    },
    {
      "epoch": 1.3919167176974891,
      "grad_norm": 0.7045593857765198,
      "learning_rate": 2.682256867112101e-05,
      "loss": 0.7563,
      "step": 6250
    },
    {
      "epoch": 1.3941435172298613,
      "grad_norm": 0.6571834683418274,
      "learning_rate": 2.6785449146250928e-05,
      "loss": 0.7144,
      "step": 6260
    },
    {
      "epoch": 1.3963703167622334,
      "grad_norm": 0.7629500031471252,
      "learning_rate": 2.6748329621380848e-05,
      "loss": 0.7564,
      "step": 6270
    },
    {
      "epoch": 1.3985971162946056,
      "grad_norm": 0.7326140403747559,
      "learning_rate": 2.6711210096510765e-05,
      "loss": 0.7318,
      "step": 6280
    },
    {
      "epoch": 1.4008239158269777,
      "grad_norm": 0.645291805267334,
      "learning_rate": 2.6674090571640688e-05,
      "loss": 0.8118,
      "step": 6290
    },
    {
      "epoch": 1.4030507153593499,
      "grad_norm": 0.730068564414978,
      "learning_rate": 2.6636971046770605e-05,
      "loss": 0.7727,
      "step": 6300
    },
    {
      "epoch": 1.4052775148917218,
      "grad_norm": 0.6731567978858948,
      "learning_rate": 2.659985152190052e-05,
      "loss": 0.7787,
      "step": 6310
    },
    {
      "epoch": 1.407504314424094,
      "grad_norm": 0.7349258661270142,
      "learning_rate": 2.6562731997030438e-05,
      "loss": 0.7332,
      "step": 6320
    },
    {
      "epoch": 1.409731113956466,
      "grad_norm": 0.7170498967170715,
      "learning_rate": 2.6525612472160354e-05,
      "loss": 0.7317,
      "step": 6330
    },
    {
      "epoch": 1.411957913488838,
      "grad_norm": 0.7804765701293945,
      "learning_rate": 2.6488492947290278e-05,
      "loss": 0.7732,
      "step": 6340
    },
    {
      "epoch": 1.4141847130212102,
      "grad_norm": 0.7101752161979675,
      "learning_rate": 2.6451373422420194e-05,
      "loss": 0.776,
      "step": 6350
    },
    {
      "epoch": 1.4164115125535823,
      "grad_norm": 0.7410455346107483,
      "learning_rate": 2.641425389755011e-05,
      "loss": 0.7631,
      "step": 6360
    },
    {
      "epoch": 1.4186383120859545,
      "grad_norm": 0.721013069152832,
      "learning_rate": 2.6377134372680028e-05,
      "loss": 0.7717,
      "step": 6370
    },
    {
      "epoch": 1.4208651116183266,
      "grad_norm": 0.7849635481834412,
      "learning_rate": 2.634001484780995e-05,
      "loss": 0.7496,
      "step": 6380
    },
    {
      "epoch": 1.4230919111506988,
      "grad_norm": 0.7526881694793701,
      "learning_rate": 2.6302895322939868e-05,
      "loss": 0.7682,
      "step": 6390
    },
    {
      "epoch": 1.4253187106830707,
      "grad_norm": 0.7169734835624695,
      "learning_rate": 2.6265775798069788e-05,
      "loss": 0.7735,
      "step": 6400
    },
    {
      "epoch": 1.4275455102154428,
      "grad_norm": 1.0483677387237549,
      "learning_rate": 2.6228656273199704e-05,
      "loss": 0.7363,
      "step": 6410
    },
    {
      "epoch": 1.429772309747815,
      "grad_norm": 0.7600422501564026,
      "learning_rate": 2.619153674832962e-05,
      "loss": 0.7397,
      "step": 6420
    },
    {
      "epoch": 1.431999109280187,
      "grad_norm": 0.6621583700180054,
      "learning_rate": 2.6154417223459544e-05,
      "loss": 0.7701,
      "step": 6430
    },
    {
      "epoch": 1.434225908812559,
      "grad_norm": 0.7529970407485962,
      "learning_rate": 2.611729769858946e-05,
      "loss": 0.7446,
      "step": 6440
    },
    {
      "epoch": 1.4364527083449312,
      "grad_norm": 0.7590736746788025,
      "learning_rate": 2.6080178173719378e-05,
      "loss": 0.736,
      "step": 6450
    },
    {
      "epoch": 1.4386795078773034,
      "grad_norm": 0.7086120843887329,
      "learning_rate": 2.6043058648849294e-05,
      "loss": 0.7652,
      "step": 6460
    },
    {
      "epoch": 1.4409063074096755,
      "grad_norm": 0.7890698909759521,
      "learning_rate": 2.600593912397921e-05,
      "loss": 0.7335,
      "step": 6470
    },
    {
      "epoch": 1.4431331069420477,
      "grad_norm": 0.7071064114570618,
      "learning_rate": 2.5968819599109134e-05,
      "loss": 0.7761,
      "step": 6480
    },
    {
      "epoch": 1.4453599064744196,
      "grad_norm": 0.7360621094703674,
      "learning_rate": 2.593170007423905e-05,
      "loss": 0.7526,
      "step": 6490
    },
    {
      "epoch": 1.4475867060067917,
      "grad_norm": 0.7524173855781555,
      "learning_rate": 2.5894580549368967e-05,
      "loss": 0.7469,
      "step": 6500
    },
    {
      "epoch": 1.4498135055391639,
      "grad_norm": 1.0220539569854736,
      "learning_rate": 2.5857461024498887e-05,
      "loss": 0.6972,
      "step": 6510
    },
    {
      "epoch": 1.4520403050715358,
      "grad_norm": 0.820173442363739,
      "learning_rate": 2.5820341499628807e-05,
      "loss": 0.7505,
      "step": 6520
    },
    {
      "epoch": 1.454267104603908,
      "grad_norm": 0.7493853569030762,
      "learning_rate": 2.5783221974758724e-05,
      "loss": 0.7483,
      "step": 6530
    },
    {
      "epoch": 1.45649390413628,
      "grad_norm": 0.6840412616729736,
      "learning_rate": 2.5746102449888644e-05,
      "loss": 0.7646,
      "step": 6540
    },
    {
      "epoch": 1.4587207036686523,
      "grad_norm": 0.6925379633903503,
      "learning_rate": 2.570898292501856e-05,
      "loss": 0.7259,
      "step": 6550
    },
    {
      "epoch": 1.4609475032010244,
      "grad_norm": 0.699388325214386,
      "learning_rate": 2.5671863400148477e-05,
      "loss": 0.7724,
      "step": 6560
    },
    {
      "epoch": 1.4631743027333965,
      "grad_norm": 0.6986285448074341,
      "learning_rate": 2.56347438752784e-05,
      "loss": 0.7337,
      "step": 6570
    },
    {
      "epoch": 1.4654011022657685,
      "grad_norm": 0.6979255676269531,
      "learning_rate": 2.5597624350408317e-05,
      "loss": 0.7448,
      "step": 6580
    },
    {
      "epoch": 1.4676279017981406,
      "grad_norm": 0.812130331993103,
      "learning_rate": 2.5560504825538234e-05,
      "loss": 0.7611,
      "step": 6590
    },
    {
      "epoch": 1.4698547013305128,
      "grad_norm": 0.7334606647491455,
      "learning_rate": 2.552338530066815e-05,
      "loss": 0.7289,
      "step": 6600
    },
    {
      "epoch": 1.4720815008628847,
      "grad_norm": 0.7839550971984863,
      "learning_rate": 2.5486265775798067e-05,
      "loss": 0.7267,
      "step": 6610
    },
    {
      "epoch": 1.4743083003952568,
      "grad_norm": 0.7748007774353027,
      "learning_rate": 2.544914625092799e-05,
      "loss": 0.7823,
      "step": 6620
    },
    {
      "epoch": 1.476535099927629,
      "grad_norm": 0.7619634866714478,
      "learning_rate": 2.5412026726057907e-05,
      "loss": 0.6983,
      "step": 6630
    },
    {
      "epoch": 1.4787618994600011,
      "grad_norm": 0.6848421096801758,
      "learning_rate": 2.5374907201187824e-05,
      "loss": 0.7694,
      "step": 6640
    },
    {
      "epoch": 1.4809886989923733,
      "grad_norm": 0.7335297465324402,
      "learning_rate": 2.5337787676317744e-05,
      "loss": 0.7217,
      "step": 6650
    },
    {
      "epoch": 1.4832154985247454,
      "grad_norm": 0.7377612590789795,
      "learning_rate": 2.5300668151447664e-05,
      "loss": 0.757,
      "step": 6660
    },
    {
      "epoch": 1.4854422980571174,
      "grad_norm": 0.733825147151947,
      "learning_rate": 2.5263548626577584e-05,
      "loss": 0.7637,
      "step": 6670
    },
    {
      "epoch": 1.4876690975894895,
      "grad_norm": 0.957509458065033,
      "learning_rate": 2.52264291017075e-05,
      "loss": 0.7739,
      "step": 6680
    },
    {
      "epoch": 1.4898958971218617,
      "grad_norm": 0.6768378615379333,
      "learning_rate": 2.5189309576837417e-05,
      "loss": 0.7421,
      "step": 6690
    },
    {
      "epoch": 1.4921226966542336,
      "grad_norm": 0.737612247467041,
      "learning_rate": 2.5152190051967334e-05,
      "loss": 0.7637,
      "step": 6700
    },
    {
      "epoch": 1.4943494961866057,
      "grad_norm": 0.7827247977256775,
      "learning_rate": 2.5115070527097257e-05,
      "loss": 0.7851,
      "step": 6710
    },
    {
      "epoch": 1.4965762957189779,
      "grad_norm": 0.6961199045181274,
      "learning_rate": 2.5077951002227174e-05,
      "loss": 0.7372,
      "step": 6720
    },
    {
      "epoch": 1.49880309525135,
      "grad_norm": 0.6760412454605103,
      "learning_rate": 2.504083147735709e-05,
      "loss": 0.7759,
      "step": 6730
    },
    {
      "epoch": 1.5010298947837222,
      "grad_norm": 0.6929837465286255,
      "learning_rate": 2.5003711952487007e-05,
      "loss": 0.8033,
      "step": 6740
    },
    {
      "epoch": 1.5032566943160943,
      "grad_norm": 0.7395492196083069,
      "learning_rate": 2.4966592427616927e-05,
      "loss": 0.7798,
      "step": 6750
    },
    {
      "epoch": 1.5054834938484662,
      "grad_norm": 0.7392377257347107,
      "learning_rate": 2.4929472902746843e-05,
      "loss": 0.7969,
      "step": 6760
    },
    {
      "epoch": 1.5077102933808384,
      "grad_norm": 0.7637430429458618,
      "learning_rate": 2.4892353377876763e-05,
      "loss": 0.7726,
      "step": 6770
    },
    {
      "epoch": 1.5099370929132105,
      "grad_norm": 0.6817173361778259,
      "learning_rate": 2.4855233853006683e-05,
      "loss": 0.7379,
      "step": 6780
    },
    {
      "epoch": 1.5121638924455825,
      "grad_norm": 0.7353878617286682,
      "learning_rate": 2.4818114328136603e-05,
      "loss": 0.7728,
      "step": 6790
    },
    {
      "epoch": 1.5143906919779546,
      "grad_norm": 0.7780792713165283,
      "learning_rate": 2.478099480326652e-05,
      "loss": 0.7783,
      "step": 6800
    },
    {
      "epoch": 1.5166174915103268,
      "grad_norm": 0.6687427163124084,
      "learning_rate": 2.474387527839644e-05,
      "loss": 0.7133,
      "step": 6810
    },
    {
      "epoch": 1.518844291042699,
      "grad_norm": 0.7911664843559265,
      "learning_rate": 2.4706755753526357e-05,
      "loss": 0.7702,
      "step": 6820
    },
    {
      "epoch": 1.521071090575071,
      "grad_norm": 0.7343510389328003,
      "learning_rate": 2.4669636228656273e-05,
      "loss": 0.749,
      "step": 6830
    },
    {
      "epoch": 1.5232978901074432,
      "grad_norm": 0.8842294216156006,
      "learning_rate": 2.4632516703786193e-05,
      "loss": 0.7695,
      "step": 6840
    },
    {
      "epoch": 1.5255246896398151,
      "grad_norm": 0.9127561450004578,
      "learning_rate": 2.459539717891611e-05,
      "loss": 0.7793,
      "step": 6850
    },
    {
      "epoch": 1.5277514891721873,
      "grad_norm": 0.6993950009346008,
      "learning_rate": 2.455827765404603e-05,
      "loss": 0.7343,
      "step": 6860
    },
    {
      "epoch": 1.5299782887045592,
      "grad_norm": 0.7735379934310913,
      "learning_rate": 2.4521158129175947e-05,
      "loss": 0.7458,
      "step": 6870
    },
    {
      "epoch": 1.5322050882369314,
      "grad_norm": 0.7041058540344238,
      "learning_rate": 2.4484038604305867e-05,
      "loss": 0.7603,
      "step": 6880
    },
    {
      "epoch": 1.5344318877693035,
      "grad_norm": 0.6797640919685364,
      "learning_rate": 2.4446919079435783e-05,
      "loss": 0.7796,
      "step": 6890
    },
    {
      "epoch": 1.5366586873016757,
      "grad_norm": 0.667934238910675,
      "learning_rate": 2.4409799554565703e-05,
      "loss": 0.7233,
      "step": 6900
    },
    {
      "epoch": 1.5388854868340478,
      "grad_norm": 0.7177388668060303,
      "learning_rate": 2.4372680029695623e-05,
      "loss": 0.7551,
      "step": 6910
    },
    {
      "epoch": 1.54111228636642,
      "grad_norm": 1.0316427946090698,
      "learning_rate": 2.433556050482554e-05,
      "loss": 0.7342,
      "step": 6920
    },
    {
      "epoch": 1.543339085898792,
      "grad_norm": 0.6289306282997131,
      "learning_rate": 2.429844097995546e-05,
      "loss": 0.756,
      "step": 6930
    },
    {
      "epoch": 1.545565885431164,
      "grad_norm": 0.8192619681358337,
      "learning_rate": 2.4261321455085376e-05,
      "loss": 0.7436,
      "step": 6940
    },
    {
      "epoch": 1.5477926849635362,
      "grad_norm": 0.67478346824646,
      "learning_rate": 2.4224201930215296e-05,
      "loss": 0.7578,
      "step": 6950
    },
    {
      "epoch": 1.550019484495908,
      "grad_norm": 0.8444412350654602,
      "learning_rate": 2.4187082405345213e-05,
      "loss": 0.7428,
      "step": 6960
    },
    {
      "epoch": 1.5522462840282802,
      "grad_norm": 0.7094143629074097,
      "learning_rate": 2.414996288047513e-05,
      "loss": 0.7439,
      "step": 6970
    },
    {
      "epoch": 1.5544730835606524,
      "grad_norm": 0.7541159987449646,
      "learning_rate": 2.411284335560505e-05,
      "loss": 0.8255,
      "step": 6980
    },
    {
      "epoch": 1.5566998830930245,
      "grad_norm": 0.7754241228103638,
      "learning_rate": 2.4075723830734966e-05,
      "loss": 0.7647,
      "step": 6990
    },
    {
      "epoch": 1.5589266826253967,
      "grad_norm": 0.744201123714447,
      "learning_rate": 2.4038604305864886e-05,
      "loss": 0.7348,
      "step": 7000
    },
    {
      "epoch": 1.5611534821577688,
      "grad_norm": 0.6847323775291443,
      "learning_rate": 2.4001484780994803e-05,
      "loss": 0.7369,
      "step": 7010
    },
    {
      "epoch": 1.563380281690141,
      "grad_norm": 0.7152776718139648,
      "learning_rate": 2.3964365256124723e-05,
      "loss": 0.7747,
      "step": 7020
    },
    {
      "epoch": 1.565607081222513,
      "grad_norm": 0.649507462978363,
      "learning_rate": 2.392724573125464e-05,
      "loss": 0.7848,
      "step": 7030
    },
    {
      "epoch": 1.567833880754885,
      "grad_norm": 0.7446476221084595,
      "learning_rate": 2.389012620638456e-05,
      "loss": 0.7365,
      "step": 7040
    },
    {
      "epoch": 1.570060680287257,
      "grad_norm": 0.6634663939476013,
      "learning_rate": 2.385300668151448e-05,
      "loss": 0.7716,
      "step": 7050
    },
    {
      "epoch": 1.5722874798196291,
      "grad_norm": 0.7336227893829346,
      "learning_rate": 2.3815887156644396e-05,
      "loss": 0.739,
      "step": 7060
    },
    {
      "epoch": 1.5745142793520013,
      "grad_norm": 0.7767437696456909,
      "learning_rate": 2.3778767631774316e-05,
      "loss": 0.8087,
      "step": 7070
    },
    {
      "epoch": 1.5767410788843734,
      "grad_norm": 0.6849551796913147,
      "learning_rate": 2.3741648106904233e-05,
      "loss": 0.7516,
      "step": 7080
    },
    {
      "epoch": 1.5789678784167456,
      "grad_norm": 0.7704424262046814,
      "learning_rate": 2.3704528582034153e-05,
      "loss": 0.7713,
      "step": 7090
    },
    {
      "epoch": 1.5811946779491177,
      "grad_norm": 0.7728302478790283,
      "learning_rate": 2.366740905716407e-05,
      "loss": 0.7704,
      "step": 7100
    },
    {
      "epoch": 1.5834214774814899,
      "grad_norm": 0.8502142429351807,
      "learning_rate": 2.3630289532293986e-05,
      "loss": 0.7372,
      "step": 7110
    },
    {
      "epoch": 1.5856482770138618,
      "grad_norm": 0.6850553154945374,
      "learning_rate": 2.3593170007423906e-05,
      "loss": 0.7588,
      "step": 7120
    },
    {
      "epoch": 1.587875076546234,
      "grad_norm": 0.8004722595214844,
      "learning_rate": 2.3556050482553823e-05,
      "loss": 0.749,
      "step": 7130
    },
    {
      "epoch": 1.5901018760786059,
      "grad_norm": 0.7996275424957275,
      "learning_rate": 2.3518930957683743e-05,
      "loss": 0.7499,
      "step": 7140
    },
    {
      "epoch": 1.592328675610978,
      "grad_norm": 0.605918288230896,
      "learning_rate": 2.348181143281366e-05,
      "loss": 0.7553,
      "step": 7150
    },
    {
      "epoch": 1.5945554751433502,
      "grad_norm": 0.626882016658783,
      "learning_rate": 2.344469190794358e-05,
      "loss": 0.7562,
      "step": 7160
    },
    {
      "epoch": 1.5967822746757223,
      "grad_norm": 0.6747201085090637,
      "learning_rate": 2.34075723830735e-05,
      "loss": 0.7315,
      "step": 7170
    },
    {
      "epoch": 1.5990090742080945,
      "grad_norm": 0.7655988335609436,
      "learning_rate": 2.3370452858203416e-05,
      "loss": 0.7631,
      "step": 7180
    },
    {
      "epoch": 1.6012358737404666,
      "grad_norm": 0.778239905834198,
      "learning_rate": 2.3333333333333336e-05,
      "loss": 0.764,
      "step": 7190
    },
    {
      "epoch": 1.6034626732728388,
      "grad_norm": 0.6831972599029541,
      "learning_rate": 2.3296213808463252e-05,
      "loss": 0.8018,
      "step": 7200
    },
    {
      "epoch": 1.6056894728052107,
      "grad_norm": 0.6693124771118164,
      "learning_rate": 2.3259094283593172e-05,
      "loss": 0.7275,
      "step": 7210
    },
    {
      "epoch": 1.6079162723375828,
      "grad_norm": 0.6868990659713745,
      "learning_rate": 2.322197475872309e-05,
      "loss": 0.7504,
      "step": 7220
    },
    {
      "epoch": 1.6101430718699548,
      "grad_norm": 0.6697458624839783,
      "learning_rate": 2.318485523385301e-05,
      "loss": 0.752,
      "step": 7230
    },
    {
      "epoch": 1.612369871402327,
      "grad_norm": 1.0890661478042603,
      "learning_rate": 2.3147735708982926e-05,
      "loss": 0.7352,
      "step": 7240
    },
    {
      "epoch": 1.614596670934699,
      "grad_norm": 0.842336118221283,
      "learning_rate": 2.3110616184112842e-05,
      "loss": 0.7214,
      "step": 7250
    },
    {
      "epoch": 1.6168234704670712,
      "grad_norm": 0.7289771437644958,
      "learning_rate": 2.3073496659242762e-05,
      "loss": 0.7407,
      "step": 7260
    },
    {
      "epoch": 1.6190502699994433,
      "grad_norm": 0.7625356912612915,
      "learning_rate": 2.303637713437268e-05,
      "loss": 0.7398,
      "step": 7270
    },
    {
      "epoch": 1.6212770695318155,
      "grad_norm": 0.6063634157180786,
      "learning_rate": 2.29992576095026e-05,
      "loss": 0.7139,
      "step": 7280
    },
    {
      "epoch": 1.6235038690641876,
      "grad_norm": 0.8061919808387756,
      "learning_rate": 2.296213808463252e-05,
      "loss": 0.7828,
      "step": 7290
    },
    {
      "epoch": 1.6257306685965596,
      "grad_norm": 0.8018181920051575,
      "learning_rate": 2.292501855976244e-05,
      "loss": 0.7215,
      "step": 7300
    },
    {
      "epoch": 1.6279574681289317,
      "grad_norm": 0.7913569808006287,
      "learning_rate": 2.2887899034892356e-05,
      "loss": 0.7591,
      "step": 7310
    },
    {
      "epoch": 1.6301842676613036,
      "grad_norm": 0.7226076126098633,
      "learning_rate": 2.2850779510022272e-05,
      "loss": 0.7639,
      "step": 7320
    },
    {
      "epoch": 1.6324110671936758,
      "grad_norm": 0.67814040184021,
      "learning_rate": 2.2813659985152192e-05,
      "loss": 0.7522,
      "step": 7330
    },
    {
      "epoch": 1.634637866726048,
      "grad_norm": 0.6302527785301208,
      "learning_rate": 2.277654046028211e-05,
      "loss": 0.7521,
      "step": 7340
    },
    {
      "epoch": 1.63686466625842,
      "grad_norm": 0.7302154898643494,
      "learning_rate": 2.273942093541203e-05,
      "loss": 0.7908,
      "step": 7350
    },
    {
      "epoch": 1.6390914657907922,
      "grad_norm": 0.6337767243385315,
      "learning_rate": 2.2702301410541945e-05,
      "loss": 0.7372,
      "step": 7360
    },
    {
      "epoch": 1.6413182653231644,
      "grad_norm": 0.6197163462638855,
      "learning_rate": 2.2665181885671865e-05,
      "loss": 0.7753,
      "step": 7370
    },
    {
      "epoch": 1.6435450648555365,
      "grad_norm": 0.7199457287788391,
      "learning_rate": 2.2628062360801782e-05,
      "loss": 0.7153,
      "step": 7380
    },
    {
      "epoch": 1.6457718643879085,
      "grad_norm": 0.7979657649993896,
      "learning_rate": 2.25909428359317e-05,
      "loss": 0.7528,
      "step": 7390
    },
    {
      "epoch": 1.6479986639202806,
      "grad_norm": 0.7932729721069336,
      "learning_rate": 2.255382331106162e-05,
      "loss": 0.7832,
      "step": 7400
    },
    {
      "epoch": 1.6502254634526525,
      "grad_norm": 0.7469703555107117,
      "learning_rate": 2.251670378619154e-05,
      "loss": 0.7733,
      "step": 7410
    },
    {
      "epoch": 1.6524522629850247,
      "grad_norm": 0.7439418435096741,
      "learning_rate": 2.2479584261321455e-05,
      "loss": 0.7083,
      "step": 7420
    },
    {
      "epoch": 1.6546790625173968,
      "grad_norm": 0.7432529330253601,
      "learning_rate": 2.2442464736451375e-05,
      "loss": 0.7384,
      "step": 7430
    },
    {
      "epoch": 1.656905862049769,
      "grad_norm": 0.7265019416809082,
      "learning_rate": 2.2405345211581295e-05,
      "loss": 0.7203,
      "step": 7440
    },
    {
      "epoch": 1.6591326615821411,
      "grad_norm": 0.702624499797821,
      "learning_rate": 2.2368225686711212e-05,
      "loss": 0.6954,
      "step": 7450
    },
    {
      "epoch": 1.6613594611145133,
      "grad_norm": 0.652024507522583,
      "learning_rate": 2.233110616184113e-05,
      "loss": 0.682,
      "step": 7460
    },
    {
      "epoch": 1.6635862606468854,
      "grad_norm": 0.7623732686042786,
      "learning_rate": 2.229398663697105e-05,
      "loss": 0.7544,
      "step": 7470
    },
    {
      "epoch": 1.6658130601792573,
      "grad_norm": 0.7270975708961487,
      "learning_rate": 2.2256867112100965e-05,
      "loss": 0.7547,
      "step": 7480
    },
    {
      "epoch": 1.6680398597116295,
      "grad_norm": 0.732712984085083,
      "learning_rate": 2.2219747587230885e-05,
      "loss": 0.704,
      "step": 7490
    },
    {
      "epoch": 1.6702666592440014,
      "grad_norm": 6.714351177215576,
      "learning_rate": 2.21826280623608e-05,
      "loss": 0.7254,
      "step": 7500
    },
    {
      "epoch": 1.6724934587763736,
      "grad_norm": 0.702083170413971,
      "learning_rate": 2.214550853749072e-05,
      "loss": 0.743,
      "step": 7510
    },
    {
      "epoch": 1.6747202583087457,
      "grad_norm": 0.8527854681015015,
      "learning_rate": 2.2108389012620638e-05,
      "loss": 0.7769,
      "step": 7520
    },
    {
      "epoch": 1.6769470578411179,
      "grad_norm": 0.6351281404495239,
      "learning_rate": 2.2071269487750555e-05,
      "loss": 0.7582,
      "step": 7530
    },
    {
      "epoch": 1.67917385737349,
      "grad_norm": 0.8143644332885742,
      "learning_rate": 2.2034149962880475e-05,
      "loss": 0.7641,
      "step": 7540
    },
    {
      "epoch": 1.6814006569058622,
      "grad_norm": 0.7124218940734863,
      "learning_rate": 2.1997030438010395e-05,
      "loss": 0.7475,
      "step": 7550
    },
    {
      "epoch": 1.6836274564382343,
      "grad_norm": 0.7271794676780701,
      "learning_rate": 2.1959910913140315e-05,
      "loss": 0.7723,
      "step": 7560
    },
    {
      "epoch": 1.6858542559706062,
      "grad_norm": 0.7791359424591064,
      "learning_rate": 2.192279138827023e-05,
      "loss": 0.7525,
      "step": 7570
    },
    {
      "epoch": 1.6880810555029784,
      "grad_norm": 0.6990023851394653,
      "learning_rate": 2.188567186340015e-05,
      "loss": 0.7228,
      "step": 7580
    },
    {
      "epoch": 1.6903078550353503,
      "grad_norm": 0.6293674111366272,
      "learning_rate": 2.1848552338530068e-05,
      "loss": 0.6991,
      "step": 7590
    },
    {
      "epoch": 1.6925346545677225,
      "grad_norm": 0.6971145868301392,
      "learning_rate": 2.1811432813659985e-05,
      "loss": 0.7584,
      "step": 7600
    },
    {
      "epoch": 1.6947614541000946,
      "grad_norm": 0.6599599123001099,
      "learning_rate": 2.1774313288789905e-05,
      "loss": 0.7329,
      "step": 7610
    },
    {
      "epoch": 1.6969882536324667,
      "grad_norm": 0.6358823180198669,
      "learning_rate": 2.173719376391982e-05,
      "loss": 0.7295,
      "step": 7620
    },
    {
      "epoch": 1.699215053164839,
      "grad_norm": 0.7092067003250122,
      "learning_rate": 2.170007423904974e-05,
      "loss": 0.7539,
      "step": 7630
    },
    {
      "epoch": 1.701441852697211,
      "grad_norm": 0.8008441925048828,
      "learning_rate": 2.1662954714179658e-05,
      "loss": 0.7912,
      "step": 7640
    },
    {
      "epoch": 1.7036686522295832,
      "grad_norm": 0.6570079326629639,
      "learning_rate": 2.1625835189309578e-05,
      "loss": 0.721,
      "step": 7650
    },
    {
      "epoch": 1.7058954517619551,
      "grad_norm": 0.7059048414230347,
      "learning_rate": 2.1588715664439495e-05,
      "loss": 0.7868,
      "step": 7660
    },
    {
      "epoch": 1.7081222512943273,
      "grad_norm": 1.5487614870071411,
      "learning_rate": 2.1551596139569415e-05,
      "loss": 0.7532,
      "step": 7670
    },
    {
      "epoch": 1.7103490508266992,
      "grad_norm": 0.6951808333396912,
      "learning_rate": 2.1514476614699335e-05,
      "loss": 0.7764,
      "step": 7680
    },
    {
      "epoch": 1.7125758503590713,
      "grad_norm": 0.7835851311683655,
      "learning_rate": 2.147735708982925e-05,
      "loss": 0.7527,
      "step": 7690
    },
    {
      "epoch": 1.7148026498914435,
      "grad_norm": 0.8057929277420044,
      "learning_rate": 2.144023756495917e-05,
      "loss": 0.7469,
      "step": 7700
    },
    {
      "epoch": 1.7170294494238156,
      "grad_norm": 0.7758145332336426,
      "learning_rate": 2.1403118040089088e-05,
      "loss": 0.769,
      "step": 7710
    },
    {
      "epoch": 1.7192562489561878,
      "grad_norm": 0.714296281337738,
      "learning_rate": 2.1365998515219008e-05,
      "loss": 0.7607,
      "step": 7720
    },
    {
      "epoch": 1.72148304848856,
      "grad_norm": 0.8263488411903381,
      "learning_rate": 2.1328878990348924e-05,
      "loss": 0.7664,
      "step": 7730
    },
    {
      "epoch": 1.723709848020932,
      "grad_norm": 0.7385662198066711,
      "learning_rate": 2.129175946547884e-05,
      "loss": 0.8064,
      "step": 7740
    },
    {
      "epoch": 1.725936647553304,
      "grad_norm": 0.6092966794967651,
      "learning_rate": 2.125463994060876e-05,
      "loss": 0.727,
      "step": 7750
    },
    {
      "epoch": 1.7281634470856762,
      "grad_norm": 0.7239982485771179,
      "learning_rate": 2.1217520415738678e-05,
      "loss": 0.7756,
      "step": 7760
    },
    {
      "epoch": 1.730390246618048,
      "grad_norm": 0.7110759615898132,
      "learning_rate": 2.1180400890868598e-05,
      "loss": 0.7586,
      "step": 7770
    },
    {
      "epoch": 1.7326170461504202,
      "grad_norm": 0.705968976020813,
      "learning_rate": 2.1143281365998514e-05,
      "loss": 0.7296,
      "step": 7780
    },
    {
      "epoch": 1.7348438456827924,
      "grad_norm": 0.9216163754463196,
      "learning_rate": 2.1106161841128434e-05,
      "loss": 0.7179,
      "step": 7790
    },
    {
      "epoch": 1.7370706452151645,
      "grad_norm": 0.6990651488304138,
      "learning_rate": 2.1069042316258354e-05,
      "loss": 0.7331,
      "step": 7800
    },
    {
      "epoch": 1.7392974447475367,
      "grad_norm": 0.7353733777999878,
      "learning_rate": 2.103192279138827e-05,
      "loss": 0.8378,
      "step": 7810
    },
    {
      "epoch": 1.7415242442799088,
      "grad_norm": 0.7601984739303589,
      "learning_rate": 2.099480326651819e-05,
      "loss": 0.742,
      "step": 7820
    },
    {
      "epoch": 1.743751043812281,
      "grad_norm": 0.636160671710968,
      "learning_rate": 2.0957683741648108e-05,
      "loss": 0.7427,
      "step": 7830
    },
    {
      "epoch": 1.745977843344653,
      "grad_norm": 0.5584288239479065,
      "learning_rate": 2.0920564216778028e-05,
      "loss": 0.699,
      "step": 7840
    },
    {
      "epoch": 1.748204642877025,
      "grad_norm": 0.6735186576843262,
      "learning_rate": 2.0883444691907944e-05,
      "loss": 0.7313,
      "step": 7850
    },
    {
      "epoch": 1.750431442409397,
      "grad_norm": 0.722187340259552,
      "learning_rate": 2.0846325167037864e-05,
      "loss": 0.7534,
      "step": 7860
    },
    {
      "epoch": 1.7526582419417691,
      "grad_norm": 0.7382332682609558,
      "learning_rate": 2.080920564216778e-05,
      "loss": 0.7065,
      "step": 7870
    },
    {
      "epoch": 1.7548850414741413,
      "grad_norm": 0.6985739469528198,
      "learning_rate": 2.0772086117297697e-05,
      "loss": 0.7325,
      "step": 7880
    },
    {
      "epoch": 1.7571118410065134,
      "grad_norm": 0.6940416693687439,
      "learning_rate": 2.0734966592427617e-05,
      "loss": 0.7162,
      "step": 7890
    },
    {
      "epoch": 1.7593386405388856,
      "grad_norm": 0.7624791860580444,
      "learning_rate": 2.0697847067557534e-05,
      "loss": 0.7948,
      "step": 7900
    },
    {
      "epoch": 1.7615654400712577,
      "grad_norm": 0.7574288249015808,
      "learning_rate": 2.0660727542687454e-05,
      "loss": 0.7721,
      "step": 7910
    },
    {
      "epoch": 1.7637922396036296,
      "grad_norm": 0.7353010773658752,
      "learning_rate": 2.0623608017817374e-05,
      "loss": 0.7223,
      "step": 7920
    },
    {
      "epoch": 1.7660190391360018,
      "grad_norm": 0.7334142923355103,
      "learning_rate": 2.058648849294729e-05,
      "loss": 0.7403,
      "step": 7930
    },
    {
      "epoch": 1.768245838668374,
      "grad_norm": 0.6967449188232422,
      "learning_rate": 2.054936896807721e-05,
      "loss": 0.738,
      "step": 7940
    },
    {
      "epoch": 1.7704726382007459,
      "grad_norm": 0.7200940251350403,
      "learning_rate": 2.0512249443207127e-05,
      "loss": 0.7701,
      "step": 7950
    },
    {
      "epoch": 1.772699437733118,
      "grad_norm": 0.6244747042655945,
      "learning_rate": 2.0475129918337047e-05,
      "loss": 0.7149,
      "step": 7960
    },
    {
      "epoch": 1.7749262372654901,
      "grad_norm": 0.7565184831619263,
      "learning_rate": 2.0438010393466964e-05,
      "loss": 0.7262,
      "step": 7970
    },
    {
      "epoch": 1.7771530367978623,
      "grad_norm": 3.927138090133667,
      "learning_rate": 2.0400890868596884e-05,
      "loss": 0.7082,
      "step": 7980
    },
    {
      "epoch": 1.7793798363302344,
      "grad_norm": 0.7511236667633057,
      "learning_rate": 2.03637713437268e-05,
      "loss": 0.776,
      "step": 7990
    },
    {
      "epoch": 1.7816066358626066,
      "grad_norm": 0.945286750793457,
      "learning_rate": 2.032665181885672e-05,
      "loss": 0.745,
      "step": 8000
    },
    {
      "epoch": 1.7838334353949785,
      "grad_norm": 0.7194238901138306,
      "learning_rate": 2.0289532293986637e-05,
      "loss": 0.7826,
      "step": 8010
    },
    {
      "epoch": 1.7860602349273507,
      "grad_norm": 0.7267417311668396,
      "learning_rate": 2.0252412769116554e-05,
      "loss": 0.7709,
      "step": 8020
    },
    {
      "epoch": 1.7882870344597228,
      "grad_norm": 0.6780979633331299,
      "learning_rate": 2.0215293244246474e-05,
      "loss": 0.7567,
      "step": 8030
    },
    {
      "epoch": 1.7905138339920947,
      "grad_norm": 0.6856001019477844,
      "learning_rate": 2.017817371937639e-05,
      "loss": 0.7794,
      "step": 8040
    },
    {
      "epoch": 1.7927406335244669,
      "grad_norm": 0.70738285779953,
      "learning_rate": 2.014105419450631e-05,
      "loss": 0.7841,
      "step": 8050
    },
    {
      "epoch": 1.794967433056839,
      "grad_norm": 1.2639813423156738,
      "learning_rate": 2.010393466963623e-05,
      "loss": 0.7735,
      "step": 8060
    },
    {
      "epoch": 1.7971942325892112,
      "grad_norm": 0.7431095838546753,
      "learning_rate": 2.006681514476615e-05,
      "loss": 0.7679,
      "step": 8070
    },
    {
      "epoch": 1.7994210321215833,
      "grad_norm": 0.770111083984375,
      "learning_rate": 2.0029695619896067e-05,
      "loss": 0.7796,
      "step": 8080
    },
    {
      "epoch": 1.8016478316539555,
      "grad_norm": 0.7888079881668091,
      "learning_rate": 1.9992576095025984e-05,
      "loss": 0.7598,
      "step": 8090
    },
    {
      "epoch": 1.8038746311863274,
      "grad_norm": 0.833698034286499,
      "learning_rate": 1.9955456570155904e-05,
      "loss": 0.7613,
      "step": 8100
    },
    {
      "epoch": 1.8061014307186996,
      "grad_norm": 0.8164994716644287,
      "learning_rate": 1.991833704528582e-05,
      "loss": 0.7372,
      "step": 8110
    },
    {
      "epoch": 1.8083282302510717,
      "grad_norm": 1.3387354612350464,
      "learning_rate": 1.988121752041574e-05,
      "loss": 0.7185,
      "step": 8120
    },
    {
      "epoch": 1.8105550297834436,
      "grad_norm": 0.7283964157104492,
      "learning_rate": 1.9844097995545657e-05,
      "loss": 0.76,
      "step": 8130
    },
    {
      "epoch": 1.8127818293158158,
      "grad_norm": 0.6124359965324402,
      "learning_rate": 1.9806978470675577e-05,
      "loss": 0.7547,
      "step": 8140
    },
    {
      "epoch": 1.815008628848188,
      "grad_norm": 0.7056037783622742,
      "learning_rate": 1.9769858945805493e-05,
      "loss": 0.7076,
      "step": 8150
    },
    {
      "epoch": 1.81723542838056,
      "grad_norm": 0.7516211867332458,
      "learning_rate": 1.973273942093541e-05,
      "loss": 0.7524,
      "step": 8160
    },
    {
      "epoch": 1.8194622279129322,
      "grad_norm": 0.7325417399406433,
      "learning_rate": 1.969561989606533e-05,
      "loss": 0.771,
      "step": 8170
    },
    {
      "epoch": 1.8216890274453044,
      "grad_norm": 0.7511669993400574,
      "learning_rate": 1.965850037119525e-05,
      "loss": 0.7552,
      "step": 8180
    },
    {
      "epoch": 1.8239158269776763,
      "grad_norm": 0.8183954954147339,
      "learning_rate": 1.962138084632517e-05,
      "loss": 0.7084,
      "step": 8190
    },
    {
      "epoch": 1.8261426265100484,
      "grad_norm": 0.7930399179458618,
      "learning_rate": 1.9584261321455087e-05,
      "loss": 0.7228,
      "step": 8200
    },
    {
      "epoch": 1.8283694260424204,
      "grad_norm": 0.8174817562103271,
      "learning_rate": 1.9547141796585007e-05,
      "loss": 0.7362,
      "step": 8210
    },
    {
      "epoch": 1.8305962255747925,
      "grad_norm": 0.7299080491065979,
      "learning_rate": 1.9510022271714923e-05,
      "loss": 0.7671,
      "step": 8220
    },
    {
      "epoch": 1.8328230251071647,
      "grad_norm": 0.6796220541000366,
      "learning_rate": 1.947290274684484e-05,
      "loss": 0.7606,
      "step": 8230
    },
    {
      "epoch": 1.8350498246395368,
      "grad_norm": 0.6090371608734131,
      "learning_rate": 1.943578322197476e-05,
      "loss": 0.775,
      "step": 8240
    },
    {
      "epoch": 1.837276624171909,
      "grad_norm": 0.9036520719528198,
      "learning_rate": 1.9398663697104677e-05,
      "loss": 0.7282,
      "step": 8250
    },
    {
      "epoch": 1.839503423704281,
      "grad_norm": 0.7403651475906372,
      "learning_rate": 1.9361544172234597e-05,
      "loss": 0.7212,
      "step": 8260
    },
    {
      "epoch": 1.8417302232366533,
      "grad_norm": 0.8300374746322632,
      "learning_rate": 1.9324424647364513e-05,
      "loss": 0.7327,
      "step": 8270
    },
    {
      "epoch": 1.8439570227690252,
      "grad_norm": 0.8012757301330566,
      "learning_rate": 1.9287305122494433e-05,
      "loss": 0.7363,
      "step": 8280
    },
    {
      "epoch": 1.8461838223013973,
      "grad_norm": 0.740086019039154,
      "learning_rate": 1.925018559762435e-05,
      "loss": 0.8496,
      "step": 8290
    },
    {
      "epoch": 1.8484106218337693,
      "grad_norm": 0.6348573565483093,
      "learning_rate": 1.921306607275427e-05,
      "loss": 0.7444,
      "step": 8300
    },
    {
      "epoch": 1.8506374213661414,
      "grad_norm": 0.7755098938941956,
      "learning_rate": 1.917594654788419e-05,
      "loss": 0.747,
      "step": 8310
    },
    {
      "epoch": 1.8528642208985135,
      "grad_norm": 0.6709741950035095,
      "learning_rate": 1.9138827023014106e-05,
      "loss": 0.7523,
      "step": 8320
    },
    {
      "epoch": 1.8550910204308857,
      "grad_norm": 0.7027788758277893,
      "learning_rate": 1.9101707498144026e-05,
      "loss": 0.6717,
      "step": 8330
    },
    {
      "epoch": 1.8573178199632578,
      "grad_norm": 0.6236329674720764,
      "learning_rate": 1.9064587973273943e-05,
      "loss": 0.7199,
      "step": 8340
    },
    {
      "epoch": 1.85954461949563,
      "grad_norm": 0.6220865249633789,
      "learning_rate": 1.9027468448403863e-05,
      "loss": 0.7832,
      "step": 8350
    },
    {
      "epoch": 1.8617714190280021,
      "grad_norm": 0.7826201319694519,
      "learning_rate": 1.899034892353378e-05,
      "loss": 0.7369,
      "step": 8360
    },
    {
      "epoch": 1.863998218560374,
      "grad_norm": 0.7373700737953186,
      "learning_rate": 1.89532293986637e-05,
      "loss": 0.7626,
      "step": 8370
    },
    {
      "epoch": 1.8662250180927462,
      "grad_norm": 0.8113235235214233,
      "learning_rate": 1.8916109873793616e-05,
      "loss": 0.7212,
      "step": 8380
    },
    {
      "epoch": 1.8684518176251181,
      "grad_norm": 0.7238682508468628,
      "learning_rate": 1.8878990348923533e-05,
      "loss": 0.7875,
      "step": 8390
    },
    {
      "epoch": 1.8706786171574903,
      "grad_norm": 0.7537895441055298,
      "learning_rate": 1.8841870824053453e-05,
      "loss": 0.7371,
      "step": 8400
    },
    {
      "epoch": 1.8729054166898624,
      "grad_norm": 0.6354029178619385,
      "learning_rate": 1.880475129918337e-05,
      "loss": 0.7321,
      "step": 8410
    },
    {
      "epoch": 1.8751322162222346,
      "grad_norm": 0.6538915038108826,
      "learning_rate": 1.876763177431329e-05,
      "loss": 0.7413,
      "step": 8420
    },
    {
      "epoch": 1.8773590157546067,
      "grad_norm": 0.7423915863037109,
      "learning_rate": 1.873051224944321e-05,
      "loss": 0.7691,
      "step": 8430
    },
    {
      "epoch": 1.8795858152869789,
      "grad_norm": 0.657944917678833,
      "learning_rate": 1.8693392724573126e-05,
      "loss": 0.7501,
      "step": 8440
    },
    {
      "epoch": 1.881812614819351,
      "grad_norm": 0.778572678565979,
      "learning_rate": 1.8656273199703046e-05,
      "loss": 0.7393,
      "step": 8450
    },
    {
      "epoch": 1.884039414351723,
      "grad_norm": 0.7361364960670471,
      "learning_rate": 1.8619153674832963e-05,
      "loss": 0.7557,
      "step": 8460
    },
    {
      "epoch": 1.886266213884095,
      "grad_norm": 0.7961292266845703,
      "learning_rate": 1.8582034149962883e-05,
      "loss": 0.7502,
      "step": 8470
    },
    {
      "epoch": 1.888493013416467,
      "grad_norm": 0.7271760106086731,
      "learning_rate": 1.85449146250928e-05,
      "loss": 0.7214,
      "step": 8480
    },
    {
      "epoch": 1.8907198129488392,
      "grad_norm": 0.6902526617050171,
      "learning_rate": 1.850779510022272e-05,
      "loss": 0.7748,
      "step": 8490
    },
    {
      "epoch": 1.8929466124812113,
      "grad_norm": 0.7041302919387817,
      "learning_rate": 1.8470675575352636e-05,
      "loss": 0.7742,
      "step": 8500
    },
    {
      "epoch": 1.8951734120135835,
      "grad_norm": 0.607014536857605,
      "learning_rate": 1.8433556050482556e-05,
      "loss": 0.7489,
      "step": 8510
    },
    {
      "epoch": 1.8974002115459556,
      "grad_norm": 0.6735551953315735,
      "learning_rate": 1.8396436525612473e-05,
      "loss": 0.777,
      "step": 8520
    },
    {
      "epoch": 1.8996270110783278,
      "grad_norm": 0.7557336091995239,
      "learning_rate": 1.835931700074239e-05,
      "loss": 0.7539,
      "step": 8530
    },
    {
      "epoch": 1.9018538106107,
      "grad_norm": 0.6883865594863892,
      "learning_rate": 1.832219747587231e-05,
      "loss": 0.7642,
      "step": 8540
    },
    {
      "epoch": 1.9040806101430718,
      "grad_norm": 0.7631590366363525,
      "learning_rate": 1.8285077951002226e-05,
      "loss": 0.7391,
      "step": 8550
    },
    {
      "epoch": 1.906307409675444,
      "grad_norm": 0.602038562297821,
      "learning_rate": 1.8247958426132146e-05,
      "loss": 0.72,
      "step": 8560
    },
    {
      "epoch": 1.908534209207816,
      "grad_norm": 0.7219465970993042,
      "learning_rate": 1.8210838901262066e-05,
      "loss": 0.7277,
      "step": 8570
    },
    {
      "epoch": 1.910761008740188,
      "grad_norm": 0.7475463151931763,
      "learning_rate": 1.8173719376391986e-05,
      "loss": 0.7539,
      "step": 8580
    },
    {
      "epoch": 1.9129878082725602,
      "grad_norm": 0.7850664258003235,
      "learning_rate": 1.8136599851521902e-05,
      "loss": 0.7659,
      "step": 8590
    },
    {
      "epoch": 1.9152146078049324,
      "grad_norm": 1.0638893842697144,
      "learning_rate": 1.809948032665182e-05,
      "loss": 0.7452,
      "step": 8600
    },
    {
      "epoch": 1.9174414073373045,
      "grad_norm": 0.7281736135482788,
      "learning_rate": 1.806236080178174e-05,
      "loss": 0.6888,
      "step": 8610
    },
    {
      "epoch": 1.9196682068696767,
      "grad_norm": 0.7901471853256226,
      "learning_rate": 1.8025241276911656e-05,
      "loss": 0.7853,
      "step": 8620
    },
    {
      "epoch": 1.9218950064020488,
      "grad_norm": 0.6669132113456726,
      "learning_rate": 1.7988121752041576e-05,
      "loss": 0.7457,
      "step": 8630
    },
    {
      "epoch": 1.9241218059344207,
      "grad_norm": 0.7779813408851624,
      "learning_rate": 1.7951002227171492e-05,
      "loss": 0.7081,
      "step": 8640
    },
    {
      "epoch": 1.9263486054667929,
      "grad_norm": 0.7019695043563843,
      "learning_rate": 1.7913882702301412e-05,
      "loss": 0.7134,
      "step": 8650
    },
    {
      "epoch": 1.9285754049991648,
      "grad_norm": 0.7403034567832947,
      "learning_rate": 1.787676317743133e-05,
      "loss": 0.7588,
      "step": 8660
    },
    {
      "epoch": 1.930802204531537,
      "grad_norm": 0.8418719172477722,
      "learning_rate": 1.7839643652561246e-05,
      "loss": 0.7038,
      "step": 8670
    },
    {
      "epoch": 1.933029004063909,
      "grad_norm": 0.7545411586761475,
      "learning_rate": 1.7802524127691166e-05,
      "loss": 0.7392,
      "step": 8680
    },
    {
      "epoch": 1.9352558035962812,
      "grad_norm": 1.8386255502700806,
      "learning_rate": 1.7765404602821086e-05,
      "loss": 0.7049,
      "step": 8690
    },
    {
      "epoch": 1.9374826031286534,
      "grad_norm": 0.6490232348442078,
      "learning_rate": 1.7728285077951006e-05,
      "loss": 0.7117,
      "step": 8700
    },
    {
      "epoch": 1.9397094026610255,
      "grad_norm": 0.7037219405174255,
      "learning_rate": 1.7691165553080922e-05,
      "loss": 0.7407,
      "step": 8710
    },
    {
      "epoch": 1.9419362021933977,
      "grad_norm": 0.7419924139976501,
      "learning_rate": 1.7654046028210842e-05,
      "loss": 0.7168,
      "step": 8720
    },
    {
      "epoch": 1.9441630017257696,
      "grad_norm": 0.7086188793182373,
      "learning_rate": 1.761692650334076e-05,
      "loss": 0.7251,
      "step": 8730
    },
    {
      "epoch": 1.9463898012581418,
      "grad_norm": 0.6936379075050354,
      "learning_rate": 1.7579806978470675e-05,
      "loss": 0.7712,
      "step": 8740
    },
    {
      "epoch": 1.9486166007905137,
      "grad_norm": 0.710677444934845,
      "learning_rate": 1.7542687453600595e-05,
      "loss": 0.7519,
      "step": 8750
    },
    {
      "epoch": 1.9508434003228858,
      "grad_norm": 0.7228116393089294,
      "learning_rate": 1.7505567928730512e-05,
      "loss": 0.7378,
      "step": 8760
    },
    {
      "epoch": 1.953070199855258,
      "grad_norm": 0.6967023611068726,
      "learning_rate": 1.7468448403860432e-05,
      "loss": 0.7576,
      "step": 8770
    },
    {
      "epoch": 1.9552969993876301,
      "grad_norm": 0.9264447689056396,
      "learning_rate": 1.743132887899035e-05,
      "loss": 0.7395,
      "step": 8780
    },
    {
      "epoch": 1.9575237989200023,
      "grad_norm": 0.6914603114128113,
      "learning_rate": 1.739420935412027e-05,
      "loss": 0.7196,
      "step": 8790
    },
    {
      "epoch": 1.9597505984523744,
      "grad_norm": 0.760994017124176,
      "learning_rate": 1.7357089829250185e-05,
      "loss": 0.7704,
      "step": 8800
    },
    {
      "epoch": 1.9619773979847466,
      "grad_norm": 0.7628377079963684,
      "learning_rate": 1.7319970304380105e-05,
      "loss": 0.7391,
      "step": 8810
    },
    {
      "epoch": 1.9642041975171185,
      "grad_norm": 0.7860884666442871,
      "learning_rate": 1.7282850779510025e-05,
      "loss": 0.7836,
      "step": 8820
    },
    {
      "epoch": 1.9664309970494906,
      "grad_norm": 0.743351936340332,
      "learning_rate": 1.7245731254639942e-05,
      "loss": 0.7328,
      "step": 8830
    },
    {
      "epoch": 1.9686577965818626,
      "grad_norm": 0.6399717926979065,
      "learning_rate": 1.7208611729769862e-05,
      "loss": 0.7186,
      "step": 8840
    },
    {
      "epoch": 1.9708845961142347,
      "grad_norm": 0.6676869988441467,
      "learning_rate": 1.717149220489978e-05,
      "loss": 0.727,
      "step": 8850
    },
    {
      "epoch": 1.9731113956466069,
      "grad_norm": 0.6621179580688477,
      "learning_rate": 1.71343726800297e-05,
      "loss": 0.7402,
      "step": 8860
    },
    {
      "epoch": 1.975338195178979,
      "grad_norm": 0.7202479243278503,
      "learning_rate": 1.7097253155159615e-05,
      "loss": 0.748,
      "step": 8870
    },
    {
      "epoch": 1.9775649947113512,
      "grad_norm": 0.6728515625,
      "learning_rate": 1.7060133630289532e-05,
      "loss": 0.7457,
      "step": 8880
    },
    {
      "epoch": 1.9797917942437233,
      "grad_norm": 0.7494919300079346,
      "learning_rate": 1.7023014105419452e-05,
      "loss": 0.7699,
      "step": 8890
    },
    {
      "epoch": 1.9820185937760955,
      "grad_norm": 0.6688557863235474,
      "learning_rate": 1.698589458054937e-05,
      "loss": 0.7705,
      "step": 8900
    },
    {
      "epoch": 1.9842453933084674,
      "grad_norm": 0.815640926361084,
      "learning_rate": 1.694877505567929e-05,
      "loss": 0.712,
      "step": 8910
    },
    {
      "epoch": 1.9864721928408395,
      "grad_norm": 0.6918551921844482,
      "learning_rate": 1.6911655530809205e-05,
      "loss": 0.7311,
      "step": 8920
    },
    {
      "epoch": 1.9886989923732115,
      "grad_norm": 0.6906364560127258,
      "learning_rate": 1.6874536005939125e-05,
      "loss": 0.7152,
      "step": 8930
    },
    {
      "epoch": 1.9909257919055836,
      "grad_norm": 0.7959092259407043,
      "learning_rate": 1.683741648106904e-05,
      "loss": 0.7198,
      "step": 8940
    },
    {
      "epoch": 1.9931525914379558,
      "grad_norm": 0.6654887795448303,
      "learning_rate": 1.680029695619896e-05,
      "loss": 0.7639,
      "step": 8950
    },
    {
      "epoch": 1.995379390970328,
      "grad_norm": 0.7830954790115356,
      "learning_rate": 1.676317743132888e-05,
      "loss": 0.7904,
      "step": 8960
    },
    {
      "epoch": 1.9976061905027,
      "grad_norm": 0.71249920129776,
      "learning_rate": 1.6726057906458798e-05,
      "loss": 0.7276,
      "step": 8970
    },
    {
      "epoch": 1.9998329900350722,
      "grad_norm": 0.7471181750297546,
      "learning_rate": 1.6688938381588718e-05,
      "loss": 0.727,
      "step": 8980
    },
    {
      "epoch": 2.002226799532372,
      "grad_norm": 0.7203976511955261,
      "learning_rate": 1.6651818856718635e-05,
      "loss": 0.7845,
      "step": 8990
    },
    {
      "epoch": 2.0044535990647443,
      "grad_norm": 0.5973644256591797,
      "learning_rate": 1.6614699331848555e-05,
      "loss": 0.7314,
      "step": 9000
    },
    {
      "epoch": 2.0066803985971164,
      "grad_norm": 0.7598690986633301,
      "learning_rate": 1.657757980697847e-05,
      "loss": 0.7623,
      "step": 9010
    },
    {
      "epoch": 2.0089071981294886,
      "grad_norm": 0.7868139147758484,
      "learning_rate": 1.6540460282108388e-05,
      "loss": 0.7704,
      "step": 9020
    },
    {
      "epoch": 2.0111339976618603,
      "grad_norm": 0.6317687034606934,
      "learning_rate": 1.6503340757238308e-05,
      "loss": 0.7275,
      "step": 9030
    },
    {
      "epoch": 2.0133607971942324,
      "grad_norm": 0.7845335006713867,
      "learning_rate": 1.6466221232368225e-05,
      "loss": 0.7465,
      "step": 9040
    },
    {
      "epoch": 2.0155875967266046,
      "grad_norm": 0.7219046950340271,
      "learning_rate": 1.6429101707498145e-05,
      "loss": 0.7591,
      "step": 9050
    },
    {
      "epoch": 2.0178143962589767,
      "grad_norm": 0.6576316356658936,
      "learning_rate": 1.639198218262806e-05,
      "loss": 0.7235,
      "step": 9060
    },
    {
      "epoch": 2.020041195791349,
      "grad_norm": 0.7388752698898315,
      "learning_rate": 1.635486265775798e-05,
      "loss": 0.6935,
      "step": 9070
    },
    {
      "epoch": 2.022267995323721,
      "grad_norm": 0.779486894607544,
      "learning_rate": 1.63177431328879e-05,
      "loss": 0.7853,
      "step": 9080
    },
    {
      "epoch": 2.024494794856093,
      "grad_norm": 0.7545512914657593,
      "learning_rate": 1.6280623608017818e-05,
      "loss": 0.7042,
      "step": 9090
    },
    {
      "epoch": 2.0267215943884653,
      "grad_norm": 0.7376537322998047,
      "learning_rate": 1.6243504083147738e-05,
      "loss": 0.7146,
      "step": 9100
    },
    {
      "epoch": 2.0289483939208375,
      "grad_norm": 0.7906866073608398,
      "learning_rate": 1.6206384558277654e-05,
      "loss": 0.7151,
      "step": 9110
    },
    {
      "epoch": 2.031175193453209,
      "grad_norm": 0.757104754447937,
      "learning_rate": 1.6169265033407574e-05,
      "loss": 0.7751,
      "step": 9120
    },
    {
      "epoch": 2.0334019929855813,
      "grad_norm": 0.7302558422088623,
      "learning_rate": 1.613214550853749e-05,
      "loss": 0.7219,
      "step": 9130
    },
    {
      "epoch": 2.0356287925179535,
      "grad_norm": 0.7752398252487183,
      "learning_rate": 1.609502598366741e-05,
      "loss": 0.7607,
      "step": 9140
    },
    {
      "epoch": 2.0378555920503256,
      "grad_norm": 0.6552326083183289,
      "learning_rate": 1.6057906458797328e-05,
      "loss": 0.7693,
      "step": 9150
    },
    {
      "epoch": 2.0400823915826978,
      "grad_norm": 0.625669538974762,
      "learning_rate": 1.6020786933927244e-05,
      "loss": 0.7515,
      "step": 9160
    },
    {
      "epoch": 2.04230919111507,
      "grad_norm": 0.7119101881980896,
      "learning_rate": 1.5983667409057164e-05,
      "loss": 0.7705,
      "step": 9170
    },
    {
      "epoch": 2.044535990647442,
      "grad_norm": 0.7537668347358704,
      "learning_rate": 1.594654788418708e-05,
      "loss": 0.7723,
      "step": 9180
    },
    {
      "epoch": 2.046762790179814,
      "grad_norm": 0.789322018623352,
      "learning_rate": 1.5909428359317e-05,
      "loss": 0.7528,
      "step": 9190
    },
    {
      "epoch": 2.0489895897121864,
      "grad_norm": 0.7390471696853638,
      "learning_rate": 1.587230883444692e-05,
      "loss": 0.7135,
      "step": 9200
    },
    {
      "epoch": 2.051216389244558,
      "grad_norm": 0.8641567826271057,
      "learning_rate": 1.583518930957684e-05,
      "loss": 0.7509,
      "step": 9210
    },
    {
      "epoch": 2.05344318877693,
      "grad_norm": 0.7811709642410278,
      "learning_rate": 1.5798069784706758e-05,
      "loss": 0.6969,
      "step": 9220
    },
    {
      "epoch": 2.0556699883093024,
      "grad_norm": 0.8094924092292786,
      "learning_rate": 1.5760950259836674e-05,
      "loss": 0.7096,
      "step": 9230
    },
    {
      "epoch": 2.0578967878416745,
      "grad_norm": 0.6878593564033508,
      "learning_rate": 1.5723830734966594e-05,
      "loss": 0.7505,
      "step": 9240
    },
    {
      "epoch": 2.0601235873740467,
      "grad_norm": 0.6813943386077881,
      "learning_rate": 1.568671121009651e-05,
      "loss": 0.7235,
      "step": 9250
    },
    {
      "epoch": 2.062350386906419,
      "grad_norm": 0.6638243198394775,
      "learning_rate": 1.564959168522643e-05,
      "loss": 0.7126,
      "step": 9260
    },
    {
      "epoch": 2.064577186438791,
      "grad_norm": 0.7310127019882202,
      "learning_rate": 1.5612472160356347e-05,
      "loss": 0.739,
      "step": 9270
    },
    {
      "epoch": 2.066803985971163,
      "grad_norm": 0.791388213634491,
      "learning_rate": 1.5575352635486267e-05,
      "loss": 0.7749,
      "step": 9280
    },
    {
      "epoch": 2.0690307855035353,
      "grad_norm": 0.672829270362854,
      "learning_rate": 1.5538233110616184e-05,
      "loss": 0.7505,
      "step": 9290
    },
    {
      "epoch": 2.071257585035907,
      "grad_norm": 0.7808369994163513,
      "learning_rate": 1.55011135857461e-05,
      "loss": 0.6941,
      "step": 9300
    },
    {
      "epoch": 2.073484384568279,
      "grad_norm": 0.7828072309494019,
      "learning_rate": 1.546399406087602e-05,
      "loss": 0.7483,
      "step": 9310
    },
    {
      "epoch": 2.0757111841006513,
      "grad_norm": 0.7073925137519836,
      "learning_rate": 1.542687453600594e-05,
      "loss": 0.73,
      "step": 9320
    },
    {
      "epoch": 2.0779379836330234,
      "grad_norm": 0.742262601852417,
      "learning_rate": 1.538975501113586e-05,
      "loss": 0.7741,
      "step": 9330
    },
    {
      "epoch": 2.0801647831653955,
      "grad_norm": 0.7340197563171387,
      "learning_rate": 1.5352635486265777e-05,
      "loss": 0.7371,
      "step": 9340
    },
    {
      "epoch": 2.0823915826977677,
      "grad_norm": 0.7046669721603394,
      "learning_rate": 1.5315515961395697e-05,
      "loss": 0.7498,
      "step": 9350
    },
    {
      "epoch": 2.08461838223014,
      "grad_norm": 0.7622023224830627,
      "learning_rate": 1.5278396436525614e-05,
      "loss": 0.7205,
      "step": 9360
    },
    {
      "epoch": 2.086845181762512,
      "grad_norm": 0.7686823606491089,
      "learning_rate": 1.524127691165553e-05,
      "loss": 0.7385,
      "step": 9370
    },
    {
      "epoch": 2.089071981294884,
      "grad_norm": 0.7458778619766235,
      "learning_rate": 1.520415738678545e-05,
      "loss": 0.7813,
      "step": 9380
    },
    {
      "epoch": 2.091298780827256,
      "grad_norm": 0.7680848240852356,
      "learning_rate": 1.5167037861915367e-05,
      "loss": 0.7537,
      "step": 9390
    },
    {
      "epoch": 2.093525580359628,
      "grad_norm": 0.6816253662109375,
      "learning_rate": 1.5129918337045287e-05,
      "loss": 0.7329,
      "step": 9400
    },
    {
      "epoch": 2.095752379892,
      "grad_norm": 0.7450854182243347,
      "learning_rate": 1.5092798812175204e-05,
      "loss": 0.739,
      "step": 9410
    },
    {
      "epoch": 2.0979791794243723,
      "grad_norm": 0.7126820087432861,
      "learning_rate": 1.5055679287305124e-05,
      "loss": 0.7464,
      "step": 9420
    },
    {
      "epoch": 2.1002059789567444,
      "grad_norm": 0.7934643030166626,
      "learning_rate": 1.5018559762435042e-05,
      "loss": 0.6816,
      "step": 9430
    },
    {
      "epoch": 2.1024327784891166,
      "grad_norm": 1.0699161291122437,
      "learning_rate": 1.4981440237564959e-05,
      "loss": 0.6736,
      "step": 9440
    },
    {
      "epoch": 2.1046595780214887,
      "grad_norm": 0.7493934035301208,
      "learning_rate": 1.4944320712694879e-05,
      "loss": 0.7661,
      "step": 9450
    },
    {
      "epoch": 2.106886377553861,
      "grad_norm": 0.8046513795852661,
      "learning_rate": 1.4907201187824795e-05,
      "loss": 0.7798,
      "step": 9460
    },
    {
      "epoch": 2.109113177086233,
      "grad_norm": 0.7934461236000061,
      "learning_rate": 1.4870081662954715e-05,
      "loss": 0.776,
      "step": 9470
    },
    {
      "epoch": 2.1113399766186047,
      "grad_norm": 0.8133106827735901,
      "learning_rate": 1.4832962138084634e-05,
      "loss": 0.69,
      "step": 9480
    },
    {
      "epoch": 2.113566776150977,
      "grad_norm": 0.7425136566162109,
      "learning_rate": 1.4795842613214554e-05,
      "loss": 0.6896,
      "step": 9490
    },
    {
      "epoch": 2.115793575683349,
      "grad_norm": 0.7014032006263733,
      "learning_rate": 1.475872308834447e-05,
      "loss": 0.7699,
      "step": 9500
    },
    {
      "epoch": 2.118020375215721,
      "grad_norm": 0.809967577457428,
      "learning_rate": 1.4721603563474387e-05,
      "loss": 0.7537,
      "step": 9510
    },
    {
      "epoch": 2.1202471747480933,
      "grad_norm": 0.6869058012962341,
      "learning_rate": 1.4684484038604307e-05,
      "loss": 0.7228,
      "step": 9520
    },
    {
      "epoch": 2.1224739742804655,
      "grad_norm": 0.7004225850105286,
      "learning_rate": 1.4647364513734223e-05,
      "loss": 0.7481,
      "step": 9530
    },
    {
      "epoch": 2.1247007738128376,
      "grad_norm": 0.6404327750205994,
      "learning_rate": 1.4610244988864143e-05,
      "loss": 0.7327,
      "step": 9540
    },
    {
      "epoch": 2.1269275733452098,
      "grad_norm": 0.715083658695221,
      "learning_rate": 1.4573125463994062e-05,
      "loss": 0.7707,
      "step": 9550
    },
    {
      "epoch": 2.129154372877582,
      "grad_norm": 0.8668566346168518,
      "learning_rate": 1.4536005939123982e-05,
      "loss": 0.8099,
      "step": 9560
    },
    {
      "epoch": 2.1313811724099536,
      "grad_norm": 0.7466904520988464,
      "learning_rate": 1.4498886414253898e-05,
      "loss": 0.7526,
      "step": 9570
    },
    {
      "epoch": 2.1336079719423258,
      "grad_norm": 0.7535880208015442,
      "learning_rate": 1.4461766889383815e-05,
      "loss": 0.8005,
      "step": 9580
    },
    {
      "epoch": 2.135834771474698,
      "grad_norm": 0.7375733256340027,
      "learning_rate": 1.4424647364513735e-05,
      "loss": 0.7506,
      "step": 9590
    },
    {
      "epoch": 2.13806157100707,
      "grad_norm": 0.7609351277351379,
      "learning_rate": 1.4387527839643652e-05,
      "loss": 0.7029,
      "step": 9600
    },
    {
      "epoch": 2.140288370539442,
      "grad_norm": 0.7732827663421631,
      "learning_rate": 1.4350408314773572e-05,
      "loss": 0.7714,
      "step": 9610
    },
    {
      "epoch": 2.1425151700718144,
      "grad_norm": 0.7200250625610352,
      "learning_rate": 1.431328878990349e-05,
      "loss": 0.7386,
      "step": 9620
    },
    {
      "epoch": 2.1447419696041865,
      "grad_norm": 0.6946061849594116,
      "learning_rate": 1.427616926503341e-05,
      "loss": 0.7505,
      "step": 9630
    },
    {
      "epoch": 2.1469687691365587,
      "grad_norm": 0.8355159163475037,
      "learning_rate": 1.4239049740163327e-05,
      "loss": 0.7632,
      "step": 9640
    },
    {
      "epoch": 2.1491955686689304,
      "grad_norm": 0.8062070608139038,
      "learning_rate": 1.4201930215293243e-05,
      "loss": 0.6858,
      "step": 9650
    },
    {
      "epoch": 2.1514223682013025,
      "grad_norm": 0.7058267593383789,
      "learning_rate": 1.4164810690423163e-05,
      "loss": 0.8086,
      "step": 9660
    },
    {
      "epoch": 2.1536491677336747,
      "grad_norm": 0.7586233615875244,
      "learning_rate": 1.4127691165553081e-05,
      "loss": 0.7657,
      "step": 9670
    },
    {
      "epoch": 2.155875967266047,
      "grad_norm": 0.6536691784858704,
      "learning_rate": 1.4090571640683001e-05,
      "loss": 0.702,
      "step": 9680
    },
    {
      "epoch": 2.158102766798419,
      "grad_norm": 0.7199558019638062,
      "learning_rate": 1.4053452115812918e-05,
      "loss": 0.7344,
      "step": 9690
    },
    {
      "epoch": 2.160329566330791,
      "grad_norm": 0.7588039636611938,
      "learning_rate": 1.4016332590942838e-05,
      "loss": 0.8058,
      "step": 9700
    },
    {
      "epoch": 2.1625563658631632,
      "grad_norm": 0.830077052116394,
      "learning_rate": 1.3979213066072755e-05,
      "loss": 0.7377,
      "step": 9710
    },
    {
      "epoch": 2.1647831653955354,
      "grad_norm": 0.6739737391471863,
      "learning_rate": 1.3942093541202671e-05,
      "loss": 0.7382,
      "step": 9720
    },
    {
      "epoch": 2.1670099649279075,
      "grad_norm": 0.7849642634391785,
      "learning_rate": 1.3904974016332591e-05,
      "loss": 0.7607,
      "step": 9730
    },
    {
      "epoch": 2.1692367644602797,
      "grad_norm": 0.7349286675453186,
      "learning_rate": 1.386785449146251e-05,
      "loss": 0.7073,
      "step": 9740
    },
    {
      "epoch": 2.1714635639926514,
      "grad_norm": 0.6827694773674011,
      "learning_rate": 1.383073496659243e-05,
      "loss": 0.7628,
      "step": 9750
    },
    {
      "epoch": 2.1736903635250235,
      "grad_norm": 0.6443653702735901,
      "learning_rate": 1.3793615441722346e-05,
      "loss": 0.7108,
      "step": 9760
    },
    {
      "epoch": 2.1759171630573957,
      "grad_norm": 0.6789073348045349,
      "learning_rate": 1.3756495916852266e-05,
      "loss": 0.7132,
      "step": 9770
    },
    {
      "epoch": 2.178143962589768,
      "grad_norm": 0.837579607963562,
      "learning_rate": 1.3719376391982183e-05,
      "loss": 0.7476,
      "step": 9780
    },
    {
      "epoch": 2.18037076212214,
      "grad_norm": 0.7955603003501892,
      "learning_rate": 1.3682256867112101e-05,
      "loss": 0.7054,
      "step": 9790
    },
    {
      "epoch": 2.182597561654512,
      "grad_norm": 0.7192939519882202,
      "learning_rate": 1.364513734224202e-05,
      "loss": 0.7324,
      "step": 9800
    },
    {
      "epoch": 2.1848243611868843,
      "grad_norm": 0.714181125164032,
      "learning_rate": 1.3608017817371938e-05,
      "loss": 0.7574,
      "step": 9810
    },
    {
      "epoch": 2.1870511607192564,
      "grad_norm": 0.6412817239761353,
      "learning_rate": 1.3570898292501858e-05,
      "loss": 0.7703,
      "step": 9820
    },
    {
      "epoch": 2.189277960251628,
      "grad_norm": 0.7763463258743286,
      "learning_rate": 1.3533778767631774e-05,
      "loss": 0.7117,
      "step": 9830
    },
    {
      "epoch": 2.1915047597840003,
      "grad_norm": 0.8150787949562073,
      "learning_rate": 1.3496659242761694e-05,
      "loss": 0.7544,
      "step": 9840
    },
    {
      "epoch": 2.1937315593163724,
      "grad_norm": 0.7827996611595154,
      "learning_rate": 1.3459539717891611e-05,
      "loss": 0.7373,
      "step": 9850
    },
    {
      "epoch": 2.1959583588487446,
      "grad_norm": 0.7692590355873108,
      "learning_rate": 1.342242019302153e-05,
      "loss": 0.7627,
      "step": 9860
    },
    {
      "epoch": 2.1981851583811167,
      "grad_norm": 0.8026083111763,
      "learning_rate": 1.338530066815145e-05,
      "loss": 0.758,
      "step": 9870
    },
    {
      "epoch": 2.200411957913489,
      "grad_norm": 0.7212969660758972,
      "learning_rate": 1.3348181143281366e-05,
      "loss": 0.6986,
      "step": 9880
    },
    {
      "epoch": 2.202638757445861,
      "grad_norm": 0.7587764263153076,
      "learning_rate": 1.3311061618411286e-05,
      "loss": 0.7232,
      "step": 9890
    },
    {
      "epoch": 2.204865556978233,
      "grad_norm": 0.7205267548561096,
      "learning_rate": 1.3273942093541203e-05,
      "loss": 0.7579,
      "step": 9900
    },
    {
      "epoch": 2.2070923565106053,
      "grad_norm": 0.8322075605392456,
      "learning_rate": 1.3236822568671123e-05,
      "loss": 0.7185,
      "step": 9910
    },
    {
      "epoch": 2.2093191560429775,
      "grad_norm": 0.7362237572669983,
      "learning_rate": 1.319970304380104e-05,
      "loss": 0.7171,
      "step": 9920
    },
    {
      "epoch": 2.211545955575349,
      "grad_norm": 0.8419303894042969,
      "learning_rate": 1.3162583518930958e-05,
      "loss": 0.7367,
      "step": 9930
    },
    {
      "epoch": 2.2137727551077213,
      "grad_norm": 0.7669885754585266,
      "learning_rate": 1.3125463994060878e-05,
      "loss": 0.7128,
      "step": 9940
    },
    {
      "epoch": 2.2159995546400935,
      "grad_norm": 0.6687613129615784,
      "learning_rate": 1.3088344469190794e-05,
      "loss": 0.7774,
      "step": 9950
    },
    {
      "epoch": 2.2182263541724656,
      "grad_norm": 0.7815169095993042,
      "learning_rate": 1.3051224944320714e-05,
      "loss": 0.7628,
      "step": 9960
    },
    {
      "epoch": 2.2204531537048378,
      "grad_norm": 0.7022247910499573,
      "learning_rate": 1.301410541945063e-05,
      "loss": 0.6871,
      "step": 9970
    },
    {
      "epoch": 2.22267995323721,
      "grad_norm": 0.746131956577301,
      "learning_rate": 1.297698589458055e-05,
      "loss": 0.6933,
      "step": 9980
    },
    {
      "epoch": 2.224906752769582,
      "grad_norm": 0.8256138563156128,
      "learning_rate": 1.2939866369710469e-05,
      "loss": 0.7707,
      "step": 9990
    },
    {
      "epoch": 2.227133552301954,
      "grad_norm": 0.6965668797492981,
      "learning_rate": 1.2902746844840386e-05,
      "loss": 0.6907,
      "step": 10000
    },
    {
      "epoch": 2.229360351834326,
      "grad_norm": 0.9168885350227356,
      "learning_rate": 1.2865627319970306e-05,
      "loss": 0.7344,
      "step": 10010
    },
    {
      "epoch": 2.231587151366698,
      "grad_norm": 0.7229090332984924,
      "learning_rate": 1.2828507795100222e-05,
      "loss": 0.7528,
      "step": 10020
    },
    {
      "epoch": 2.23381395089907,
      "grad_norm": 0.7306698560714722,
      "learning_rate": 1.2791388270230142e-05,
      "loss": 0.7519,
      "step": 10030
    },
    {
      "epoch": 2.2360407504314423,
      "grad_norm": 0.7689445614814758,
      "learning_rate": 1.2754268745360059e-05,
      "loss": 0.7385,
      "step": 10040
    },
    {
      "epoch": 2.2382675499638145,
      "grad_norm": 0.8570152521133423,
      "learning_rate": 1.2717149220489979e-05,
      "loss": 0.7497,
      "step": 10050
    },
    {
      "epoch": 2.2404943494961866,
      "grad_norm": 0.835067868232727,
      "learning_rate": 1.2680029695619897e-05,
      "loss": 0.7806,
      "step": 10060
    },
    {
      "epoch": 2.242721149028559,
      "grad_norm": 0.7894138097763062,
      "learning_rate": 1.2642910170749814e-05,
      "loss": 0.7308,
      "step": 10070
    },
    {
      "epoch": 2.244947948560931,
      "grad_norm": 0.7685432434082031,
      "learning_rate": 1.2605790645879734e-05,
      "loss": 0.7334,
      "step": 10080
    },
    {
      "epoch": 2.247174748093303,
      "grad_norm": 0.7854786515235901,
      "learning_rate": 1.256867112100965e-05,
      "loss": 0.7218,
      "step": 10090
    },
    {
      "epoch": 2.249401547625675,
      "grad_norm": 0.6796964406967163,
      "learning_rate": 1.253155159613957e-05,
      "loss": 0.709,
      "step": 10100
    },
    {
      "epoch": 2.251628347158047,
      "grad_norm": 0.7445451617240906,
      "learning_rate": 1.2494432071269487e-05,
      "loss": 0.7191,
      "step": 10110
    },
    {
      "epoch": 2.253855146690419,
      "grad_norm": 0.7248079776763916,
      "learning_rate": 1.2457312546399407e-05,
      "loss": 0.749,
      "step": 10120
    },
    {
      "epoch": 2.2560819462227912,
      "grad_norm": 0.644089937210083,
      "learning_rate": 1.2420193021529325e-05,
      "loss": 0.7686,
      "step": 10130
    },
    {
      "epoch": 2.2583087457551634,
      "grad_norm": 0.7365722060203552,
      "learning_rate": 1.2383073496659244e-05,
      "loss": 0.6835,
      "step": 10140
    },
    {
      "epoch": 2.2605355452875355,
      "grad_norm": 0.7385188341140747,
      "learning_rate": 1.2345953971789162e-05,
      "loss": 0.7259,
      "step": 10150
    },
    {
      "epoch": 2.2627623448199077,
      "grad_norm": 0.749692440032959,
      "learning_rate": 1.230883444691908e-05,
      "loss": 0.7122,
      "step": 10160
    },
    {
      "epoch": 2.26498914435228,
      "grad_norm": 0.748412013053894,
      "learning_rate": 1.2271714922048997e-05,
      "loss": 0.7373,
      "step": 10170
    },
    {
      "epoch": 2.267215943884652,
      "grad_norm": 0.6931360363960266,
      "learning_rate": 1.2234595397178917e-05,
      "loss": 0.693,
      "step": 10180
    },
    {
      "epoch": 2.2694427434170237,
      "grad_norm": 0.8839157223701477,
      "learning_rate": 1.2197475872308835e-05,
      "loss": 0.7853,
      "step": 10190
    },
    {
      "epoch": 2.271669542949396,
      "grad_norm": 0.7525266408920288,
      "learning_rate": 1.2160356347438754e-05,
      "loss": 0.7473,
      "step": 10200
    },
    {
      "epoch": 2.273896342481768,
      "grad_norm": 0.7042366862297058,
      "learning_rate": 1.2123236822568672e-05,
      "loss": 0.732,
      "step": 10210
    },
    {
      "epoch": 2.27612314201414,
      "grad_norm": 0.6939811110496521,
      "learning_rate": 1.208611729769859e-05,
      "loss": 0.7446,
      "step": 10220
    },
    {
      "epoch": 2.2783499415465123,
      "grad_norm": 0.7622002959251404,
      "learning_rate": 1.2048997772828508e-05,
      "loss": 0.7748,
      "step": 10230
    },
    {
      "epoch": 2.2805767410788844,
      "grad_norm": 0.7266148924827576,
      "learning_rate": 1.2011878247958427e-05,
      "loss": 0.7593,
      "step": 10240
    },
    {
      "epoch": 2.2828035406112566,
      "grad_norm": 0.7027562260627747,
      "learning_rate": 1.1974758723088345e-05,
      "loss": 0.7331,
      "step": 10250
    },
    {
      "epoch": 2.2850303401436287,
      "grad_norm": 0.893906831741333,
      "learning_rate": 1.1937639198218263e-05,
      "loss": 0.7557,
      "step": 10260
    },
    {
      "epoch": 2.287257139676001,
      "grad_norm": 0.8105905652046204,
      "learning_rate": 1.1900519673348182e-05,
      "loss": 0.7234,
      "step": 10270
    },
    {
      "epoch": 2.289483939208373,
      "grad_norm": 0.705243706703186,
      "learning_rate": 1.18634001484781e-05,
      "loss": 0.7548,
      "step": 10280
    },
    {
      "epoch": 2.2917107387407447,
      "grad_norm": 0.8052021265029907,
      "learning_rate": 1.1826280623608018e-05,
      "loss": 0.7489,
      "step": 10290
    },
    {
      "epoch": 2.293937538273117,
      "grad_norm": 1.2721483707427979,
      "learning_rate": 1.1789161098737937e-05,
      "loss": 0.7718,
      "step": 10300
    },
    {
      "epoch": 2.296164337805489,
      "grad_norm": 0.7588550448417664,
      "learning_rate": 1.1752041573867855e-05,
      "loss": 0.7422,
      "step": 10310
    },
    {
      "epoch": 2.298391137337861,
      "grad_norm": 0.7452989220619202,
      "learning_rate": 1.1714922048997773e-05,
      "loss": 0.6859,
      "step": 10320
    },
    {
      "epoch": 2.3006179368702333,
      "grad_norm": 0.7697994709014893,
      "learning_rate": 1.1677802524127692e-05,
      "loss": 0.7135,
      "step": 10330
    },
    {
      "epoch": 2.3028447364026055,
      "grad_norm": 0.6984789371490479,
      "learning_rate": 1.164068299925761e-05,
      "loss": 0.755,
      "step": 10340
    },
    {
      "epoch": 2.3050715359349776,
      "grad_norm": 0.7685089111328125,
      "learning_rate": 1.1603563474387528e-05,
      "loss": 0.7533,
      "step": 10350
    },
    {
      "epoch": 2.3072983354673493,
      "grad_norm": 0.7858192920684814,
      "learning_rate": 1.1566443949517446e-05,
      "loss": 0.7268,
      "step": 10360
    },
    {
      "epoch": 2.3095251349997215,
      "grad_norm": 0.7449508905410767,
      "learning_rate": 1.1529324424647365e-05,
      "loss": 0.7523,
      "step": 10370
    },
    {
      "epoch": 2.3117519345320936,
      "grad_norm": 0.8089587688446045,
      "learning_rate": 1.1492204899777285e-05,
      "loss": 0.7387,
      "step": 10380
    },
    {
      "epoch": 2.3139787340644657,
      "grad_norm": 0.7214552760124207,
      "learning_rate": 1.1455085374907201e-05,
      "loss": 0.7991,
      "step": 10390
    },
    {
      "epoch": 2.316205533596838,
      "grad_norm": 0.7964183688163757,
      "learning_rate": 1.141796585003712e-05,
      "loss": 0.7212,
      "step": 10400
    },
    {
      "epoch": 2.31843233312921,
      "grad_norm": 0.7138116359710693,
      "learning_rate": 1.1380846325167038e-05,
      "loss": 0.7361,
      "step": 10410
    },
    {
      "epoch": 2.320659132661582,
      "grad_norm": 0.7254571914672852,
      "learning_rate": 1.1343726800296956e-05,
      "loss": 0.7691,
      "step": 10420
    },
    {
      "epoch": 2.3228859321939543,
      "grad_norm": 0.7485443353652954,
      "learning_rate": 1.1306607275426875e-05,
      "loss": 0.7535,
      "step": 10430
    },
    {
      "epoch": 2.3251127317263265,
      "grad_norm": 0.7830549478530884,
      "learning_rate": 1.1269487750556795e-05,
      "loss": 0.7815,
      "step": 10440
    },
    {
      "epoch": 2.3273395312586986,
      "grad_norm": 0.7013208866119385,
      "learning_rate": 1.1232368225686713e-05,
      "loss": 0.755,
      "step": 10450
    },
    {
      "epoch": 2.329566330791071,
      "grad_norm": 0.9470134377479553,
      "learning_rate": 1.119524870081663e-05,
      "loss": 0.735,
      "step": 10460
    },
    {
      "epoch": 2.3317931303234425,
      "grad_norm": 0.68473219871521,
      "learning_rate": 1.1158129175946548e-05,
      "loss": 0.733,
      "step": 10470
    },
    {
      "epoch": 2.3340199298558146,
      "grad_norm": 0.747498095035553,
      "learning_rate": 1.1121009651076466e-05,
      "loss": 0.7435,
      "step": 10480
    },
    {
      "epoch": 2.336246729388187,
      "grad_norm": 0.7246553897857666,
      "learning_rate": 1.1083890126206385e-05,
      "loss": 0.7503,
      "step": 10490
    },
    {
      "epoch": 2.338473528920559,
      "grad_norm": 0.7606720924377441,
      "learning_rate": 1.1046770601336305e-05,
      "loss": 0.7646,
      "step": 10500
    },
    {
      "epoch": 2.340700328452931,
      "grad_norm": 0.7232051491737366,
      "learning_rate": 1.1009651076466223e-05,
      "loss": 0.7157,
      "step": 10510
    },
    {
      "epoch": 2.3429271279853032,
      "grad_norm": 0.7460904717445374,
      "learning_rate": 1.0972531551596141e-05,
      "loss": 0.7368,
      "step": 10520
    },
    {
      "epoch": 2.3451539275176754,
      "grad_norm": 0.6940390467643738,
      "learning_rate": 1.0935412026726058e-05,
      "loss": 0.7729,
      "step": 10530
    },
    {
      "epoch": 2.347380727050047,
      "grad_norm": 0.6764605045318604,
      "learning_rate": 1.0898292501855976e-05,
      "loss": 0.7342,
      "step": 10540
    },
    {
      "epoch": 2.3496075265824192,
      "grad_norm": 0.6561822891235352,
      "learning_rate": 1.0861172976985894e-05,
      "loss": 0.8079,
      "step": 10550
    },
    {
      "epoch": 2.3518343261147914,
      "grad_norm": 0.747394323348999,
      "learning_rate": 1.0824053452115813e-05,
      "loss": 0.7113,
      "step": 10560
    },
    {
      "epoch": 2.3540611256471635,
      "grad_norm": 0.726704478263855,
      "learning_rate": 1.0786933927245733e-05,
      "loss": 0.738,
      "step": 10570
    },
    {
      "epoch": 2.3562879251795357,
      "grad_norm": 0.8028461933135986,
      "learning_rate": 1.0749814402375651e-05,
      "loss": 0.7414,
      "step": 10580
    },
    {
      "epoch": 2.358514724711908,
      "grad_norm": 0.786730170249939,
      "learning_rate": 1.071269487750557e-05,
      "loss": 0.7377,
      "step": 10590
    },
    {
      "epoch": 2.36074152424428,
      "grad_norm": 0.6327607035636902,
      "learning_rate": 1.0675575352635486e-05,
      "loss": 0.7308,
      "step": 10600
    },
    {
      "epoch": 2.362968323776652,
      "grad_norm": 0.7195345163345337,
      "learning_rate": 1.0638455827765404e-05,
      "loss": 0.7637,
      "step": 10610
    },
    {
      "epoch": 2.3651951233090243,
      "grad_norm": 0.754361093044281,
      "learning_rate": 1.0601336302895323e-05,
      "loss": 0.7111,
      "step": 10620
    },
    {
      "epoch": 2.3674219228413964,
      "grad_norm": 1.3805732727050781,
      "learning_rate": 1.0564216778025243e-05,
      "loss": 0.7548,
      "step": 10630
    },
    {
      "epoch": 2.369648722373768,
      "grad_norm": 0.6978809833526611,
      "learning_rate": 1.052709725315516e-05,
      "loss": 0.7532,
      "step": 10640
    },
    {
      "epoch": 2.3718755219061403,
      "grad_norm": 0.6658161282539368,
      "learning_rate": 1.0489977728285079e-05,
      "loss": 0.7273,
      "step": 10650
    },
    {
      "epoch": 2.3741023214385124,
      "grad_norm": 0.6751264929771423,
      "learning_rate": 1.0452858203414997e-05,
      "loss": 0.7457,
      "step": 10660
    },
    {
      "epoch": 2.3763291209708846,
      "grad_norm": 0.7013352513313293,
      "learning_rate": 1.0415738678544914e-05,
      "loss": 0.7813,
      "step": 10670
    },
    {
      "epoch": 2.3785559205032567,
      "grad_norm": 0.6640909314155579,
      "learning_rate": 1.0378619153674832e-05,
      "loss": 0.7941,
      "step": 10680
    },
    {
      "epoch": 2.380782720035629,
      "grad_norm": 0.7707523107528687,
      "learning_rate": 1.0341499628804752e-05,
      "loss": 0.71,
      "step": 10690
    },
    {
      "epoch": 2.383009519568001,
      "grad_norm": 0.7297452092170715,
      "learning_rate": 1.030438010393467e-05,
      "loss": 0.7223,
      "step": 10700
    },
    {
      "epoch": 2.385236319100373,
      "grad_norm": 0.7357271313667297,
      "learning_rate": 1.0267260579064589e-05,
      "loss": 0.6992,
      "step": 10710
    },
    {
      "epoch": 2.387463118632745,
      "grad_norm": 0.6796754598617554,
      "learning_rate": 1.0230141054194507e-05,
      "loss": 0.7588,
      "step": 10720
    },
    {
      "epoch": 2.389689918165117,
      "grad_norm": 0.7118135690689087,
      "learning_rate": 1.0193021529324426e-05,
      "loss": 0.7974,
      "step": 10730
    },
    {
      "epoch": 2.391916717697489,
      "grad_norm": 1.0005841255187988,
      "learning_rate": 1.0155902004454342e-05,
      "loss": 0.7561,
      "step": 10740
    },
    {
      "epoch": 2.3941435172298613,
      "grad_norm": 0.7220930457115173,
      "learning_rate": 1.0118782479584262e-05,
      "loss": 0.7437,
      "step": 10750
    },
    {
      "epoch": 2.3963703167622334,
      "grad_norm": 0.7456995248794556,
      "learning_rate": 1.008166295471418e-05,
      "loss": 0.6914,
      "step": 10760
    },
    {
      "epoch": 2.3985971162946056,
      "grad_norm": 0.6843878626823425,
      "learning_rate": 1.0044543429844099e-05,
      "loss": 0.7314,
      "step": 10770
    },
    {
      "epoch": 2.4008239158269777,
      "grad_norm": 0.6711022853851318,
      "learning_rate": 1.0007423904974017e-05,
      "loss": 0.7172,
      "step": 10780
    },
    {
      "epoch": 2.40305071535935,
      "grad_norm": 0.6308370232582092,
      "learning_rate": 9.970304380103935e-06,
      "loss": 0.7421,
      "step": 10790
    },
    {
      "epoch": 2.405277514891722,
      "grad_norm": 0.7367758750915527,
      "learning_rate": 9.933184855233854e-06,
      "loss": 0.7513,
      "step": 10800
    },
    {
      "epoch": 2.407504314424094,
      "grad_norm": 0.7392134666442871,
      "learning_rate": 9.896065330363772e-06,
      "loss": 0.7287,
      "step": 10810
    },
    {
      "epoch": 2.409731113956466,
      "grad_norm": 0.9187365174293518,
      "learning_rate": 9.85894580549369e-06,
      "loss": 0.7716,
      "step": 10820
    },
    {
      "epoch": 2.411957913488838,
      "grad_norm": 0.7432980537414551,
      "learning_rate": 9.821826280623609e-06,
      "loss": 0.7495,
      "step": 10830
    },
    {
      "epoch": 2.41418471302121,
      "grad_norm": 0.7501955032348633,
      "learning_rate": 9.784706755753527e-06,
      "loss": 0.7557,
      "step": 10840
    },
    {
      "epoch": 2.4164115125535823,
      "grad_norm": 0.7088656425476074,
      "learning_rate": 9.747587230883445e-06,
      "loss": 0.745,
      "step": 10850
    },
    {
      "epoch": 2.4186383120859545,
      "grad_norm": 0.7132808566093445,
      "learning_rate": 9.710467706013364e-06,
      "loss": 0.7192,
      "step": 10860
    },
    {
      "epoch": 2.4208651116183266,
      "grad_norm": 0.7495602965354919,
      "learning_rate": 9.673348181143282e-06,
      "loss": 0.7117,
      "step": 10870
    },
    {
      "epoch": 2.4230919111506988,
      "grad_norm": 0.6984146237373352,
      "learning_rate": 9.6362286562732e-06,
      "loss": 0.7557,
      "step": 10880
    },
    {
      "epoch": 2.425318710683071,
      "grad_norm": 0.7148749828338623,
      "learning_rate": 9.599109131403119e-06,
      "loss": 0.6588,
      "step": 10890
    },
    {
      "epoch": 2.4275455102154426,
      "grad_norm": 0.6900761127471924,
      "learning_rate": 9.561989606533037e-06,
      "loss": 0.7399,
      "step": 10900
    },
    {
      "epoch": 2.4297723097478148,
      "grad_norm": 0.7075710892677307,
      "learning_rate": 9.524870081662955e-06,
      "loss": 0.7232,
      "step": 10910
    },
    {
      "epoch": 2.431999109280187,
      "grad_norm": 0.7391598224639893,
      "learning_rate": 9.487750556792873e-06,
      "loss": 0.7221,
      "step": 10920
    },
    {
      "epoch": 2.434225908812559,
      "grad_norm": 0.7415649890899658,
      "learning_rate": 9.450631031922792e-06,
      "loss": 0.7379,
      "step": 10930
    },
    {
      "epoch": 2.436452708344931,
      "grad_norm": 0.68328857421875,
      "learning_rate": 9.41351150705271e-06,
      "loss": 0.6991,
      "step": 10940
    },
    {
      "epoch": 2.4386795078773034,
      "grad_norm": 0.7465920448303223,
      "learning_rate": 9.376391982182628e-06,
      "loss": 0.7214,
      "step": 10950
    },
    {
      "epoch": 2.4409063074096755,
      "grad_norm": 0.7454827427864075,
      "learning_rate": 9.339272457312547e-06,
      "loss": 0.6987,
      "step": 10960
    },
    {
      "epoch": 2.4431331069420477,
      "grad_norm": 0.7356770634651184,
      "learning_rate": 9.302152932442465e-06,
      "loss": 0.748,
      "step": 10970
    },
    {
      "epoch": 2.44535990647442,
      "grad_norm": 0.7481067180633545,
      "learning_rate": 9.265033407572383e-06,
      "loss": 0.7115,
      "step": 10980
    },
    {
      "epoch": 2.447586706006792,
      "grad_norm": 0.74428790807724,
      "learning_rate": 9.227913882702302e-06,
      "loss": 0.7484,
      "step": 10990
    },
    {
      "epoch": 2.4498135055391637,
      "grad_norm": 0.7242940664291382,
      "learning_rate": 9.19079435783222e-06,
      "loss": 0.7005,
      "step": 11000
    },
    {
      "epoch": 2.452040305071536,
      "grad_norm": 0.7378360033035278,
      "learning_rate": 9.15367483296214e-06,
      "loss": 0.7327,
      "step": 11010
    },
    {
      "epoch": 2.454267104603908,
      "grad_norm": 0.7670759558677673,
      "learning_rate": 9.116555308092057e-06,
      "loss": 0.7089,
      "step": 11020
    },
    {
      "epoch": 2.45649390413628,
      "grad_norm": 0.6660552024841309,
      "learning_rate": 9.079435783221975e-06,
      "loss": 0.7284,
      "step": 11030
    },
    {
      "epoch": 2.4587207036686523,
      "grad_norm": 0.7319461703300476,
      "learning_rate": 9.042316258351893e-06,
      "loss": 0.6971,
      "step": 11040
    },
    {
      "epoch": 2.4609475032010244,
      "grad_norm": 0.8033047914505005,
      "learning_rate": 9.005196733481811e-06,
      "loss": 0.767,
      "step": 11050
    },
    {
      "epoch": 2.4631743027333965,
      "grad_norm": 0.6689284443855286,
      "learning_rate": 8.96807720861173e-06,
      "loss": 0.7452,
      "step": 11060
    },
    {
      "epoch": 2.4654011022657687,
      "grad_norm": 0.7412230372428894,
      "learning_rate": 8.930957683741648e-06,
      "loss": 0.7868,
      "step": 11070
    },
    {
      "epoch": 2.4676279017981404,
      "grad_norm": 0.6864338517189026,
      "learning_rate": 8.893838158871568e-06,
      "loss": 0.7517,
      "step": 11080
    },
    {
      "epoch": 2.4698547013305125,
      "grad_norm": 0.7189159989356995,
      "learning_rate": 8.856718634001485e-06,
      "loss": 0.7794,
      "step": 11090
    },
    {
      "epoch": 2.4720815008628847,
      "grad_norm": 0.7271289229393005,
      "learning_rate": 8.819599109131403e-06,
      "loss": 0.6931,
      "step": 11100
    },
    {
      "epoch": 2.474308300395257,
      "grad_norm": 0.7005749344825745,
      "learning_rate": 8.782479584261321e-06,
      "loss": 0.6934,
      "step": 11110
    },
    {
      "epoch": 2.476535099927629,
      "grad_norm": 0.7829218506813049,
      "learning_rate": 8.74536005939124e-06,
      "loss": 0.7476,
      "step": 11120
    },
    {
      "epoch": 2.478761899460001,
      "grad_norm": 0.7499237060546875,
      "learning_rate": 8.708240534521158e-06,
      "loss": 0.7139,
      "step": 11130
    },
    {
      "epoch": 2.4809886989923733,
      "grad_norm": 0.7295786738395691,
      "learning_rate": 8.671121009651078e-06,
      "loss": 0.7527,
      "step": 11140
    },
    {
      "epoch": 2.4832154985247454,
      "grad_norm": 0.7970958352088928,
      "learning_rate": 8.634001484780996e-06,
      "loss": 0.7588,
      "step": 11150
    },
    {
      "epoch": 2.4854422980571176,
      "grad_norm": 0.636131227016449,
      "learning_rate": 8.596881959910913e-06,
      "loss": 0.7271,
      "step": 11160
    },
    {
      "epoch": 2.4876690975894897,
      "grad_norm": 0.8051837682723999,
      "learning_rate": 8.559762435040831e-06,
      "loss": 0.6984,
      "step": 11170
    },
    {
      "epoch": 2.4898958971218614,
      "grad_norm": 0.7146771550178528,
      "learning_rate": 8.52264291017075e-06,
      "loss": 0.7016,
      "step": 11180
    },
    {
      "epoch": 2.4921226966542336,
      "grad_norm": 0.7894238233566284,
      "learning_rate": 8.485523385300668e-06,
      "loss": 0.7355,
      "step": 11190
    },
    {
      "epoch": 2.4943494961866057,
      "grad_norm": 0.808178186416626,
      "learning_rate": 8.448403860430588e-06,
      "loss": 0.7252,
      "step": 11200
    },
    {
      "epoch": 2.496576295718978,
      "grad_norm": 0.7434048652648926,
      "learning_rate": 8.411284335560506e-06,
      "loss": 0.7436,
      "step": 11210
    },
    {
      "epoch": 2.49880309525135,
      "grad_norm": 0.6844410300254822,
      "learning_rate": 8.374164810690424e-06,
      "loss": 0.7441,
      "step": 11220
    },
    {
      "epoch": 2.501029894783722,
      "grad_norm": 0.8023996353149414,
      "learning_rate": 8.337045285820341e-06,
      "loss": 0.7323,
      "step": 11230
    },
    {
      "epoch": 2.5032566943160943,
      "grad_norm": 0.7650938034057617,
      "learning_rate": 8.29992576095026e-06,
      "loss": 0.7401,
      "step": 11240
    },
    {
      "epoch": 2.505483493848466,
      "grad_norm": 0.71584153175354,
      "learning_rate": 8.262806236080178e-06,
      "loss": 0.7227,
      "step": 11250
    },
    {
      "epoch": 2.507710293380838,
      "grad_norm": 1.022473931312561,
      "learning_rate": 8.225686711210098e-06,
      "loss": 0.776,
      "step": 11260
    },
    {
      "epoch": 2.5099370929132103,
      "grad_norm": 0.6297802925109863,
      "learning_rate": 8.188567186340016e-06,
      "loss": 0.6792,
      "step": 11270
    },
    {
      "epoch": 2.5121638924455825,
      "grad_norm": 0.7493383884429932,
      "learning_rate": 8.151447661469934e-06,
      "loss": 0.7511,
      "step": 11280
    },
    {
      "epoch": 2.5143906919779546,
      "grad_norm": 0.8604684472084045,
      "learning_rate": 8.114328136599853e-06,
      "loss": 0.7318,
      "step": 11290
    },
    {
      "epoch": 2.5166174915103268,
      "grad_norm": 0.9925899505615234,
      "learning_rate": 8.07720861172977e-06,
      "loss": 0.7585,
      "step": 11300
    },
    {
      "epoch": 2.518844291042699,
      "grad_norm": 0.7607097625732422,
      "learning_rate": 8.040089086859688e-06,
      "loss": 0.7949,
      "step": 11310
    },
    {
      "epoch": 2.521071090575071,
      "grad_norm": 0.8448923230171204,
      "learning_rate": 8.002969561989606e-06,
      "loss": 0.7229,
      "step": 11320
    },
    {
      "epoch": 2.523297890107443,
      "grad_norm": 0.8058218955993652,
      "learning_rate": 7.965850037119526e-06,
      "loss": 0.7653,
      "step": 11330
    },
    {
      "epoch": 2.5255246896398154,
      "grad_norm": 0.7420839667320251,
      "learning_rate": 7.928730512249444e-06,
      "loss": 0.7401,
      "step": 11340
    },
    {
      "epoch": 2.5277514891721875,
      "grad_norm": 0.7349177598953247,
      "learning_rate": 7.891610987379362e-06,
      "loss": 0.7167,
      "step": 11350
    },
    {
      "epoch": 2.529978288704559,
      "grad_norm": 0.7462541460990906,
      "learning_rate": 7.85449146250928e-06,
      "loss": 0.7556,
      "step": 11360
    },
    {
      "epoch": 2.5322050882369314,
      "grad_norm": 0.8823104500770569,
      "learning_rate": 7.817371937639197e-06,
      "loss": 0.7026,
      "step": 11370
    },
    {
      "epoch": 2.5344318877693035,
      "grad_norm": 0.7821053266525269,
      "learning_rate": 7.780252412769116e-06,
      "loss": 0.7248,
      "step": 11380
    },
    {
      "epoch": 2.5366586873016757,
      "grad_norm": 0.8804827332496643,
      "learning_rate": 7.743132887899036e-06,
      "loss": 0.7307,
      "step": 11390
    },
    {
      "epoch": 2.538885486834048,
      "grad_norm": 0.7229669690132141,
      "learning_rate": 7.706013363028954e-06,
      "loss": 0.7504,
      "step": 11400
    },
    {
      "epoch": 2.54111228636642,
      "grad_norm": 0.7795013785362244,
      "learning_rate": 7.668893838158872e-06,
      "loss": 0.7515,
      "step": 11410
    },
    {
      "epoch": 2.543339085898792,
      "grad_norm": 0.808957576751709,
      "learning_rate": 7.63177431328879e-06,
      "loss": 0.8066,
      "step": 11420
    },
    {
      "epoch": 2.545565885431164,
      "grad_norm": 0.7416072487831116,
      "learning_rate": 7.594654788418709e-06,
      "loss": 0.7131,
      "step": 11430
    },
    {
      "epoch": 2.547792684963536,
      "grad_norm": 0.6848260760307312,
      "learning_rate": 7.557535263548626e-06,
      "loss": 0.7691,
      "step": 11440
    },
    {
      "epoch": 2.550019484495908,
      "grad_norm": 0.7184826135635376,
      "learning_rate": 7.520415738678545e-06,
      "loss": 0.7602,
      "step": 11450
    },
    {
      "epoch": 2.5522462840282802,
      "grad_norm": 0.7829497456550598,
      "learning_rate": 7.483296213808463e-06,
      "loss": 0.7216,
      "step": 11460
    },
    {
      "epoch": 2.5544730835606524,
      "grad_norm": 0.7988249659538269,
      "learning_rate": 7.446176688938382e-06,
      "loss": 0.782,
      "step": 11470
    },
    {
      "epoch": 2.5566998830930245,
      "grad_norm": 0.7996052503585815,
      "learning_rate": 7.4090571640683005e-06,
      "loss": 0.7506,
      "step": 11480
    },
    {
      "epoch": 2.5589266826253967,
      "grad_norm": 0.7554916739463806,
      "learning_rate": 7.371937639198219e-06,
      "loss": 0.7556,
      "step": 11490
    },
    {
      "epoch": 2.561153482157769,
      "grad_norm": 0.8532869815826416,
      "learning_rate": 7.334818114328137e-06,
      "loss": 0.7624,
      "step": 11500
    },
    {
      "epoch": 2.563380281690141,
      "grad_norm": 0.7025195956230164,
      "learning_rate": 7.2976985894580545e-06,
      "loss": 0.6914,
      "step": 11510
    },
    {
      "epoch": 2.565607081222513,
      "grad_norm": 0.7352007031440735,
      "learning_rate": 7.260579064587973e-06,
      "loss": 0.7539,
      "step": 11520
    },
    {
      "epoch": 2.5678338807548853,
      "grad_norm": 0.654511034488678,
      "learning_rate": 7.223459539717892e-06,
      "loss": 0.7098,
      "step": 11530
    },
    {
      "epoch": 2.570060680287257,
      "grad_norm": 0.6753273010253906,
      "learning_rate": 7.18634001484781e-06,
      "loss": 0.7419,
      "step": 11540
    },
    {
      "epoch": 2.572287479819629,
      "grad_norm": 0.6295565962791443,
      "learning_rate": 7.149220489977729e-06,
      "loss": 0.6971,
      "step": 11550
    },
    {
      "epoch": 2.5745142793520013,
      "grad_norm": 0.951565682888031,
      "learning_rate": 7.112100965107647e-06,
      "loss": 0.7252,
      "step": 11560
    },
    {
      "epoch": 2.5767410788843734,
      "grad_norm": 0.8916395902633667,
      "learning_rate": 7.074981440237566e-06,
      "loss": 0.7328,
      "step": 11570
    },
    {
      "epoch": 2.5789678784167456,
      "grad_norm": 0.6728563904762268,
      "learning_rate": 7.037861915367483e-06,
      "loss": 0.751,
      "step": 11580
    },
    {
      "epoch": 2.5811946779491177,
      "grad_norm": 0.779775083065033,
      "learning_rate": 7.000742390497402e-06,
      "loss": 0.7275,
      "step": 11590
    },
    {
      "epoch": 2.58342147748149,
      "grad_norm": 0.6883532404899597,
      "learning_rate": 6.96362286562732e-06,
      "loss": 0.7389,
      "step": 11600
    },
    {
      "epoch": 2.5856482770138616,
      "grad_norm": 0.7361308932304382,
      "learning_rate": 6.9265033407572385e-06,
      "loss": 0.7275,
      "step": 11610
    },
    {
      "epoch": 2.5878750765462337,
      "grad_norm": 0.7069658041000366,
      "learning_rate": 6.889383815887157e-06,
      "loss": 0.7112,
      "step": 11620
    },
    {
      "epoch": 2.590101876078606,
      "grad_norm": 0.7202081680297852,
      "learning_rate": 6.852264291017076e-06,
      "loss": 0.7714,
      "step": 11630
    },
    {
      "epoch": 2.592328675610978,
      "grad_norm": 0.7887707948684692,
      "learning_rate": 6.815144766146994e-06,
      "loss": 0.8172,
      "step": 11640
    },
    {
      "epoch": 2.59455547514335,
      "grad_norm": 0.7716211676597595,
      "learning_rate": 6.778025241276912e-06,
      "loss": 0.7316,
      "step": 11650
    },
    {
      "epoch": 2.5967822746757223,
      "grad_norm": 0.7532944083213806,
      "learning_rate": 6.74090571640683e-06,
      "loss": 0.7504,
      "step": 11660
    },
    {
      "epoch": 2.5990090742080945,
      "grad_norm": 0.7601661086082458,
      "learning_rate": 6.703786191536748e-06,
      "loss": 0.7289,
      "step": 11670
    },
    {
      "epoch": 2.6012358737404666,
      "grad_norm": 0.7102547883987427,
      "learning_rate": 6.666666666666667e-06,
      "loss": 0.7293,
      "step": 11680
    },
    {
      "epoch": 2.6034626732728388,
      "grad_norm": 0.7373802661895752,
      "learning_rate": 6.629547141796586e-06,
      "loss": 0.7668,
      "step": 11690
    },
    {
      "epoch": 2.605689472805211,
      "grad_norm": 0.7396079897880554,
      "learning_rate": 6.592427616926504e-06,
      "loss": 0.7408,
      "step": 11700
    },
    {
      "epoch": 2.607916272337583,
      "grad_norm": 0.8363624215126038,
      "learning_rate": 6.555308092056422e-06,
      "loss": 0.7269,
      "step": 11710
    },
    {
      "epoch": 2.6101430718699548,
      "grad_norm": 0.6994203925132751,
      "learning_rate": 6.51818856718634e-06,
      "loss": 0.7494,
      "step": 11720
    },
    {
      "epoch": 2.612369871402327,
      "grad_norm": 0.6072250604629517,
      "learning_rate": 6.481069042316258e-06,
      "loss": 0.7082,
      "step": 11730
    },
    {
      "epoch": 2.614596670934699,
      "grad_norm": 0.788539469242096,
      "learning_rate": 6.4439495174461765e-06,
      "loss": 0.7546,
      "step": 11740
    },
    {
      "epoch": 2.616823470467071,
      "grad_norm": 0.7103947997093201,
      "learning_rate": 6.406829992576095e-06,
      "loss": 0.7268,
      "step": 11750
    },
    {
      "epoch": 2.6190502699994433,
      "grad_norm": 0.6970206499099731,
      "learning_rate": 6.369710467706014e-06,
      "loss": 0.6903,
      "step": 11760
    },
    {
      "epoch": 2.6212770695318155,
      "grad_norm": 0.721032977104187,
      "learning_rate": 6.332590942835932e-06,
      "loss": 0.7438,
      "step": 11770
    },
    {
      "epoch": 2.6235038690641876,
      "grad_norm": 0.8781286478042603,
      "learning_rate": 6.295471417965851e-06,
      "loss": 0.7535,
      "step": 11780
    },
    {
      "epoch": 2.6257306685965593,
      "grad_norm": 0.7606468200683594,
      "learning_rate": 6.25835189309577e-06,
      "loss": 0.7062,
      "step": 11790
    },
    {
      "epoch": 2.6279574681289315,
      "grad_norm": 0.757134199142456,
      "learning_rate": 6.221232368225687e-06,
      "loss": 0.7828,
      "step": 11800
    },
    {
      "epoch": 2.6301842676613036,
      "grad_norm": 0.788781464099884,
      "learning_rate": 6.184112843355605e-06,
      "loss": 0.7266,
      "step": 11810
    },
    {
      "epoch": 2.632411067193676,
      "grad_norm": 0.7617833614349365,
      "learning_rate": 6.146993318485524e-06,
      "loss": 0.7094,
      "step": 11820
    },
    {
      "epoch": 2.634637866726048,
      "grad_norm": 0.6232205033302307,
      "learning_rate": 6.109873793615442e-06,
      "loss": 0.7803,
      "step": 11830
    },
    {
      "epoch": 2.63686466625842,
      "grad_norm": 0.8100544214248657,
      "learning_rate": 6.0727542687453604e-06,
      "loss": 0.6872,
      "step": 11840
    },
    {
      "epoch": 2.6390914657907922,
      "grad_norm": 0.7648354172706604,
      "learning_rate": 6.035634743875279e-06,
      "loss": 0.754,
      "step": 11850
    },
    {
      "epoch": 2.6413182653231644,
      "grad_norm": 0.7875551581382751,
      "learning_rate": 5.998515219005197e-06,
      "loss": 0.7175,
      "step": 11860
    },
    {
      "epoch": 2.6435450648555365,
      "grad_norm": 0.7407645583152771,
      "learning_rate": 5.961395694135115e-06,
      "loss": 0.7448,
      "step": 11870
    },
    {
      "epoch": 2.6457718643879087,
      "grad_norm": 0.809131920337677,
      "learning_rate": 5.924276169265034e-06,
      "loss": 0.7789,
      "step": 11880
    },
    {
      "epoch": 2.647998663920281,
      "grad_norm": 0.8983550667762756,
      "learning_rate": 5.887156644394952e-06,
      "loss": 0.7959,
      "step": 11890
    },
    {
      "epoch": 2.6502254634526525,
      "grad_norm": 0.8108277916908264,
      "learning_rate": 5.85003711952487e-06,
      "loss": 0.7689,
      "step": 11900
    },
    {
      "epoch": 2.6524522629850247,
      "grad_norm": 0.7805709838867188,
      "learning_rate": 5.812917594654789e-06,
      "loss": 0.698,
      "step": 11910
    },
    {
      "epoch": 2.654679062517397,
      "grad_norm": 0.7519738674163818,
      "learning_rate": 5.775798069784707e-06,
      "loss": 0.7248,
      "step": 11920
    },
    {
      "epoch": 2.656905862049769,
      "grad_norm": 0.7050553560256958,
      "learning_rate": 5.738678544914625e-06,
      "loss": 0.7254,
      "step": 11930
    },
    {
      "epoch": 2.659132661582141,
      "grad_norm": 0.88135826587677,
      "learning_rate": 5.7015590200445435e-06,
      "loss": 0.7068,
      "step": 11940
    },
    {
      "epoch": 2.6613594611145133,
      "grad_norm": 0.6686872243881226,
      "learning_rate": 5.664439495174462e-06,
      "loss": 0.7508,
      "step": 11950
    },
    {
      "epoch": 2.6635862606468854,
      "grad_norm": 0.9405054450035095,
      "learning_rate": 5.62731997030438e-06,
      "loss": 0.7928,
      "step": 11960
    },
    {
      "epoch": 2.665813060179257,
      "grad_norm": 0.7617512941360474,
      "learning_rate": 5.5902004454342985e-06,
      "loss": 0.7302,
      "step": 11970
    },
    {
      "epoch": 2.6680398597116293,
      "grad_norm": 0.692355751991272,
      "learning_rate": 5.553080920564218e-06,
      "loss": 0.7344,
      "step": 11980
    },
    {
      "epoch": 2.6702666592440014,
      "grad_norm": 0.705193042755127,
      "learning_rate": 5.515961395694135e-06,
      "loss": 0.7285,
      "step": 11990
    },
    {
      "epoch": 2.6724934587763736,
      "grad_norm": 0.8299221396446228,
      "learning_rate": 5.478841870824053e-06,
      "loss": 0.7339,
      "step": 12000
    },
    {
      "epoch": 2.6747202583087457,
      "grad_norm": 0.7622255086898804,
      "learning_rate": 5.4417223459539725e-06,
      "loss": 0.6789,
      "step": 12010
    },
    {
      "epoch": 2.676947057841118,
      "grad_norm": 0.6902635097503662,
      "learning_rate": 5.40460282108389e-06,
      "loss": 0.6966,
      "step": 12020
    },
    {
      "epoch": 2.67917385737349,
      "grad_norm": 0.6966426372528076,
      "learning_rate": 5.367483296213808e-06,
      "loss": 0.7389,
      "step": 12030
    },
    {
      "epoch": 2.681400656905862,
      "grad_norm": 0.7749346494674683,
      "learning_rate": 5.3303637713437275e-06,
      "loss": 0.7993,
      "step": 12040
    },
    {
      "epoch": 2.6836274564382343,
      "grad_norm": 0.7190611362457275,
      "learning_rate": 5.293244246473646e-06,
      "loss": 0.7325,
      "step": 12050
    },
    {
      "epoch": 2.6858542559706065,
      "grad_norm": 0.737410843372345,
      "learning_rate": 5.256124721603563e-06,
      "loss": 0.744,
      "step": 12060
    },
    {
      "epoch": 2.6880810555029786,
      "grad_norm": 0.8200161457061768,
      "learning_rate": 5.219005196733482e-06,
      "loss": 0.7219,
      "step": 12070
    },
    {
      "epoch": 2.6903078550353503,
      "grad_norm": 0.7732033729553223,
      "learning_rate": 5.181885671863401e-06,
      "loss": 0.7322,
      "step": 12080
    },
    {
      "epoch": 2.6925346545677225,
      "grad_norm": 0.7892482280731201,
      "learning_rate": 5.144766146993319e-06,
      "loss": 0.7185,
      "step": 12090
    },
    {
      "epoch": 2.6947614541000946,
      "grad_norm": 0.7066119313240051,
      "learning_rate": 5.107646622123237e-06,
      "loss": 0.7342,
      "step": 12100
    },
    {
      "epoch": 2.6969882536324667,
      "grad_norm": 0.6579950451850891,
      "learning_rate": 5.070527097253156e-06,
      "loss": 0.7674,
      "step": 12110
    },
    {
      "epoch": 2.699215053164839,
      "grad_norm": 0.6911709904670715,
      "learning_rate": 5.033407572383074e-06,
      "loss": 0.7455,
      "step": 12120
    },
    {
      "epoch": 2.701441852697211,
      "grad_norm": 1.9809801578521729,
      "learning_rate": 4.996288047512991e-06,
      "loss": 0.7625,
      "step": 12130
    },
    {
      "epoch": 2.703668652229583,
      "grad_norm": 0.7624685168266296,
      "learning_rate": 4.9591685226429105e-06,
      "loss": 0.7384,
      "step": 12140
    },
    {
      "epoch": 2.705895451761955,
      "grad_norm": 0.712795078754425,
      "learning_rate": 4.922048997772829e-06,
      "loss": 0.7071,
      "step": 12150
    },
    {
      "epoch": 2.708122251294327,
      "grad_norm": 0.6610375046730042,
      "learning_rate": 4.884929472902747e-06,
      "loss": 0.7345,
      "step": 12160
    },
    {
      "epoch": 2.710349050826699,
      "grad_norm": 0.8304591774940491,
      "learning_rate": 4.8478099480326655e-06,
      "loss": 0.7367,
      "step": 12170
    },
    {
      "epoch": 2.7125758503590713,
      "grad_norm": 0.6870518326759338,
      "learning_rate": 4.810690423162584e-06,
      "loss": 0.7312,
      "step": 12180
    },
    {
      "epoch": 2.7148026498914435,
      "grad_norm": 0.7410963773727417,
      "learning_rate": 4.773570898292502e-06,
      "loss": 0.7093,
      "step": 12190
    },
    {
      "epoch": 2.7170294494238156,
      "grad_norm": 0.7775647640228271,
      "learning_rate": 4.73645137342242e-06,
      "loss": 0.7833,
      "step": 12200
    },
    {
      "epoch": 2.719256248956188,
      "grad_norm": 0.6988734602928162,
      "learning_rate": 4.699331848552339e-06,
      "loss": 0.7266,
      "step": 12210
    },
    {
      "epoch": 2.72148304848856,
      "grad_norm": 0.7102094292640686,
      "learning_rate": 4.662212323682257e-06,
      "loss": 0.8043,
      "step": 12220
    },
    {
      "epoch": 2.723709848020932,
      "grad_norm": 0.7447689771652222,
      "learning_rate": 4.625092798812175e-06,
      "loss": 0.798,
      "step": 12230
    },
    {
      "epoch": 2.7259366475533042,
      "grad_norm": 0.7901514172554016,
      "learning_rate": 4.587973273942094e-06,
      "loss": 0.7083,
      "step": 12240
    },
    {
      "epoch": 2.7281634470856764,
      "grad_norm": 1.1963748931884766,
      "learning_rate": 4.550853749072012e-06,
      "loss": 0.7551,
      "step": 12250
    },
    {
      "epoch": 2.730390246618048,
      "grad_norm": 0.8168783783912659,
      "learning_rate": 4.51373422420193e-06,
      "loss": 0.7007,
      "step": 12260
    },
    {
      "epoch": 2.7326170461504202,
      "grad_norm": 0.7436819076538086,
      "learning_rate": 4.4766146993318486e-06,
      "loss": 0.7602,
      "step": 12270
    },
    {
      "epoch": 2.7348438456827924,
      "grad_norm": 0.7434145212173462,
      "learning_rate": 4.439495174461767e-06,
      "loss": 0.7284,
      "step": 12280
    },
    {
      "epoch": 2.7370706452151645,
      "grad_norm": 0.7957887053489685,
      "learning_rate": 4.402375649591685e-06,
      "loss": 0.772,
      "step": 12290
    },
    {
      "epoch": 2.7392974447475367,
      "grad_norm": 0.7609837055206299,
      "learning_rate": 4.365256124721604e-06,
      "loss": 0.695,
      "step": 12300
    },
    {
      "epoch": 2.741524244279909,
      "grad_norm": 0.7882001399993896,
      "learning_rate": 4.328136599851522e-06,
      "loss": 0.7512,
      "step": 12310
    },
    {
      "epoch": 2.743751043812281,
      "grad_norm": 0.7335203886032104,
      "learning_rate": 4.29101707498144e-06,
      "loss": 0.7391,
      "step": 12320
    },
    {
      "epoch": 2.7459778433446527,
      "grad_norm": 0.6875292062759399,
      "learning_rate": 4.253897550111359e-06,
      "loss": 0.7357,
      "step": 12330
    },
    {
      "epoch": 2.748204642877025,
      "grad_norm": 0.7514373660087585,
      "learning_rate": 4.216778025241277e-06,
      "loss": 0.7277,
      "step": 12340
    },
    {
      "epoch": 2.750431442409397,
      "grad_norm": 0.746216356754303,
      "learning_rate": 4.179658500371195e-06,
      "loss": 0.7361,
      "step": 12350
    },
    {
      "epoch": 2.752658241941769,
      "grad_norm": 0.7034128904342651,
      "learning_rate": 4.142538975501114e-06,
      "loss": 0.7774,
      "step": 12360
    },
    {
      "epoch": 2.7548850414741413,
      "grad_norm": 0.7426614761352539,
      "learning_rate": 4.1054194506310325e-06,
      "loss": 0.7303,
      "step": 12370
    },
    {
      "epoch": 2.7571118410065134,
      "grad_norm": 0.7082977890968323,
      "learning_rate": 4.06829992576095e-06,
      "loss": 0.7006,
      "step": 12380
    },
    {
      "epoch": 2.7593386405388856,
      "grad_norm": 0.7702799439430237,
      "learning_rate": 4.031180400890869e-06,
      "loss": 0.7238,
      "step": 12390
    },
    {
      "epoch": 2.7615654400712577,
      "grad_norm": 0.6345691680908203,
      "learning_rate": 3.9940608760207874e-06,
      "loss": 0.713,
      "step": 12400
    },
    {
      "epoch": 2.76379223960363,
      "grad_norm": 0.7841594815254211,
      "learning_rate": 3.956941351150705e-06,
      "loss": 0.7834,
      "step": 12410
    },
    {
      "epoch": 2.766019039136002,
      "grad_norm": 0.8545466661453247,
      "learning_rate": 3.919821826280624e-06,
      "loss": 0.7389,
      "step": 12420
    },
    {
      "epoch": 2.768245838668374,
      "grad_norm": 0.7832798957824707,
      "learning_rate": 3.882702301410542e-06,
      "loss": 0.7154,
      "step": 12430
    },
    {
      "epoch": 2.770472638200746,
      "grad_norm": 0.7212615013122559,
      "learning_rate": 3.845582776540461e-06,
      "loss": 0.7544,
      "step": 12440
    },
    {
      "epoch": 2.772699437733118,
      "grad_norm": 0.6381064057350159,
      "learning_rate": 3.8084632516703785e-06,
      "loss": 0.7384,
      "step": 12450
    },
    {
      "epoch": 2.77492623726549,
      "grad_norm": 0.8121217489242554,
      "learning_rate": 3.7713437268002973e-06,
      "loss": 0.7439,
      "step": 12460
    },
    {
      "epoch": 2.7771530367978623,
      "grad_norm": 0.8996118307113647,
      "learning_rate": 3.7342242019302156e-06,
      "loss": 0.6952,
      "step": 12470
    },
    {
      "epoch": 2.7793798363302344,
      "grad_norm": 0.7040563821792603,
      "learning_rate": 3.6971046770601335e-06,
      "loss": 0.7247,
      "step": 12480
    },
    {
      "epoch": 2.7816066358626066,
      "grad_norm": 0.7851295471191406,
      "learning_rate": 3.659985152190052e-06,
      "loss": 0.7576,
      "step": 12490
    },
    {
      "epoch": 2.7838334353949783,
      "grad_norm": 0.6295440196990967,
      "learning_rate": 3.6228656273199705e-06,
      "loss": 0.71,
      "step": 12500
    },
    {
      "epoch": 2.7860602349273504,
      "grad_norm": 0.840015709400177,
      "learning_rate": 3.5857461024498892e-06,
      "loss": 0.7247,
      "step": 12510
    },
    {
      "epoch": 2.7882870344597226,
      "grad_norm": 0.7157265543937683,
      "learning_rate": 3.548626577579807e-06,
      "loss": 0.6911,
      "step": 12520
    },
    {
      "epoch": 2.7905138339920947,
      "grad_norm": 0.9299542903900146,
      "learning_rate": 3.5115070527097254e-06,
      "loss": 0.7373,
      "step": 12530
    },
    {
      "epoch": 2.792740633524467,
      "grad_norm": 0.7675543427467346,
      "learning_rate": 3.474387527839644e-06,
      "loss": 0.6923,
      "step": 12540
    },
    {
      "epoch": 2.794967433056839,
      "grad_norm": 0.7328622341156006,
      "learning_rate": 3.4372680029695616e-06,
      "loss": 0.7203,
      "step": 12550
    },
    {
      "epoch": 2.797194232589211,
      "grad_norm": 0.7437323927879333,
      "learning_rate": 3.4001484780994804e-06,
      "loss": 0.7084,
      "step": 12560
    },
    {
      "epoch": 2.7994210321215833,
      "grad_norm": 0.7115694880485535,
      "learning_rate": 3.363028953229399e-06,
      "loss": 0.7581,
      "step": 12570
    },
    {
      "epoch": 2.8016478316539555,
      "grad_norm": 0.6798532009124756,
      "learning_rate": 3.3259094283593174e-06,
      "loss": 0.7125,
      "step": 12580
    },
    {
      "epoch": 2.8038746311863276,
      "grad_norm": 0.7767360806465149,
      "learning_rate": 3.2887899034892353e-06,
      "loss": 0.7301,
      "step": 12590
    },
    {
      "epoch": 2.8061014307186998,
      "grad_norm": 0.7288995981216431,
      "learning_rate": 3.2516703786191536e-06,
      "loss": 0.7381,
      "step": 12600
    },
    {
      "epoch": 2.808328230251072,
      "grad_norm": 0.6760746240615845,
      "learning_rate": 3.2145508537490723e-06,
      "loss": 0.7706,
      "step": 12610
    },
    {
      "epoch": 2.8105550297834436,
      "grad_norm": 0.815605878829956,
      "learning_rate": 3.1774313288789902e-06,
      "loss": 0.7814,
      "step": 12620
    },
    {
      "epoch": 2.8127818293158158,
      "grad_norm": 0.7296213507652283,
      "learning_rate": 3.1403118040089085e-06,
      "loss": 0.7322,
      "step": 12630
    },
    {
      "epoch": 2.815008628848188,
      "grad_norm": 0.7307763695716858,
      "learning_rate": 3.1031922791388273e-06,
      "loss": 0.7542,
      "step": 12640
    },
    {
      "epoch": 2.81723542838056,
      "grad_norm": 0.6087955236434937,
      "learning_rate": 3.0660727542687456e-06,
      "loss": 0.7076,
      "step": 12650
    },
    {
      "epoch": 2.819462227912932,
      "grad_norm": 0.7239677906036377,
      "learning_rate": 3.028953229398664e-06,
      "loss": 0.7176,
      "step": 12660
    },
    {
      "epoch": 2.8216890274453044,
      "grad_norm": 0.7446327805519104,
      "learning_rate": 2.991833704528582e-06,
      "loss": 0.7401,
      "step": 12670
    },
    {
      "epoch": 2.823915826977676,
      "grad_norm": 0.6575825214385986,
      "learning_rate": 2.9547141796585005e-06,
      "loss": 0.7194,
      "step": 12680
    },
    {
      "epoch": 2.826142626510048,
      "grad_norm": 0.706968367099762,
      "learning_rate": 2.917594654788419e-06,
      "loss": 0.7358,
      "step": 12690
    },
    {
      "epoch": 2.8283694260424204,
      "grad_norm": 0.7672249674797058,
      "learning_rate": 2.880475129918337e-06,
      "loss": 0.7475,
      "step": 12700
    },
    {
      "epoch": 2.8305962255747925,
      "grad_norm": 0.7085825204849243,
      "learning_rate": 2.8433556050482554e-06,
      "loss": 0.7439,
      "step": 12710
    },
    {
      "epoch": 2.8328230251071647,
      "grad_norm": 0.7703351378440857,
      "learning_rate": 2.8062360801781737e-06,
      "loss": 0.7795,
      "step": 12720
    },
    {
      "epoch": 2.835049824639537,
      "grad_norm": 0.7890596389770508,
      "learning_rate": 2.7691165553080925e-06,
      "loss": 0.7712,
      "step": 12730
    },
    {
      "epoch": 2.837276624171909,
      "grad_norm": 0.801702082157135,
      "learning_rate": 2.7319970304380104e-06,
      "loss": 0.733,
      "step": 12740
    },
    {
      "epoch": 2.839503423704281,
      "grad_norm": 0.7705674767494202,
      "learning_rate": 2.694877505567929e-06,
      "loss": 0.7176,
      "step": 12750
    },
    {
      "epoch": 2.8417302232366533,
      "grad_norm": 0.7464073300361633,
      "learning_rate": 2.6577579806978474e-06,
      "loss": 0.7934,
      "step": 12760
    },
    {
      "epoch": 2.8439570227690254,
      "grad_norm": 0.712387204170227,
      "learning_rate": 2.6206384558277653e-06,
      "loss": 0.7518,
      "step": 12770
    },
    {
      "epoch": 2.8461838223013975,
      "grad_norm": 0.7729831337928772,
      "learning_rate": 2.583518930957684e-06,
      "loss": 0.7044,
      "step": 12780
    },
    {
      "epoch": 2.8484106218337693,
      "grad_norm": 0.727350115776062,
      "learning_rate": 2.546399406087602e-06,
      "loss": 0.7367,
      "step": 12790
    },
    {
      "epoch": 2.8506374213661414,
      "grad_norm": 0.7577881217002869,
      "learning_rate": 2.5092798812175206e-06,
      "loss": 0.6975,
      "step": 12800
    },
    {
      "epoch": 2.8528642208985135,
      "grad_norm": 0.6809479594230652,
      "learning_rate": 2.472160356347439e-06,
      "loss": 0.731,
      "step": 12810
    },
    {
      "epoch": 2.8550910204308857,
      "grad_norm": 0.7351939082145691,
      "learning_rate": 2.4350408314773572e-06,
      "loss": 0.7758,
      "step": 12820
    },
    {
      "epoch": 2.857317819963258,
      "grad_norm": 0.8141433596611023,
      "learning_rate": 2.3979213066072756e-06,
      "loss": 0.7269,
      "step": 12830
    },
    {
      "epoch": 2.85954461949563,
      "grad_norm": 0.7731645107269287,
      "learning_rate": 2.360801781737194e-06,
      "loss": 0.7123,
      "step": 12840
    },
    {
      "epoch": 2.861771419028002,
      "grad_norm": 0.7855708003044128,
      "learning_rate": 2.323682256867112e-06,
      "loss": 0.7612,
      "step": 12850
    },
    {
      "epoch": 2.863998218560374,
      "grad_norm": 0.6915579438209534,
      "learning_rate": 2.2865627319970305e-06,
      "loss": 0.7631,
      "step": 12860
    },
    {
      "epoch": 2.866225018092746,
      "grad_norm": 0.7470896244049072,
      "learning_rate": 2.249443207126949e-06,
      "loss": 0.7123,
      "step": 12870
    },
    {
      "epoch": 2.868451817625118,
      "grad_norm": 0.6731839179992676,
      "learning_rate": 2.212323682256867e-06,
      "loss": 0.7371,
      "step": 12880
    },
    {
      "epoch": 2.8706786171574903,
      "grad_norm": 0.7021644115447998,
      "learning_rate": 2.175204157386786e-06,
      "loss": 0.7251,
      "step": 12890
    },
    {
      "epoch": 2.8729054166898624,
      "grad_norm": 0.631770670413971,
      "learning_rate": 2.1380846325167037e-06,
      "loss": 0.7608,
      "step": 12900
    },
    {
      "epoch": 2.8751322162222346,
      "grad_norm": 0.9135423302650452,
      "learning_rate": 2.1009651076466224e-06,
      "loss": 0.7146,
      "step": 12910
    },
    {
      "epoch": 2.8773590157546067,
      "grad_norm": 0.6981375813484192,
      "learning_rate": 2.0638455827765408e-06,
      "loss": 0.7466,
      "step": 12920
    },
    {
      "epoch": 2.879585815286979,
      "grad_norm": 0.6806748509407043,
      "learning_rate": 2.0267260579064586e-06,
      "loss": 0.759,
      "step": 12930
    },
    {
      "epoch": 2.881812614819351,
      "grad_norm": 0.6799878478050232,
      "learning_rate": 1.9896065330363774e-06,
      "loss": 0.6986,
      "step": 12940
    },
    {
      "epoch": 2.884039414351723,
      "grad_norm": 0.7015032172203064,
      "learning_rate": 1.9524870081662957e-06,
      "loss": 0.7046,
      "step": 12950
    },
    {
      "epoch": 2.8862662138840953,
      "grad_norm": 0.723539412021637,
      "learning_rate": 1.915367483296214e-06,
      "loss": 0.7155,
      "step": 12960
    },
    {
      "epoch": 2.888493013416467,
      "grad_norm": 0.8428224325180054,
      "learning_rate": 1.8782479584261323e-06,
      "loss": 0.7579,
      "step": 12970
    },
    {
      "epoch": 2.890719812948839,
      "grad_norm": 0.748863935470581,
      "learning_rate": 1.8411284335560508e-06,
      "loss": 0.7062,
      "step": 12980
    },
    {
      "epoch": 2.8929466124812113,
      "grad_norm": 0.6763566136360168,
      "learning_rate": 1.8077208611729769e-06,
      "loss": 0.7048,
      "step": 12990
    },
    {
      "epoch": 2.8951734120135835,
      "grad_norm": 0.7114309668540955,
      "learning_rate": 1.7706013363028954e-06,
      "loss": 0.7477,
      "step": 13000
    }
  ],
  "logging_steps": 10,
  "max_steps": 13470,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.2723345971321242e+17,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
